{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: filing_texts: File exists\n",
      "mkdir: prices: File exists\n"
     ]
    }
   ],
   "source": [
    "% mkdir filing_texts\n",
    "% mkdir prices "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, TfidfTransformer, CountVectorizer\n",
    "import glob\n",
    "import os \n",
    "import pandas as pd\n",
    "import datetime\n",
    "from scipy.stats import iqr\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import metrics\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "import math\n",
    "\n",
    "spm = None\n",
    "english_words = None\n",
    "excepted_stock_names = []\n",
    "limit = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.layers import Dense, Dropout, RepeatVector, BatchNormalization, Convolution1D, Flatten, Lambda, Permute, MaxPooling1D, AlphaDropout\n",
    "from keras.models import Sequential\n",
    "from keras.utils import to_categorical\n",
    "import keras.backend as K\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import load_model\n",
    "from tqdm import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DateRange():\n",
    "    def __init__(self, start_int, end_int):\n",
    "        self.start_int = start_int\n",
    "        self.end_int = end_int\n",
    "        self.range = (range(start_int, end_int))\n",
    "\n",
    "    def transform(self, date):\n",
    "        output = {}\n",
    "        dates = []\n",
    "        for delta in self.range:\n",
    "            date_delta = datetime.timedelta(days=delta)\n",
    "            date_string = datetime.datetime.strptime(date, '%Y-%m-%d').date()\n",
    "            dates.append(str(date_string + date_delta))\n",
    "        return dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SNDM():\n",
    "    def __init__(self, limit=-1):\n",
    "        self.limit = limit\n",
    "        self.sndm = self.__get_sndm()\n",
    "\n",
    "    def __get_sndm(self):\n",
    "        spm = self.__stock_name_date_mapping()\n",
    "        output = {}\n",
    "        for stock_name in spm.keys():\n",
    "            numerical_prices = 0\n",
    "            for date in DateRange(-3,2).transform(spm[stock_name]['date']):\n",
    "                try:\n",
    "                    isnan = math.isnan(PriceData(stock_name).on_date(date, 'open'))\n",
    "                except:\n",
    "                    isnan = True\n",
    "                if not (isnan):\n",
    "                    numerical_prices += 1 \n",
    "            # Delete stock_name from spm if numerical prices under a threshold. (nans)\n",
    "            if numerical_prices >= 3:\n",
    "                output[stock_name] = spm[stock_name]\n",
    "        return output\n",
    "    \n",
    "    def __stock_name_date_mapping(self):\n",
    "        spm = {}\n",
    "        for i, file in enumerate(glob.glob('filing_texts/*')[0:self.limit]):\n",
    "            try:\n",
    "                st_name = self.__stock_name_from_filename(file)\n",
    "                file_data = open(file, 'r').read()[:200]\n",
    "                time_data = file_data.split('<ACCEPTANCE-DATETIME>')[1].split('\\\\n')[0][:8]\n",
    "                year = time_data[0:4]\n",
    "                month = time_data[4:6]\n",
    "                day = time_data[6:8]\n",
    "                date_stamp = (\"%s-%s-%s\" %(year, month, day))\n",
    "                spm[st_name] = {'date': date_stamp}\n",
    "            except:\n",
    "                print(\"No data for \", file)\n",
    "                continue\n",
    "        return spm\n",
    "    def __stock_name_from_filename(self, filename):\n",
    "        return filename.split('/')[-1].split('_')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1120,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spm = SNDM().sndm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1121,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class PriceData:\n",
    "    def __init__(self, stock_name):\n",
    "        self.stock_name = stock_name\n",
    "        try:\n",
    "            self.price_data = pd.read_csv('prices/' + stock_name + '.csv')\n",
    "        except:\n",
    "            self.price_data = pd.DataFrame.from_dict({})\n",
    "\n",
    "    def on_date(self, date, market_time = 'open'): \n",
    "        try:\n",
    "            return float(self.price_data.loc[self.price_data['date'] == date][market_time])\n",
    "        except: \n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1122,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class FilenamesToStockNamesTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        return None\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def fit_transform(self, X, y=None):\n",
    "        return self.fit(X, y).transform(X)\n",
    "        \n",
    "    def transform(self, X):\n",
    "        output = []\n",
    "        for filename in X:\n",
    "            stock_name = self.__stock_name_from_filename(filename)\n",
    "            output.append(stock_name)\n",
    "        return output\n",
    "    def __stock_name_from_filename(self, filename):\n",
    "        return filename.split('/')[-1].split('_')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1123,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Used in y pipeline\n",
    "class SpmToFileNamesTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        return None\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def fit_transform(self, X, y=None):\n",
    "        return self.fit(X, y).transform(X)\n",
    "    def transform(self, X):\n",
    "        output = []\n",
    "        for key in X.keys():\n",
    "            date = X[key]['date']\n",
    "            output.append(f'filing_texts/{key}_{date}')\n",
    "        return output\n",
    "    \n",
    "class SpmToStockNamesTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        return None\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def fit_transform(self, X, y=None):\n",
    "        return self.fit(X, y).transform(X)\n",
    "        \n",
    "    def transform(self, X):\n",
    "        return X.keys()\n",
    "    \n",
    "class StockNamesToFileNamesTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        return None\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def fit_transform(self, X, y=None):\n",
    "        return self.fit(X, y).transform(X)\n",
    "        \n",
    "    def transform(self, X):\n",
    "        return [f'filing_texts/{stock_name}' for stock_name in X]\n",
    "    \n",
    "# class MapStockNamesToDatesTransformer(BaseEstimator, TransformerMixin):\n",
    "#     def __init__(self, start_int, end_int):\n",
    "#         self.start_int = start_int\n",
    "#         self.end_int = end_int\n",
    "#         self.range = (range(start_int, end_int))\n",
    "    \n",
    "#     def fit(self, X, y=None):\n",
    "#         return self\n",
    "    \n",
    "#     def fit_transform(self, X, y=None):\n",
    "#         return self.fit(X, y).transform(X)\n",
    "        \n",
    "#     def transform(self, X):\n",
    "#         output = {}\n",
    "#         for i, stock_name in enumerate(X):\n",
    "#             try:\n",
    "#                 date = spm[stock_name]['date']\n",
    "#             except:\n",
    "#                 continue # Ignore stocks which don't have a date\n",
    "#             dates = []\n",
    "#             for delta in self.range:\n",
    "#                 date_delta = datetime.timedelta(days=delta)\n",
    "#                 date_string = datetime.datetime.strptime(date, '%Y-%m-%d').date()\n",
    "#                 dates.append(str(date_string + date_delta))\n",
    "#             output[stock_name] = dates\n",
    "#         return output\n",
    "# class StockNameDatesMapToPricesListTransformer(BaseEstimator, TransformerMixin):\n",
    "#     def __init__(self):\n",
    "#         return None\n",
    "    \n",
    "#     def fit(self, X, y=None):\n",
    "#         return self\n",
    "    \n",
    "#     def fit_transform(self, X, y=None):\n",
    "#         return self.fit(X, y).transform(X)\n",
    "        \n",
    "#     def transform(self, X):\n",
    "#         output = []\n",
    "#         for stock_name in X.keys():\n",
    "#             prices = []\n",
    "#             for date in X[stock_name]:\n",
    "#                 prices.append(PriceData(stock_name).on_date(date, 'open'))\n",
    "#             output.append(prices)\n",
    "#         return np.array(output)\n",
    "    \n",
    "# class LabelsTransform(BaseEstimator, TransformerMixin):\n",
    "#     # Returns the interquartile-range and median.\n",
    "#     def __init__(self):\n",
    "#         return None\n",
    "        \n",
    "#     def fit(self, X, y=None):\n",
    "#         return self\n",
    "    \n",
    "#     def fit_transform(self, X, y=None):\n",
    "#         return self.fit(X, y).transform(X)\n",
    "        \n",
    "#     def transform(self, X):\n",
    "#         # ldcom = last_day_change_over_median\n",
    "#         print(X.shape)\n",
    "#         ldcom = []\n",
    "#         for prices in X:\n",
    "#             this_median = np.median(prices[0:-3])\n",
    "#             ldcom.append(((prices[-1]-this_median)/this_median))\n",
    "#         return np.array(ldcom)\n",
    "            \n",
    "class StatisticalMeasuresTransformer(BaseEstimator, TransformerMixin):\n",
    "    # Returns the interquartile-range and median.\n",
    "    def __init__(self):\n",
    "        return None\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        # ldcom = last_day_change_over_median\n",
    "        \n",
    "        output = []\n",
    "        self.iqr_var = []\n",
    "        self.median = []\n",
    "        for prices in X:\n",
    "            this_iqr = iqr(prices[0:-3])\n",
    "            this_median = np.median(prices[0:-3])\n",
    "            self.iqr_var.append(this_iqr)\n",
    "            self.median.append(this_median)\n",
    "        return self\n",
    "    \n",
    "    def fit_transform(self, X, y=None):\n",
    "        return self.fit(X, y).transform(X)\n",
    "        \n",
    "    def transform(self, X):\n",
    "        X_output = []\n",
    "        for i, prices in enumerate(X):\n",
    "            stats   = []\n",
    "            iqr_var = self.iqr_var[i] or iqr(prices)\n",
    "            median  = self.median[i]  or np.median(prices)\n",
    "            \n",
    "            stats.append(iqr_var)\n",
    "            stats.append(median)\n",
    "            \n",
    "            X_output.append(stats)\n",
    "        return np.array(X_output)\n",
    "class SparseToArray(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        return None\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def fit_transform(self, X, y=None):\n",
    "        return self.fit(X, y).transform(X)\n",
    "        \n",
    "    def transform(self, X):\n",
    "        return np.array(X.toarray()) #[ar.toarray() for ar in X]\n",
    "    \n",
    "class ReadFiles(BaseEstimator, TransformerMixin): \n",
    "    def __init__(self):\n",
    "        return None\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def fit_transform(self, X, y=None):\n",
    "        return self.fit(X, y).transform(X)\n",
    "        \n",
    "    def transform(self, X):\n",
    "        return (open(filename, 'r').read() for filename in tqdm(X))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1124,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_filenames(limit = -1):\n",
    "    filenames = []\n",
    "    directory_files = glob.glob('filing_texts/*')\n",
    "    excepted_files = [('filing_texts/' + sn) for sn in excepted_stock_names]\n",
    "    filenames_with_data = [x for x in directory_files[:limit] if x not in excepted_files]\n",
    "    for filename in filenames_with_data:\n",
    "        filenames.append(filename)\n",
    "    return filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1125,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class StockNamesToLabelsTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, start_int, end_int):\n",
    "        self.filing_int = start_int * -1\n",
    "        self.start_int = start_int\n",
    "        self.end_int = end_int\n",
    "        self.range = (range(start_int, end_int))\n",
    "    # Not implemented since only used to generate labels\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    # Not implemented since only used to generate labels\n",
    "    def fit_transform(self, X, y=None):\n",
    "        return self.fit(X, y).transform(X)\n",
    "        \n",
    "    def transform(self, X):\n",
    "        h = {}\n",
    "        for i, stock_name in enumerate(X):\n",
    "            try:\n",
    "                date = spm[stock_name]['date']\n",
    "            except:\n",
    "                continue # Ignore stocks which don't have a date\n",
    "            dates = []\n",
    "            for delta in self.range:\n",
    "                date_delta = datetime.timedelta(days=delta)\n",
    "                date_string = datetime.datetime.strptime(date, '%Y-%m-%d').date()\n",
    "                dates.append(str(date_string + date_delta))\n",
    "            h[stock_name] = dates\n",
    "        \n",
    "        ldcom = []\n",
    "        for stock_name in h.keys():\n",
    "            earliest_price_after_filing = None\n",
    "            hist_p = []\n",
    "            for i, date in enumerate(h[stock_name]):\n",
    "                price = PriceData(stock_name).on_date(date, 'open')\n",
    "                if price and not math.isnan(price):\n",
    "                    if i > self.filing_int:\n",
    "                        earliest_price_after_filing = earliest_price_after_filing or price\n",
    "                    else:\n",
    "                        hist_p.append(price)\n",
    "            # Closing price on day of filing\n",
    "            price_close_filing = PriceData(stock_name).on_date(h[stock_name][self.filing_int], 'close')\n",
    "            # Use either the next open day of trading or the close price on day of filing\n",
    "            comparison_price = earliest_price_after_filing or price_close_filing\n",
    "            # Remove nans from historical prices before taking the mean\n",
    "            this_mean = np.mean(hist_p)\n",
    "            ldcom.append(((comparison_price-this_mean)/this_mean))\n",
    "        return np.array(ldcom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.01682823])"
      ]
     },
     "execution_count": 1126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "StockNamesToLabelsTransformer(-5,5).transform(['XOXO'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1127,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Used to build other pipelines\n",
    "from sklearn.preprocessing import Imputer\n",
    "\n",
    "# prices_pipeline = Pipeline([\n",
    "#     ('spm_to_filenames', SpmToFileNamesTransformer()),\n",
    "#     ('filenames_to_stock_names', FilenamesToStockNamesTransformer()),\n",
    "#     ('stock_names_to_dates', MapStockNamesToDatesTransformer(-5, 2)),\n",
    "#     ('dates_to_prices_transformer', StockNameDatesMapToPricesListTransformer()),\n",
    "#     ('imputer', Imputer(axis=1))\n",
    "# ])\n",
    "\n",
    "labels_pipeline = Pipeline([\n",
    "    ('spm_to_filenames', SpmToFileNamesTransformer()),\n",
    "    ('filenames_to_stock_names', FilenamesToStockNamesTransformer()),\n",
    "    ('dates_to_prices_transformer', StockNamesToLabelsTransformer(-5,2))\n",
    "])\n",
    "\n",
    "# Used as the final y values \n",
    "# labels_pipeline = Pipeline([\n",
    "#     ('prices_pipeline', prices_pipeline),\n",
    "#     ('labels_transform', LabelsTransform())\n",
    "# ])\n",
    "\n",
    "# Used for training and test set features\n",
    "stock_stats = Pipeline([\n",
    "    ('prices_pipeline', prices_pipeline),\n",
    "    ('stats_transform', StatisticalMeasuresTransformer())\n",
    "])\n",
    "\n",
    "# Used for training and test set features \n",
    "# ('stock_names_to_file_names', StockNamesToFileNamesTransformer()),\n",
    "text_word_counts = Pipeline([\n",
    "    ('spm_to_file_names', SpmToFileNamesTransformer()),\n",
    "    ('read_files', ReadFiles()),\n",
    "    ('vect', TfidfVectorizer(\n",
    "                token_pattern=r\"[a-zA-Z]+\", \n",
    "                min_df = 0.10,\n",
    "                max_df = 0.80,\n",
    "                stop_words = 'english',\n",
    "                max_features=9000,\n",
    "                ngram_range=(1, 1))),\n",
    "    ('sparse_to_array', SparseToArray()),\n",
    "    ('std_scaler', StandardScaler()),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1128,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/188 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  1%|          | 2/188 [00:00<00:20,  9.14it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  5%|▌         | 10/188 [00:00<00:12, 14.09it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  7%|▋         | 14/188 [00:00<00:10, 16.44it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  9%|▊         | 16/188 [00:02<00:22,  7.68it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 13%|█▎        | 24/188 [00:02<00:15, 10.83it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 14%|█▍        | 27/188 [00:02<00:14, 11.12it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 18%|█▊        | 34/188 [00:02<00:13, 11.59it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 21%|██        | 39/188 [00:03<00:11, 12.72it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 22%|██▏       | 42/188 [00:03<00:11, 12.80it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 24%|██▍       | 46/188 [00:03<00:10, 13.60it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 30%|██▉       | 56/188 [00:03<00:08, 16.01it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 32%|███▏      | 61/188 [00:03<00:07, 16.82it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 35%|███▌      | 66/188 [00:04<00:07, 16.44it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 37%|███▋      | 70/188 [00:06<00:11, 10.67it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 42%|████▏     | 79/188 [00:06<00:09, 11.86it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 45%|████▍     | 84/188 [00:06<00:08, 12.12it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 47%|████▋     | 88/188 [00:07<00:08, 11.80it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 50%|█████     | 94/188 [00:07<00:07, 12.41it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 54%|█████▍    | 102/188 [00:07<00:06, 13.27it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 57%|█████▋    | 107/188 [00:08<00:06, 11.96it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 60%|██████    | 113/188 [00:09<00:06, 11.63it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 62%|██████▏   | 117/188 [00:09<00:05, 11.91it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 66%|██████▌   | 124/188 [00:09<00:05, 12.48it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 68%|██████▊   | 128/188 [00:10<00:04, 12.66it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 70%|███████   | 132/188 [00:14<00:06,  9.26it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 74%|███████▍  | 139/188 [00:14<00:05,  9.68it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 79%|███████▉  | 149/188 [00:15<00:03,  9.85it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 81%|████████  | 152/188 [00:22<00:05,  6.82it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 82%|████████▏ | 155/188 [00:22<00:04,  6.84it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 84%|████████▎ | 157/188 [00:22<00:04,  6.88it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 85%|████████▍ | 159/188 [00:26<00:04,  5.94it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 87%|████████▋ | 163/188 [00:26<00:04,  6.06it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 91%|█████████ | 171/188 [00:27<00:02,  6.31it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 93%|█████████▎| 174/188 [00:27<00:02,  6.40it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 95%|█████████▌| 179/188 [00:27<00:01,  6.55it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 97%|█████████▋| 183/188 [00:27<00:00,  6.67it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████| 188/188 [00:28<00:00,  6.55it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(188, 6004)\n",
      "(188,)\n"
     ]
    }
   ],
   "source": [
    "X = text_word_counts.fit_transform(spm)\n",
    "y = labels_pipeline.fit_transform(spm)\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1129,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train_continuous, y_test_continuous = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1130,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bool_arr(arr, limit=0.5):\n",
    "    y_bool = []\n",
    "    for num in arr:\n",
    "        if num >= limit:\n",
    "            y_bool.append(1)\n",
    "        else:\n",
    "            y_bool.append(0)\n",
    "    return np.array(y_bool)\n",
    "y_train = bool_arr(y_train_continuous, 0.05)\n",
    "y_test = bool_arr(y_test_continuous, 0.00)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.44736842105263158"
      ]
     },
     "execution_count": 1131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectFpr\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "\n",
    "svc_train_pipeline = Pipeline([\n",
    "    ('reduce_false_pos', SelectFpr(alpha=0.1)),\n",
    "    ('svc', GridSearchCV(estimator=SVC(gamma='auto'), param_grid=dict(C=np.logspace(-1,3)), n_jobs=1))\n",
    "])\n",
    "svc_train_pipeline.fit(X_train, y_train)\n",
    "svc_train_pipeline.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1132,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Piper/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:194: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SVC(C=100, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
       "  kernel='rbf', max_iter=-1, probability=False, random_state=None,\n",
       "  shrinking=True, tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 1132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "clf = SVC(C=100)\n",
    "# clf = RandomForestRegressor(n_estimators=200, max_depth=3, verbose=1, n_jobs=2)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1133,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras import regularizers\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1151,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepEstimator():\n",
    "    def __init__(self):\n",
    "        return None\n",
    "    def fit(self, X, y, X_val, y_val):\n",
    "        class_weight = {0: 0.1,\n",
    "                1: 0.9}\n",
    "        self.model = self.__define_model(X)\n",
    "        sched = [[0.0001, 2], [0.001, 20], [0.01, 2], [0.1, 2], [0.5, 1], [0.1, 5], [0.01, 20], [0.001, 40], [0.0001, 80], [0.00005, 120]]\n",
    "        for lr, epochs in sched:\n",
    "            self.model.optimizer.lr = lr\n",
    "            self.model.fit(np.array(X), np.array(y), epochs=epochs,  class_weight=class_weight, batch_size=64, validation_data=(X_val, y_val))\n",
    "        return self\n",
    "    def predict(self, X, y=None):\n",
    "        return self.model.predict(X)\n",
    "    def __define_model(self, X):\n",
    "        shape = X.shape[1]\n",
    "        model = Sequential([\n",
    "            BatchNormalization(input_shape=(shape,)),\n",
    "            Dense(25, activation='relu', kernel_regularizer=regularizers.l1(0.01)),\n",
    "            Dropout(0.8),\n",
    "            BatchNormalization(),\n",
    "            Dense(1, activation='sigmoid')   \n",
    "        ])\n",
    "\n",
    "        model.compile(optimizer='adam',\n",
    "                      loss='binary_crossentropy',\n",
    "                      metrics=['accuracy'])\n",
    "        return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 150 samples, validate on 38 samples\n",
      "Epoch 1/2\n",
      "150/150 [==============================] - 3s 20ms/step - loss: 9.3447 - acc: 0.5467 - val_loss: 9.8915 - val_acc: 0.5526\n",
      "Epoch 2/2\n",
      "150/150 [==============================] - 0s 190us/step - loss: 9.2921 - acc: 0.5867 - val_loss: 9.8204 - val_acc: 0.5526\n",
      "Train on 150 samples, validate on 38 samples\n",
      "Epoch 1/20\n",
      "150/150 [==============================] - 0s 185us/step - loss: 9.2166 - acc: 0.5533 - val_loss: 9.7557 - val_acc: 0.5526\n",
      "Epoch 2/20\n",
      "150/150 [==============================] - 0s 265us/step - loss: 9.1698 - acc: 0.5200 - val_loss: 9.6970 - val_acc: 0.5526\n",
      "Epoch 3/20\n",
      "150/150 [==============================] - 0s 203us/step - loss: 9.0677 - acc: 0.6333 - val_loss: 9.6341 - val_acc: 0.5526\n",
      "Epoch 4/20\n",
      "150/150 [==============================] - 0s 217us/step - loss: 9.0662 - acc: 0.5733 - val_loss: 9.5749 - val_acc: 0.5526\n",
      "Epoch 5/20\n",
      "150/150 [==============================] - 0s 157us/step - loss: 8.9575 - acc: 0.5800 - val_loss: 9.5152 - val_acc: 0.5526\n",
      "Epoch 6/20\n",
      "150/150 [==============================] - 0s 295us/step - loss: 8.9395 - acc: 0.5800 - val_loss: 9.4553 - val_acc: 0.5526\n",
      "Epoch 7/20\n",
      "150/150 [==============================] - 0s 167us/step - loss: 8.8513 - acc: 0.5933 - val_loss: 9.4013 - val_acc: 0.5526\n",
      "Epoch 8/20\n",
      "150/150 [==============================] - 0s 274us/step - loss: 8.7738 - acc: 0.6133 - val_loss: 9.3419 - val_acc: 0.5263\n",
      "Epoch 9/20\n",
      "150/150 [==============================] - 0s 162us/step - loss: 8.7206 - acc: 0.6200 - val_loss: 9.2819 - val_acc: 0.5263\n",
      "Epoch 10/20\n",
      "150/150 [==============================] - 0s 268us/step - loss: 8.6525 - acc: 0.6000 - val_loss: 9.2252 - val_acc: 0.5000\n",
      "Epoch 11/20\n",
      "150/150 [==============================] - 0s 167us/step - loss: 8.6047 - acc: 0.6333 - val_loss: 9.1682 - val_acc: 0.5000\n",
      "Epoch 12/20\n",
      "150/150 [==============================] - 0s 213us/step - loss: 8.5383 - acc: 0.6267 - val_loss: 9.1097 - val_acc: 0.5000\n",
      "Epoch 13/20\n",
      "150/150 [==============================] - 0s 185us/step - loss: 8.5200 - acc: 0.6000 - val_loss: 9.0504 - val_acc: 0.5000\n",
      "Epoch 14/20\n",
      "150/150 [==============================] - 0s 151us/step - loss: 8.4295 - acc: 0.6133 - val_loss: 8.9934 - val_acc: 0.5000\n",
      "Epoch 15/20\n",
      "150/150 [==============================] - 0s 257us/step - loss: 8.4317 - acc: 0.5867 - val_loss: 8.9335 - val_acc: 0.5000\n",
      "Epoch 16/20\n",
      "150/150 [==============================] - 0s 158us/step - loss: 8.3252 - acc: 0.6333 - val_loss: 8.8762 - val_acc: 0.5263\n",
      "Epoch 17/20\n",
      "150/150 [==============================] - 0s 223us/step - loss: 8.2672 - acc: 0.5933 - val_loss: 8.8249 - val_acc: 0.5263\n",
      "Epoch 18/20\n",
      "150/150 [==============================] - 0s 199us/step - loss: 8.2452 - acc: 0.5333 - val_loss: 8.7705 - val_acc: 0.5526\n",
      "Epoch 19/20\n",
      "150/150 [==============================] - 0s 252us/step - loss: 8.1478 - acc: 0.6133 - val_loss: 8.7144 - val_acc: 0.5526\n",
      "Epoch 20/20\n",
      "150/150 [==============================] - 0s 210us/step - loss: 8.1354 - acc: 0.5600 - val_loss: 8.6626 - val_acc: 0.5526\n",
      "Train on 150 samples, validate on 38 samples\n",
      "Epoch 1/2\n",
      "150/150 [==============================] - 0s 181us/step - loss: 8.0792 - acc: 0.5467 - val_loss: 8.6078 - val_acc: 0.5263\n",
      "Epoch 2/2\n",
      "150/150 [==============================] - 0s 206us/step - loss: 7.9833 - acc: 0.6400 - val_loss: 8.5510 - val_acc: 0.5263\n",
      "Train on 150 samples, validate on 38 samples\n",
      "Epoch 1/2\n",
      "150/150 [==============================] - 0s 186us/step - loss: 7.9526 - acc: 0.6000 - val_loss: 8.4943 - val_acc: 0.6053\n",
      "Epoch 2/2\n",
      "150/150 [==============================] - 0s 265us/step - loss: 7.9133 - acc: 0.5133 - val_loss: 8.4387 - val_acc: 0.5789\n",
      "Train on 150 samples, validate on 38 samples\n",
      "Epoch 1/1\n",
      "150/150 [==============================] - 0s 167us/step - loss: 7.8564 - acc: 0.5667 - val_loss: 8.3858 - val_acc: 0.6053\n",
      "Train on 150 samples, validate on 38 samples\n",
      "Epoch 1/5\n",
      "150/150 [==============================] - 0s 260us/step - loss: 7.7911 - acc: 0.5600 - val_loss: 8.3307 - val_acc: 0.5789\n",
      "Epoch 2/5\n",
      "150/150 [==============================] - 0s 200us/step - loss: 7.7450 - acc: 0.6000 - val_loss: 8.2794 - val_acc: 0.6053\n",
      "Epoch 3/5\n",
      "150/150 [==============================] - 0s 243us/step - loss: 7.6807 - acc: 0.5667 - val_loss: 8.2304 - val_acc: 0.5789\n",
      "Epoch 4/5\n",
      "150/150 [==============================] - 0s 206us/step - loss: 7.6486 - acc: 0.6133 - val_loss: 8.1794 - val_acc: 0.5789\n",
      "Epoch 5/5\n",
      "150/150 [==============================] - 0s 195us/step - loss: 7.5785 - acc: 0.5933 - val_loss: 8.1274 - val_acc: 0.5789\n",
      "Train on 150 samples, validate on 38 samples\n",
      "Epoch 1/20\n",
      "150/150 [==============================] - 0s 237us/step - loss: 7.5050 - acc: 0.5800 - val_loss: 8.0698 - val_acc: 0.5526\n",
      "Epoch 2/20\n",
      "150/150 [==============================] - 0s 163us/step - loss: 7.4751 - acc: 0.6600 - val_loss: 8.0172 - val_acc: 0.5263\n",
      "Epoch 3/20\n",
      "150/150 [==============================] - 0s 264us/step - loss: 7.4261 - acc: 0.5667 - val_loss: 7.9666 - val_acc: 0.5263\n",
      "Epoch 4/20\n",
      "150/150 [==============================] - 0s 176us/step - loss: 7.3587 - acc: 0.6400 - val_loss: 7.9145 - val_acc: 0.4474\n",
      "Epoch 5/20\n",
      "150/150 [==============================] - 0s 165us/step - loss: 7.3269 - acc: 0.5600 - val_loss: 7.8627 - val_acc: 0.4474\n",
      "Epoch 6/20\n",
      "150/150 [==============================] - 0s 187us/step - loss: 7.2496 - acc: 0.6400 - val_loss: 7.8094 - val_acc: 0.4474\n",
      "Epoch 7/20\n",
      "150/150 [==============================] - 0s 212us/step - loss: 7.1859 - acc: 0.6533 - val_loss: 7.7568 - val_acc: 0.4737\n",
      "Epoch 8/20\n",
      "150/150 [==============================] - 0s 188us/step - loss: 7.1746 - acc: 0.5600 - val_loss: 7.7059 - val_acc: 0.5000\n",
      "Epoch 9/20\n",
      "150/150 [==============================] - 0s 186us/step - loss: 7.1100 - acc: 0.6000 - val_loss: 7.6557 - val_acc: 0.5000\n",
      "Epoch 10/20\n",
      "150/150 [==============================] - 0s 167us/step - loss: 7.0510 - acc: 0.6400 - val_loss: 7.6045 - val_acc: 0.5000\n",
      "Epoch 11/20\n",
      "150/150 [==============================] - 0s 175us/step - loss: 6.9892 - acc: 0.6267 - val_loss: 7.5527 - val_acc: 0.5000\n",
      "Epoch 12/20\n",
      "150/150 [==============================] - 0s 343us/step - loss: 6.9672 - acc: 0.6467 - val_loss: 7.5042 - val_acc: 0.5000\n",
      "Epoch 13/20\n",
      "150/150 [==============================] - 0s 391us/step - loss: 6.9190 - acc: 0.5667 - val_loss: 7.4541 - val_acc: 0.5000\n",
      "Epoch 14/20\n",
      "150/150 [==============================] - 0s 232us/step - loss: 6.8674 - acc: 0.6067 - val_loss: 7.4070 - val_acc: 0.5000\n",
      "Epoch 15/20\n",
      "150/150 [==============================] - 0s 250us/step - loss: 6.8350 - acc: 0.5533 - val_loss: 7.3573 - val_acc: 0.4737\n",
      "Epoch 16/20\n",
      "150/150 [==============================] - 0s 183us/step - loss: 6.7773 - acc: 0.6400 - val_loss: 7.3072 - val_acc: 0.4737\n",
      "Epoch 17/20\n",
      "150/150 [==============================] - 0s 177us/step - loss: 6.7308 - acc: 0.5867 - val_loss: 7.2565 - val_acc: 0.4737\n",
      "Epoch 18/20\n",
      "150/150 [==============================] - 0s 171us/step - loss: 6.6573 - acc: 0.6000 - val_loss: 7.2067 - val_acc: 0.4737\n",
      "Epoch 19/20\n",
      "150/150 [==============================] - 0s 177us/step - loss: 6.6042 - acc: 0.5600 - val_loss: 7.1588 - val_acc: 0.4737\n",
      "Epoch 20/20\n",
      "150/150 [==============================] - 0s 177us/step - loss: 6.5563 - acc: 0.5933 - val_loss: 7.1101 - val_acc: 0.4737\n",
      "Train on 150 samples, validate on 38 samples\n",
      "Epoch 1/40\n",
      "150/150 [==============================] - 0s 160us/step - loss: 6.5184 - acc: 0.6200 - val_loss: 7.0641 - val_acc: 0.4737\n",
      "Epoch 2/40\n",
      "150/150 [==============================] - 0s 181us/step - loss: 6.4789 - acc: 0.5533 - val_loss: 7.0194 - val_acc: 0.5000\n",
      "Epoch 3/40\n",
      "150/150 [==============================] - 0s 198us/step - loss: 6.4172 - acc: 0.6267 - val_loss: 6.9738 - val_acc: 0.5000\n",
      "Epoch 4/40\n",
      "150/150 [==============================] - 0s 195us/step - loss: 6.3807 - acc: 0.6000 - val_loss: 6.9280 - val_acc: 0.5000\n",
      "Epoch 5/40\n",
      "150/150 [==============================] - 0s 372us/step - loss: 6.3072 - acc: 0.6400 - val_loss: 6.8815 - val_acc: 0.5000\n",
      "Epoch 6/40\n",
      "150/150 [==============================] - 0s 229us/step - loss: 6.2907 - acc: 0.5600 - val_loss: 6.8403 - val_acc: 0.5000\n",
      "Epoch 7/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150/150 [==============================] - 0s 275us/step - loss: 6.2212 - acc: 0.6333 - val_loss: 6.7950 - val_acc: 0.5000\n",
      "Epoch 8/40\n",
      "150/150 [==============================] - 0s 347us/step - loss: 6.1811 - acc: 0.5467 - val_loss: 6.7477 - val_acc: 0.5000\n",
      "Epoch 9/40\n",
      "150/150 [==============================] - 0s 357us/step - loss: 6.1381 - acc: 0.6733 - val_loss: 6.7011 - val_acc: 0.5000\n",
      "Epoch 10/40\n",
      "150/150 [==============================] - 0s 249us/step - loss: 6.0795 - acc: 0.6067 - val_loss: 6.6563 - val_acc: 0.5000\n",
      "Epoch 11/40\n",
      "150/150 [==============================] - 0s 387us/step - loss: 6.0234 - acc: 0.6733 - val_loss: 6.6088 - val_acc: 0.5000\n",
      "Epoch 12/40\n",
      "150/150 [==============================] - 0s 298us/step - loss: 5.9916 - acc: 0.6000 - val_loss: 6.5645 - val_acc: 0.5000\n",
      "Epoch 13/40\n",
      "150/150 [==============================] - 0s 347us/step - loss: 5.9533 - acc: 0.6000 - val_loss: 6.5214 - val_acc: 0.5000\n",
      "Epoch 14/40\n",
      "150/150 [==============================] - 0s 358us/step - loss: 5.9339 - acc: 0.6067 - val_loss: 6.4804 - val_acc: 0.5000\n",
      "Epoch 15/40\n",
      "150/150 [==============================] - 0s 187us/step - loss: 5.8732 - acc: 0.6467 - val_loss: 6.4363 - val_acc: 0.5000\n",
      "Epoch 16/40\n",
      "150/150 [==============================] - 0s 249us/step - loss: 5.8010 - acc: 0.6067 - val_loss: 6.3946 - val_acc: 0.5000\n",
      "Epoch 17/40\n",
      "150/150 [==============================] - 0s 209us/step - loss: 5.8002 - acc: 0.5600 - val_loss: 6.3500 - val_acc: 0.5000\n",
      "Epoch 18/40\n",
      "150/150 [==============================] - 0s 204us/step - loss: 5.7240 - acc: 0.6333 - val_loss: 6.3103 - val_acc: 0.5000\n",
      "Epoch 19/40\n",
      "150/150 [==============================] - 0s 201us/step - loss: 5.6869 - acc: 0.6067 - val_loss: 6.2740 - val_acc: 0.5000\n",
      "Epoch 20/40\n",
      "150/150 [==============================] - ETA: 0s - loss: 5.6135 - acc: 0.609 - 0s 171us/step - loss: 5.6385 - acc: 0.6133 - val_loss: 6.2364 - val_acc: 0.5000\n",
      "Epoch 21/40\n",
      "150/150 [==============================] - ETA: 0s - loss: 5.5605 - acc: 0.593 - 0s 196us/step - loss: 5.5821 - acc: 0.6067 - val_loss: 6.1965 - val_acc: 0.4737\n",
      "Epoch 22/40\n",
      "150/150 [==============================] - 0s 217us/step - loss: 5.5701 - acc: 0.6667 - val_loss: 6.1586 - val_acc: 0.4737\n",
      "Epoch 23/40\n",
      "150/150 [==============================] - 0s 197us/step - loss: 5.5152 - acc: 0.6600 - val_loss: 6.1212 - val_acc: 0.4737\n",
      "Epoch 24/40\n",
      "150/150 [==============================] - 0s 204us/step - loss: 5.4503 - acc: 0.7000 - val_loss: 6.0819 - val_acc: 0.5000\n",
      "Epoch 25/40\n",
      "150/150 [==============================] - 0s 201us/step - loss: 5.4104 - acc: 0.5667 - val_loss: 6.0418 - val_acc: 0.5000\n",
      "Epoch 26/40\n",
      "150/150 [==============================] - 0s 200us/step - loss: 5.3712 - acc: 0.6533 - val_loss: 5.9990 - val_acc: 0.5000\n",
      "Epoch 27/40\n",
      "150/150 [==============================] - 0s 245us/step - loss: 5.3328 - acc: 0.6467 - val_loss: 5.9548 - val_acc: 0.5000\n",
      "Epoch 28/40\n",
      "150/150 [==============================] - 0s 202us/step - loss: 5.2836 - acc: 0.6333 - val_loss: 5.9112 - val_acc: 0.5000\n",
      "Epoch 29/40\n",
      "150/150 [==============================] - 0s 219us/step - loss: 5.2816 - acc: 0.5467 - val_loss: 5.8708 - val_acc: 0.5000\n",
      "Epoch 30/40\n",
      "150/150 [==============================] - 0s 220us/step - loss: 5.2010 - acc: 0.6133 - val_loss: 5.8270 - val_acc: 0.5000\n",
      "Epoch 31/40\n",
      "150/150 [==============================] - 0s 203us/step - loss: 5.1792 - acc: 0.6267 - val_loss: 5.7866 - val_acc: 0.5000\n",
      "Epoch 32/40\n",
      "150/150 [==============================] - 0s 250us/step - loss: 5.1326 - acc: 0.6133 - val_loss: 5.7466 - val_acc: 0.5000\n",
      "Epoch 33/40\n",
      "150/150 [==============================] - 0s 182us/step - loss: 5.0730 - acc: 0.6867 - val_loss: 5.7056 - val_acc: 0.5000\n",
      "Epoch 34/40\n",
      "150/150 [==============================] - 0s 212us/step - loss: 5.0231 - acc: 0.6267 - val_loss: 5.6626 - val_acc: 0.5000\n",
      "Epoch 35/40\n",
      "150/150 [==============================] - 0s 191us/step - loss: 4.9830 - acc: 0.6933 - val_loss: 5.6262 - val_acc: 0.5000\n",
      "Epoch 36/40\n",
      "150/150 [==============================] - 0s 204us/step - loss: 4.9588 - acc: 0.6400 - val_loss: 5.5859 - val_acc: 0.5000\n",
      "Epoch 37/40\n",
      "150/150 [==============================] - 0s 227us/step - loss: 4.9067 - acc: 0.5467 - val_loss: 5.5442 - val_acc: 0.5000\n",
      "Epoch 38/40\n",
      "150/150 [==============================] - 0s 164us/step - loss: 4.8921 - acc: 0.6333 - val_loss: 5.5064 - val_acc: 0.5000\n",
      "Epoch 39/40\n",
      "150/150 [==============================] - 0s 222us/step - loss: 4.8034 - acc: 0.6733 - val_loss: 5.4664 - val_acc: 0.5000\n",
      "Epoch 40/40\n",
      "150/150 [==============================] - 0s 175us/step - loss: 4.8101 - acc: 0.6533 - val_loss: 5.4300 - val_acc: 0.5000\n",
      "Train on 150 samples, validate on 38 samples\n",
      "Epoch 1/80\n",
      "150/150 [==============================] - 0s 159us/step - loss: 4.7628 - acc: 0.6400 - val_loss: 5.3908 - val_acc: 0.5000\n",
      "Epoch 2/80\n",
      "150/150 [==============================] - 0s 167us/step - loss: 4.7252 - acc: 0.6200 - val_loss: 5.3540 - val_acc: 0.5000\n",
      "Epoch 3/80\n",
      "150/150 [==============================] - 0s 207us/step - loss: 4.6852 - acc: 0.6600 - val_loss: 5.3166 - val_acc: 0.5000\n",
      "Epoch 4/80\n",
      "150/150 [==============================] - 0s 206us/step - loss: 4.6380 - acc: 0.6533 - val_loss: 5.2807 - val_acc: 0.5000\n",
      "Epoch 5/80\n",
      "150/150 [==============================] - 0s 319us/step - loss: 4.5982 - acc: 0.6133 - val_loss: 5.2473 - val_acc: 0.5000\n",
      "Epoch 6/80\n",
      "150/150 [==============================] - 0s 340us/step - loss: 4.5707 - acc: 0.6067 - val_loss: 5.2110 - val_acc: 0.5000\n",
      "Epoch 7/80\n",
      "150/150 [==============================] - 0s 325us/step - loss: 4.5066 - acc: 0.7000 - val_loss: 5.1772 - val_acc: 0.5000\n",
      "Epoch 8/80\n",
      "150/150 [==============================] - 0s 262us/step - loss: 4.5053 - acc: 0.7000 - val_loss: 5.1435 - val_acc: 0.5000\n",
      "Epoch 9/80\n",
      "150/150 [==============================] - 0s 290us/step - loss: 4.4824 - acc: 0.5867 - val_loss: 5.1049 - val_acc: 0.5000\n",
      "Epoch 10/80\n",
      "150/150 [==============================] - 0s 287us/step - loss: 4.3896 - acc: 0.6800 - val_loss: 5.0667 - val_acc: 0.5000\n",
      "Epoch 11/80\n",
      "150/150 [==============================] - 0s 254us/step - loss: 4.3595 - acc: 0.6600 - val_loss: 5.0291 - val_acc: 0.5000\n",
      "Epoch 12/80\n",
      "150/150 [==============================] - 0s 241us/step - loss: 4.3398 - acc: 0.6867 - val_loss: 4.9904 - val_acc: 0.5000\n",
      "Epoch 13/80\n",
      "150/150 [==============================] - 0s 349us/step - loss: 4.2911 - acc: 0.6533 - val_loss: 4.9555 - val_acc: 0.5000\n",
      "Epoch 14/80\n",
      "150/150 [==============================] - 0s 391us/step - loss: 4.2491 - acc: 0.6933 - val_loss: 4.9131 - val_acc: 0.5000\n",
      "Epoch 15/80\n",
      "150/150 [==============================] - 0s 296us/step - loss: 4.2002 - acc: 0.7267 - val_loss: 4.8741 - val_acc: 0.5000\n",
      "Epoch 16/80\n",
      "150/150 [==============================] - 0s 292us/step - loss: 4.1773 - acc: 0.6733 - val_loss: 4.8367 - val_acc: 0.5000\n",
      "Epoch 17/80\n",
      "150/150 [==============================] - 0s 273us/step - loss: 4.1514 - acc: 0.6267 - val_loss: 4.7981 - val_acc: 0.5000\n",
      "Epoch 18/80\n",
      "150/150 [==============================] - 0s 286us/step - loss: 4.0997 - acc: 0.7000 - val_loss: 4.7600 - val_acc: 0.5000\n",
      "Epoch 19/80\n",
      "150/150 [==============================] - 0s 316us/step - loss: 4.0758 - acc: 0.6533 - val_loss: 4.7256 - val_acc: 0.5000\n",
      "Epoch 20/80\n",
      "150/150 [==============================] - 0s 435us/step - loss: 4.0388 - acc: 0.6333 - val_loss: 4.6918 - val_acc: 0.5000\n",
      "Epoch 21/80\n",
      "150/150 [==============================] - 0s 396us/step - loss: 3.9945 - acc: 0.7067 - val_loss: 4.6559 - val_acc: 0.5000\n",
      "Epoch 22/80\n",
      "150/150 [==============================] - 0s 294us/step - loss: 3.9761 - acc: 0.6133 - val_loss: 4.6198 - val_acc: 0.5000\n",
      "Epoch 23/80\n",
      "150/150 [==============================] - 0s 309us/step - loss: 3.9344 - acc: 0.6733 - val_loss: 4.5850 - val_acc: 0.5000\n",
      "Epoch 24/80\n",
      "150/150 [==============================] - 0s 401us/step - loss: 3.8929 - acc: 0.6533 - val_loss: 4.5534 - val_acc: 0.5000\n",
      "Epoch 25/80\n",
      "150/150 [==============================] - 0s 295us/step - loss: 3.8609 - acc: 0.6133 - val_loss: 4.5186 - val_acc: 0.5000\n",
      "Epoch 26/80\n",
      "150/150 [==============================] - 0s 315us/step - loss: 3.8245 - acc: 0.6533 - val_loss: 4.4826 - val_acc: 0.5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/80\n",
      "150/150 [==============================] - 0s 350us/step - loss: 3.7872 - acc: 0.7133 - val_loss: 4.4466 - val_acc: 0.5000\n",
      "Epoch 28/80\n",
      "150/150 [==============================] - 0s 452us/step - loss: 3.7513 - acc: 0.6600 - val_loss: 4.4139 - val_acc: 0.5000\n",
      "Epoch 29/80\n",
      "150/150 [==============================] - 0s 295us/step - loss: 3.7323 - acc: 0.6333 - val_loss: 4.3807 - val_acc: 0.5000\n",
      "Epoch 30/80\n",
      "150/150 [==============================] - 0s 308us/step - loss: 3.6945 - acc: 0.6533 - val_loss: 4.3487 - val_acc: 0.5000\n",
      "Epoch 31/80\n",
      "150/150 [==============================] - 0s 348us/step - loss: 3.6654 - acc: 0.6800 - val_loss: 4.3133 - val_acc: 0.5000\n",
      "Epoch 32/80\n",
      "150/150 [==============================] - 0s 263us/step - loss: 3.6176 - acc: 0.6667 - val_loss: 4.2788 - val_acc: 0.5000\n",
      "Epoch 33/80\n",
      "150/150 [==============================] - 0s 381us/step - loss: 3.5841 - acc: 0.6467 - val_loss: 4.2475 - val_acc: 0.5000\n",
      "Epoch 34/80\n",
      "150/150 [==============================] - 0s 263us/step - loss: 3.5623 - acc: 0.6733 - val_loss: 4.2155 - val_acc: 0.4737\n",
      "Epoch 35/80\n",
      "150/150 [==============================] - 0s 255us/step - loss: 3.5222 - acc: 0.6733 - val_loss: 4.1846 - val_acc: 0.4737\n",
      "Epoch 36/80\n",
      "150/150 [==============================] - 0s 229us/step - loss: 3.4840 - acc: 0.6667 - val_loss: 4.1532 - val_acc: 0.4737\n",
      "Epoch 37/80\n",
      "150/150 [==============================] - 0s 325us/step - loss: 3.4391 - acc: 0.6867 - val_loss: 4.1227 - val_acc: 0.4737\n",
      "Epoch 38/80\n",
      "150/150 [==============================] - 0s 389us/step - loss: 3.4126 - acc: 0.7533 - val_loss: 4.0940 - val_acc: 0.4737\n",
      "Epoch 39/80\n",
      "150/150 [==============================] - 0s 334us/step - loss: 3.3790 - acc: 0.7000 - val_loss: 4.0647 - val_acc: 0.4737\n",
      "Epoch 40/80\n",
      "150/150 [==============================] - 0s 313us/step - loss: 3.3489 - acc: 0.7200 - val_loss: 4.0331 - val_acc: 0.4737\n",
      "Epoch 41/80\n",
      "150/150 [==============================] - 0s 273us/step - loss: 3.3380 - acc: 0.6667 - val_loss: 4.0017 - val_acc: 0.4737\n",
      "Epoch 42/80\n",
      "150/150 [==============================] - 0s 294us/step - loss: 3.3181 - acc: 0.6600 - val_loss: 3.9746 - val_acc: 0.4737\n",
      "Epoch 43/80\n",
      "150/150 [==============================] - 0s 233us/step - loss: 3.2586 - acc: 0.7200 - val_loss: 3.9436 - val_acc: 0.4737\n",
      "Epoch 44/80\n",
      "150/150 [==============================] - 0s 275us/step - loss: 3.2304 - acc: 0.6867 - val_loss: 3.9172 - val_acc: 0.4737\n",
      "Epoch 45/80\n",
      "150/150 [==============================] - 0s 267us/step - loss: 3.2142 - acc: 0.6733 - val_loss: 3.8878 - val_acc: 0.4737\n",
      "Epoch 46/80\n",
      "150/150 [==============================] - 0s 272us/step - loss: 3.1619 - acc: 0.6800 - val_loss: 3.8604 - val_acc: 0.4737\n",
      "Epoch 47/80\n",
      "150/150 [==============================] - 0s 260us/step - loss: 3.1228 - acc: 0.7267 - val_loss: 3.8306 - val_acc: 0.4737\n",
      "Epoch 48/80\n",
      "150/150 [==============================] - 0s 351us/step - loss: 3.1148 - acc: 0.7133 - val_loss: 3.8000 - val_acc: 0.4737\n",
      "Epoch 49/80\n",
      "150/150 [==============================] - 0s 299us/step - loss: 3.0515 - acc: 0.7800 - val_loss: 3.7691 - val_acc: 0.4737\n",
      "Epoch 50/80\n",
      "150/150 [==============================] - 0s 478us/step - loss: 3.0426 - acc: 0.7267 - val_loss: 3.7395 - val_acc: 0.4737\n",
      "Epoch 51/80\n",
      "150/150 [==============================] - 0s 421us/step - loss: 3.0140 - acc: 0.6733 - val_loss: 3.7105 - val_acc: 0.4737\n",
      "Epoch 52/80\n",
      "150/150 [==============================] - 0s 342us/step - loss: 3.0010 - acc: 0.6933 - val_loss: 3.6845 - val_acc: 0.4737\n",
      "Epoch 53/80\n",
      "150/150 [==============================] - 0s 316us/step - loss: 2.9587 - acc: 0.6933 - val_loss: 3.6579 - val_acc: 0.4737\n",
      "Epoch 54/80\n",
      "150/150 [==============================] - 0s 418us/step - loss: 2.9130 - acc: 0.7400 - val_loss: 3.6282 - val_acc: 0.4737\n",
      "Epoch 55/80\n",
      "150/150 [==============================] - 0s 268us/step - loss: 2.9142 - acc: 0.6933 - val_loss: 3.5950 - val_acc: 0.4737\n",
      "Epoch 56/80\n",
      "150/150 [==============================] - 0s 270us/step - loss: 2.8413 - acc: 0.7867 - val_loss: 3.5636 - val_acc: 0.4737\n",
      "Epoch 57/80\n",
      "150/150 [==============================] - 0s 303us/step - loss: 2.8537 - acc: 0.6867 - val_loss: 3.5381 - val_acc: 0.4737\n",
      "Epoch 58/80\n",
      "150/150 [==============================] - 0s 256us/step - loss: 2.8285 - acc: 0.7067 - val_loss: 3.5132 - val_acc: 0.4737\n",
      "Epoch 59/80\n",
      "150/150 [==============================] - 0s 290us/step - loss: 2.7738 - acc: 0.7000 - val_loss: 3.4874 - val_acc: 0.4737\n",
      "Epoch 60/80\n",
      "150/150 [==============================] - 0s 284us/step - loss: 2.7699 - acc: 0.7067 - val_loss: 3.4619 - val_acc: 0.4737\n",
      "Epoch 61/80\n",
      "150/150 [==============================] - 0s 290us/step - loss: 2.7032 - acc: 0.7600 - val_loss: 3.4410 - val_acc: 0.4737\n",
      "Epoch 62/80\n",
      "150/150 [==============================] - 0s 291us/step - loss: 2.7013 - acc: 0.7533 - val_loss: 3.4115 - val_acc: 0.4737\n",
      "Epoch 63/80\n",
      "150/150 [==============================] - 0s 245us/step - loss: 2.6995 - acc: 0.6733 - val_loss: 3.3848 - val_acc: 0.4737\n",
      "Epoch 64/80\n",
      "150/150 [==============================] - 0s 262us/step - loss: 2.6609 - acc: 0.7000 - val_loss: 3.3572 - val_acc: 0.4737\n",
      "Epoch 65/80\n",
      "150/150 [==============================] - 0s 196us/step - loss: 2.6182 - acc: 0.7133 - val_loss: 3.3274 - val_acc: 0.4737\n",
      "Epoch 66/80\n",
      "150/150 [==============================] - 0s 206us/step - loss: 2.5572 - acc: 0.7600 - val_loss: 3.3046 - val_acc: 0.4737\n",
      "Epoch 67/80\n",
      "150/150 [==============================] - 0s 245us/step - loss: 2.5381 - acc: 0.7667 - val_loss: 3.2817 - val_acc: 0.4737\n",
      "Epoch 68/80\n",
      "150/150 [==============================] - 0s 297us/step - loss: 2.5323 - acc: 0.7400 - val_loss: 3.2650 - val_acc: 0.4737\n",
      "Epoch 69/80\n",
      "150/150 [==============================] - 0s 225us/step - loss: 2.5139 - acc: 0.7267 - val_loss: 3.2454 - val_acc: 0.4737\n",
      "Epoch 70/80\n",
      "150/150 [==============================] - 0s 239us/step - loss: 2.4640 - acc: 0.7467 - val_loss: 3.2251 - val_acc: 0.4737\n",
      "Epoch 71/80\n",
      "150/150 [==============================] - 0s 318us/step - loss: 2.4486 - acc: 0.7267 - val_loss: 3.2035 - val_acc: 0.4737\n",
      "Epoch 72/80\n",
      "150/150 [==============================] - 0s 287us/step - loss: 2.4290 - acc: 0.7400 - val_loss: 3.1764 - val_acc: 0.4737\n",
      "Epoch 73/80\n",
      "150/150 [==============================] - 0s 276us/step - loss: 2.4073 - acc: 0.7400 - val_loss: 3.1530 - val_acc: 0.4737\n",
      "Epoch 74/80\n",
      "150/150 [==============================] - 0s 242us/step - loss: 2.3763 - acc: 0.7467 - val_loss: 3.1269 - val_acc: 0.4737\n",
      "Epoch 75/80\n",
      "150/150 [==============================] - 0s 285us/step - loss: 2.3338 - acc: 0.7733 - val_loss: 3.1032 - val_acc: 0.4737\n",
      "Epoch 76/80\n",
      "150/150 [==============================] - 0s 253us/step - loss: 2.3237 - acc: 0.7400 - val_loss: 3.0800 - val_acc: 0.4737\n",
      "Epoch 77/80\n",
      "150/150 [==============================] - 0s 267us/step - loss: 2.2963 - acc: 0.7133 - val_loss: 3.0599 - val_acc: 0.4737\n",
      "Epoch 78/80\n",
      "150/150 [==============================] - 0s 275us/step - loss: 2.2667 - acc: 0.7600 - val_loss: 3.0373 - val_acc: 0.4737\n",
      "Epoch 79/80\n",
      "150/150 [==============================] - 0s 308us/step - loss: 2.2350 - acc: 0.7667 - val_loss: 3.0153 - val_acc: 0.4737\n",
      "Epoch 80/80\n",
      "150/150 [==============================] - 0s 301us/step - loss: 2.2333 - acc: 0.7333 - val_loss: 2.9887 - val_acc: 0.4737\n",
      "Train on 150 samples, validate on 38 samples\n",
      "Epoch 1/120\n",
      "150/150 [==============================] - 0s 235us/step - loss: 2.2050 - acc: 0.7867 - val_loss: 2.9638 - val_acc: 0.4737\n",
      "Epoch 2/120\n",
      "150/150 [==============================] - 0s 245us/step - loss: 2.1786 - acc: 0.7333 - val_loss: 2.9416 - val_acc: 0.4737\n",
      "Epoch 3/120\n",
      "150/150 [==============================] - 0s 286us/step - loss: 2.1538 - acc: 0.7733 - val_loss: 2.9202 - val_acc: 0.4737\n",
      "Epoch 4/120\n",
      "150/150 [==============================] - 0s 210us/step - loss: 2.1236 - acc: 0.7800 - val_loss: 2.8974 - val_acc: 0.4737\n",
      "Epoch 5/120\n",
      "150/150 [==============================] - 0s 270us/step - loss: 2.1012 - acc: 0.7800 - val_loss: 2.8758 - val_acc: 0.4737\n",
      "Epoch 6/120\n",
      "150/150 [==============================] - 0s 279us/step - loss: 2.0821 - acc: 0.7467 - val_loss: 2.8584 - val_acc: 0.4737\n",
      "Epoch 7/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150/150 [==============================] - 0s 201us/step - loss: 2.0555 - acc: 0.7333 - val_loss: 2.8428 - val_acc: 0.4737\n",
      "Epoch 8/120\n",
      "150/150 [==============================] - 0s 245us/step - loss: 2.0477 - acc: 0.7667 - val_loss: 2.8184 - val_acc: 0.4737\n",
      "Epoch 9/120\n",
      "150/150 [==============================] - 0s 281us/step - loss: 2.0263 - acc: 0.7533 - val_loss: 2.7931 - val_acc: 0.4737\n",
      "Epoch 10/120\n",
      "150/150 [==============================] - 0s 231us/step - loss: 1.9877 - acc: 0.7733 - val_loss: 2.7737 - val_acc: 0.4737\n",
      "Epoch 11/120\n",
      "150/150 [==============================] - ETA: 0s - loss: 1.9751 - acc: 0.703 - 0s 297us/step - loss: 1.9661 - acc: 0.7200 - val_loss: 2.7608 - val_acc: 0.4737\n",
      "Epoch 12/120\n",
      "150/150 [==============================] - 0s 324us/step - loss: 1.9296 - acc: 0.7867 - val_loss: 2.7491 - val_acc: 0.4737\n",
      "Epoch 13/120\n",
      "150/150 [==============================] - 0s 245us/step - loss: 1.9250 - acc: 0.7667 - val_loss: 2.7254 - val_acc: 0.4737\n",
      "Epoch 14/120\n",
      "150/150 [==============================] - 0s 332us/step - loss: 1.9195 - acc: 0.7533 - val_loss: 2.7085 - val_acc: 0.4737\n",
      "Epoch 15/120\n",
      "150/150 [==============================] - 0s 258us/step - loss: 1.9091 - acc: 0.7333 - val_loss: 2.6900 - val_acc: 0.4737\n",
      "Epoch 16/120\n",
      "150/150 [==============================] - 0s 294us/step - loss: 1.8543 - acc: 0.7533 - val_loss: 2.6713 - val_acc: 0.4737\n",
      "Epoch 17/120\n",
      "150/150 [==============================] - 0s 266us/step - loss: 1.8461 - acc: 0.7733 - val_loss: 2.6571 - val_acc: 0.4737\n",
      "Epoch 18/120\n",
      "150/150 [==============================] - 0s 262us/step - loss: 1.8318 - acc: 0.7733 - val_loss: 2.6383 - val_acc: 0.4737\n",
      "Epoch 19/120\n",
      "150/150 [==============================] - 0s 228us/step - loss: 1.7788 - acc: 0.8067 - val_loss: 2.6155 - val_acc: 0.4737\n",
      "Epoch 20/120\n",
      "150/150 [==============================] - 0s 299us/step - loss: 1.7626 - acc: 0.8200 - val_loss: 2.5916 - val_acc: 0.4737\n",
      "Epoch 21/120\n",
      "150/150 [==============================] - 0s 257us/step - loss: 1.7548 - acc: 0.7400 - val_loss: 2.5678 - val_acc: 0.4737\n",
      "Epoch 22/120\n",
      "150/150 [==============================] - 0s 313us/step - loss: 1.7417 - acc: 0.7533 - val_loss: 2.5523 - val_acc: 0.5000\n",
      "Epoch 23/120\n",
      "150/150 [==============================] - 0s 215us/step - loss: 1.7248 - acc: 0.7533 - val_loss: 2.5326 - val_acc: 0.5000\n",
      "Epoch 24/120\n",
      "150/150 [==============================] - 0s 270us/step - loss: 1.6971 - acc: 0.7800 - val_loss: 2.5131 - val_acc: 0.5000\n",
      "Epoch 25/120\n",
      "150/150 [==============================] - 0s 185us/step - loss: 1.6793 - acc: 0.7600 - val_loss: 2.4985 - val_acc: 0.5000\n",
      "Epoch 26/120\n",
      "150/150 [==============================] - 0s 264us/step - loss: 1.6499 - acc: 0.7867 - val_loss: 2.4817 - val_acc: 0.5000\n",
      "Epoch 27/120\n",
      "150/150 [==============================] - 0s 296us/step - loss: 1.6470 - acc: 0.7533 - val_loss: 2.4613 - val_acc: 0.5000\n",
      "Epoch 28/120\n",
      "150/150 [==============================] - 0s 244us/step - loss: 1.6084 - acc: 0.7733 - val_loss: 2.4498 - val_acc: 0.5000\n",
      "Epoch 29/120\n",
      "150/150 [==============================] - 0s 248us/step - loss: 1.6130 - acc: 0.7467 - val_loss: 2.4466 - val_acc: 0.4737\n",
      "Epoch 30/120\n",
      "150/150 [==============================] - 0s 279us/step - loss: 1.5926 - acc: 0.7867 - val_loss: 2.4325 - val_acc: 0.4737\n",
      "Epoch 31/120\n",
      "150/150 [==============================] - 0s 277us/step - loss: 1.5683 - acc: 0.7333 - val_loss: 2.4290 - val_acc: 0.4737\n",
      "Epoch 32/120\n",
      "150/150 [==============================] - 0s 281us/step - loss: 1.5374 - acc: 0.7533 - val_loss: 2.4170 - val_acc: 0.4737\n",
      "Epoch 33/120\n",
      "150/150 [==============================] - 0s 235us/step - loss: 1.5573 - acc: 0.7467 - val_loss: 2.4013 - val_acc: 0.4737\n",
      "Epoch 34/120\n",
      "150/150 [==============================] - 0s 235us/step - loss: 1.5213 - acc: 0.7733 - val_loss: 2.3842 - val_acc: 0.4737\n",
      "Epoch 35/120\n",
      "150/150 [==============================] - 0s 372us/step - loss: 1.4850 - acc: 0.7867 - val_loss: 2.3634 - val_acc: 0.4737\n",
      "Epoch 36/120\n",
      "150/150 [==============================] - 0s 336us/step - loss: 1.4874 - acc: 0.8067 - val_loss: 2.3453 - val_acc: 0.4737\n",
      "Epoch 37/120\n",
      "150/150 [==============================] - 0s 339us/step - loss: 1.4600 - acc: 0.8200 - val_loss: 2.3234 - val_acc: 0.4737\n",
      "Epoch 38/120\n",
      "150/150 [==============================] - 0s 403us/step - loss: 1.4479 - acc: 0.7200 - val_loss: 2.3026 - val_acc: 0.5000\n",
      "Epoch 39/120\n",
      "150/150 [==============================] - 0s 342us/step - loss: 1.4465 - acc: 0.7867 - val_loss: 2.2900 - val_acc: 0.5000\n",
      "Epoch 40/120\n",
      "150/150 [==============================] - 0s 415us/step - loss: 1.4038 - acc: 0.7867 - val_loss: 2.2705 - val_acc: 0.5000\n",
      "Epoch 41/120\n",
      "150/150 [==============================] - 0s 376us/step - loss: 1.4021 - acc: 0.7933 - val_loss: 2.2535 - val_acc: 0.5000\n",
      "Epoch 42/120\n",
      "150/150 [==============================] - 0s 278us/step - loss: 1.3919 - acc: 0.7533 - val_loss: 2.2368 - val_acc: 0.4737\n",
      "Epoch 43/120\n",
      "150/150 [==============================] - 0s 234us/step - loss: 1.3646 - acc: 0.7867 - val_loss: 2.2233 - val_acc: 0.4737\n",
      "Epoch 44/120\n",
      "150/150 [==============================] - 0s 206us/step - loss: 1.3380 - acc: 0.7800 - val_loss: 2.2121 - val_acc: 0.4737\n",
      "Epoch 45/120\n",
      "150/150 [==============================] - 0s 251us/step - loss: 1.3521 - acc: 0.7667 - val_loss: 2.1902 - val_acc: 0.5000\n",
      "Epoch 46/120\n",
      "150/150 [==============================] - 0s 255us/step - loss: 1.3208 - acc: 0.8067 - val_loss: 2.1672 - val_acc: 0.5000\n",
      "Epoch 47/120\n",
      "150/150 [==============================] - 0s 278us/step - loss: 1.2901 - acc: 0.8133 - val_loss: 2.1558 - val_acc: 0.5000\n",
      "Epoch 48/120\n",
      "150/150 [==============================] - 0s 236us/step - loss: 1.2942 - acc: 0.7667 - val_loss: 2.1372 - val_acc: 0.5000\n",
      "Epoch 49/120\n",
      "150/150 [==============================] - 0s 211us/step - loss: 1.2664 - acc: 0.7600 - val_loss: 2.1192 - val_acc: 0.5000\n",
      "Epoch 50/120\n",
      "150/150 [==============================] - 0s 262us/step - loss: 1.2726 - acc: 0.7600 - val_loss: 2.1028 - val_acc: 0.5000\n",
      "Epoch 51/120\n",
      "150/150 [==============================] - 0s 320us/step - loss: 1.2295 - acc: 0.7867 - val_loss: 2.0876 - val_acc: 0.5000\n",
      "Epoch 52/120\n",
      "150/150 [==============================] - 0s 300us/step - loss: 1.2262 - acc: 0.7933 - val_loss: 2.0628 - val_acc: 0.5000\n",
      "Epoch 53/120\n",
      "150/150 [==============================] - 0s 256us/step - loss: 1.2160 - acc: 0.7600 - val_loss: 2.0680 - val_acc: 0.5000\n",
      "Epoch 54/120\n",
      "150/150 [==============================] - 0s 368us/step - loss: 1.2030 - acc: 0.7933 - val_loss: 2.0746 - val_acc: 0.5000\n",
      "Epoch 55/120\n",
      "150/150 [==============================] - 0s 464us/step - loss: 1.1822 - acc: 0.8133 - val_loss: 2.0542 - val_acc: 0.5000\n",
      "Epoch 56/120\n",
      "150/150 [==============================] - 0s 419us/step - loss: 1.1575 - acc: 0.8000 - val_loss: 2.0337 - val_acc: 0.5000\n",
      "Epoch 57/120\n",
      "150/150 [==============================] - 0s 362us/step - loss: 1.1680 - acc: 0.7667 - val_loss: 2.0043 - val_acc: 0.5000\n",
      "Epoch 58/120\n",
      "150/150 [==============================] - 0s 325us/step - loss: 1.1600 - acc: 0.7600 - val_loss: 1.9906 - val_acc: 0.5000\n",
      "Epoch 59/120\n",
      "150/150 [==============================] - 0s 360us/step - loss: 1.1414 - acc: 0.7467 - val_loss: 1.9736 - val_acc: 0.5000\n",
      "Epoch 60/120\n",
      "150/150 [==============================] - 0s 333us/step - loss: 1.1373 - acc: 0.7467 - val_loss: 1.9436 - val_acc: 0.5000\n",
      "Epoch 61/120\n",
      "150/150 [==============================] - 0s 337us/step - loss: 1.1470 - acc: 0.6867 - val_loss: 1.9191 - val_acc: 0.5000\n",
      "Epoch 62/120\n",
      "150/150 [==============================] - 0s 277us/step - loss: 1.0994 - acc: 0.7400 - val_loss: 1.9063 - val_acc: 0.5000\n",
      "Epoch 63/120\n",
      "150/150 [==============================] - 0s 271us/step - loss: 1.0942 - acc: 0.7733 - val_loss: 1.8877 - val_acc: 0.5000\n",
      "Epoch 64/120\n",
      "150/150 [==============================] - 0s 390us/step - loss: 1.1061 - acc: 0.6800 - val_loss: 1.8835 - val_acc: 0.5000\n",
      "Epoch 65/120\n",
      "150/150 [==============================] - 0s 292us/step - loss: 1.0698 - acc: 0.8133 - val_loss: 1.8757 - val_acc: 0.5000\n",
      "Epoch 66/120\n",
      "150/150 [==============================] - 0s 313us/step - loss: 1.0571 - acc: 0.8400 - val_loss: 1.8667 - val_acc: 0.5000\n",
      "Epoch 67/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150/150 [==============================] - 0s 405us/step - loss: 1.0232 - acc: 0.8133 - val_loss: 1.8576 - val_acc: 0.5000\n",
      "Epoch 68/120\n",
      "150/150 [==============================] - 0s 345us/step - loss: 1.0267 - acc: 0.7467 - val_loss: 1.8423 - val_acc: 0.5000\n",
      "Epoch 69/120\n",
      "150/150 [==============================] - 0s 448us/step - loss: 1.0129 - acc: 0.8067 - val_loss: 1.8349 - val_acc: 0.5000\n",
      "Epoch 70/120\n",
      "150/150 [==============================] - 0s 280us/step - loss: 0.9898 - acc: 0.8133 - val_loss: 1.8291 - val_acc: 0.5000\n",
      "Epoch 71/120\n",
      "150/150 [==============================] - 0s 292us/step - loss: 0.9843 - acc: 0.8067 - val_loss: 1.8187 - val_acc: 0.5000\n",
      "Epoch 72/120\n",
      "150/150 [==============================] - 0s 340us/step - loss: 0.9895 - acc: 0.8067 - val_loss: 1.7957 - val_acc: 0.5000\n",
      "Epoch 73/120\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.9802 - acc: 0.781 - 0s 337us/step - loss: 0.9727 - acc: 0.7800 - val_loss: 1.7814 - val_acc: 0.5000\n",
      "Epoch 74/120\n",
      "150/150 [==============================] - 0s 310us/step - loss: 0.9585 - acc: 0.7800 - val_loss: 1.7680 - val_acc: 0.5263\n",
      "Epoch 75/120\n",
      "150/150 [==============================] - 0s 400us/step - loss: 0.9574 - acc: 0.7733 - val_loss: 1.7646 - val_acc: 0.5263\n",
      "Epoch 76/120\n",
      "150/150 [==============================] - 0s 463us/step - loss: 0.9460 - acc: 0.8200 - val_loss: 1.7528 - val_acc: 0.5263\n",
      "Epoch 77/120\n",
      "150/150 [==============================] - 0s 356us/step - loss: 0.9170 - acc: 0.7933 - val_loss: 1.7433 - val_acc: 0.5000\n",
      "Epoch 78/120\n",
      "150/150 [==============================] - 0s 335us/step - loss: 0.9195 - acc: 0.7933 - val_loss: 1.7467 - val_acc: 0.4737\n",
      "Epoch 79/120\n",
      "150/150 [==============================] - 0s 451us/step - loss: 0.8951 - acc: 0.7867 - val_loss: 1.7614 - val_acc: 0.4737\n",
      "Epoch 80/120\n",
      "150/150 [==============================] - 0s 348us/step - loss: 0.9123 - acc: 0.7867 - val_loss: 1.7683 - val_acc: 0.4474\n",
      "Epoch 81/120\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.8794 - acc: 0.812 - 0s 386us/step - loss: 0.8792 - acc: 0.8200 - val_loss: 1.7675 - val_acc: 0.4474\n",
      "Epoch 82/120\n",
      "150/150 [==============================] - 0s 376us/step - loss: 0.8901 - acc: 0.7733 - val_loss: 1.7629 - val_acc: 0.4474\n",
      "Epoch 83/120\n",
      "150/150 [==============================] - 0s 295us/step - loss: 0.8828 - acc: 0.7600 - val_loss: 1.7578 - val_acc: 0.4737\n",
      "Epoch 84/120\n",
      "150/150 [==============================] - 0s 317us/step - loss: 0.8567 - acc: 0.7867 - val_loss: 1.7447 - val_acc: 0.4737\n",
      "Epoch 85/120\n",
      "150/150 [==============================] - 0s 304us/step - loss: 0.8509 - acc: 0.8000 - val_loss: 1.7470 - val_acc: 0.4474\n",
      "Epoch 86/120\n",
      "150/150 [==============================] - 0s 342us/step - loss: 0.8302 - acc: 0.8067 - val_loss: 1.7561 - val_acc: 0.4211\n",
      "Epoch 87/120\n",
      "150/150 [==============================] - 0s 356us/step - loss: 0.8299 - acc: 0.8000 - val_loss: 1.7370 - val_acc: 0.4211\n",
      "Epoch 88/120\n",
      "150/150 [==============================] - 0s 274us/step - loss: 0.8227 - acc: 0.8400 - val_loss: 1.7062 - val_acc: 0.4474\n",
      "Epoch 89/120\n",
      "150/150 [==============================] - 0s 357us/step - loss: 0.8138 - acc: 0.8267 - val_loss: 1.6845 - val_acc: 0.4737\n",
      "Epoch 90/120\n",
      "150/150 [==============================] - 0s 463us/step - loss: 0.8026 - acc: 0.7867 - val_loss: 1.6596 - val_acc: 0.4737\n",
      "Epoch 91/120\n",
      "150/150 [==============================] - 0s 412us/step - loss: 0.7839 - acc: 0.8200 - val_loss: 1.6348 - val_acc: 0.4474\n",
      "Epoch 92/120\n",
      "150/150 [==============================] - 0s 345us/step - loss: 0.8006 - acc: 0.7867 - val_loss: 1.6097 - val_acc: 0.4737\n",
      "Epoch 93/120\n",
      "150/150 [==============================] - 0s 278us/step - loss: 0.7787 - acc: 0.8133 - val_loss: 1.5899 - val_acc: 0.4737\n",
      "Epoch 94/120\n",
      "150/150 [==============================] - 0s 328us/step - loss: 0.7784 - acc: 0.7867 - val_loss: 1.5847 - val_acc: 0.4737\n",
      "Epoch 95/120\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.7887 - acc: 0.812 - 0s 264us/step - loss: 0.7777 - acc: 0.8133 - val_loss: 1.5892 - val_acc: 0.5000\n",
      "Epoch 96/120\n",
      "150/150 [==============================] - 0s 276us/step - loss: 0.7459 - acc: 0.8133 - val_loss: 1.5901 - val_acc: 0.4737\n",
      "Epoch 97/120\n",
      "150/150 [==============================] - 0s 313us/step - loss: 0.7424 - acc: 0.8133 - val_loss: 1.5880 - val_acc: 0.4737\n",
      "Epoch 98/120\n",
      "150/150 [==============================] - 0s 266us/step - loss: 0.7357 - acc: 0.8067 - val_loss: 1.5847 - val_acc: 0.4474\n",
      "Epoch 99/120\n",
      "150/150 [==============================] - 0s 235us/step - loss: 0.7419 - acc: 0.7933 - val_loss: 1.5780 - val_acc: 0.4474\n",
      "Epoch 100/120\n",
      "150/150 [==============================] - 0s 243us/step - loss: 0.7292 - acc: 0.7867 - val_loss: 1.5730 - val_acc: 0.4474\n",
      "Epoch 101/120\n",
      "150/150 [==============================] - 0s 250us/step - loss: 0.7131 - acc: 0.8000 - val_loss: 1.5645 - val_acc: 0.4474\n",
      "Epoch 102/120\n",
      "150/150 [==============================] - 0s 348us/step - loss: 0.7113 - acc: 0.8267 - val_loss: 1.5583 - val_acc: 0.4474\n",
      "Epoch 103/120\n",
      "150/150 [==============================] - 0s 403us/step - loss: 0.7088 - acc: 0.8267 - val_loss: 1.5480 - val_acc: 0.4474\n",
      "Epoch 104/120\n",
      "150/150 [==============================] - 0s 289us/step - loss: 0.6910 - acc: 0.8400 - val_loss: 1.5383 - val_acc: 0.4474\n",
      "Epoch 105/120\n",
      "150/150 [==============================] - 0s 265us/step - loss: 0.6889 - acc: 0.8267 - val_loss: 1.5225 - val_acc: 0.4737\n",
      "Epoch 106/120\n",
      "150/150 [==============================] - 0s 283us/step - loss: 0.6807 - acc: 0.7867 - val_loss: 1.5147 - val_acc: 0.4737\n",
      "Epoch 107/120\n",
      "150/150 [==============================] - 0s 309us/step - loss: 0.6811 - acc: 0.8000 - val_loss: 1.5131 - val_acc: 0.4737\n",
      "Epoch 108/120\n",
      "150/150 [==============================] - 0s 316us/step - loss: 0.6798 - acc: 0.7933 - val_loss: 1.5099 - val_acc: 0.4737\n",
      "Epoch 109/120\n",
      "150/150 [==============================] - 0s 314us/step - loss: 0.6594 - acc: 0.8467 - val_loss: 1.5007 - val_acc: 0.4737\n",
      "Epoch 110/120\n",
      "150/150 [==============================] - 0s 439us/step - loss: 0.6584 - acc: 0.7933 - val_loss: 1.4829 - val_acc: 0.5000\n",
      "Epoch 111/120\n",
      "150/150 [==============================] - 0s 364us/step - loss: 0.6513 - acc: 0.8200 - val_loss: 1.4860 - val_acc: 0.5000\n",
      "Epoch 112/120\n",
      "150/150 [==============================] - 0s 418us/step - loss: 0.6446 - acc: 0.8267 - val_loss: 1.4934 - val_acc: 0.4737\n",
      "Epoch 113/120\n",
      "150/150 [==============================] - 0s 388us/step - loss: 0.6396 - acc: 0.7933 - val_loss: 1.4888 - val_acc: 0.4737\n",
      "Epoch 114/120\n",
      "150/150 [==============================] - 0s 260us/step - loss: 0.6371 - acc: 0.8333 - val_loss: 1.4835 - val_acc: 0.5000\n",
      "Epoch 115/120\n",
      "150/150 [==============================] - 0s 336us/step - loss: 0.6302 - acc: 0.8000 - val_loss: 1.4859 - val_acc: 0.4474\n",
      "Epoch 116/120\n",
      "150/150 [==============================] - 0s 359us/step - loss: 0.6378 - acc: 0.8333 - val_loss: 1.4925 - val_acc: 0.4211\n",
      "Epoch 117/120\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.6008 - acc: 0.765 - 0s 336us/step - loss: 0.6208 - acc: 0.7533 - val_loss: 1.4887 - val_acc: 0.4211\n",
      "Epoch 118/120\n",
      "150/150 [==============================] - 0s 361us/step - loss: 0.6107 - acc: 0.8333 - val_loss: 1.4851 - val_acc: 0.4211\n",
      "Epoch 119/120\n",
      "150/150 [==============================] - 0s 351us/step - loss: 0.6114 - acc: 0.8333 - val_loss: 1.4984 - val_acc: 0.4211\n",
      "Epoch 120/120\n",
      "150/150 [==============================] - 0s 307us/step - loss: 0.6001 - acc: 0.8400 - val_loss: 1.4983 - val_acc: 0.4211\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.DeepEstimator at 0x1a12e3ec50>"
      ]
     },
     "execution_count": 1157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train_continuous, y_test_continuous = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "y_train = bool_arr(y_train_continuous, 0.05)\n",
    "y_test = bool_arr(y_test_continuous, 0.00)\n",
    "\n",
    "select_fpr = SelectFpr(alpha=0.1)\n",
    "\n",
    "X_train = select_fpr.fit_transform(X_train, y_train)\n",
    "X_test = select_fpr.transform(X_test)\n",
    "\n",
    "DeepEstimator().fit(X_train, y_train, X_test, y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(38, 924)"
      ]
     },
     "execution_count": 1161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1159,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "deep_train_pipeline = Pipeline([\n",
    "    ('reduce_false_pos', select_fpr),\n",
    "    ('deep', DeepEstimator())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1162,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "X has a different shape than during fitting.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1162-eca009c5a7f0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdeep_train_pipeline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test_continuous\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-1160-9cddf076b3d9>\u001b[0m in \u001b[0;36mscore\u001b[0;34m(est, X, y, cutoff)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcutoff\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.75\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0;31m# Invest if over 50% confident\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0my_pred_bool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbool_arr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcutoff\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/utils/metaestimators.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;31m# lambda, but not partial, allows help() to work with update_wrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m         \u001b[0;31m# update the docstring of the returned function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0mupdate_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X, **predict_params)\u001b[0m\n\u001b[1;32m    312\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 314\u001b[0;31m                 \u001b[0mXt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    315\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpredict_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/feature_selection/base.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     81\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"X has a different shape than during fitting.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msafe_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: X has a different shape than during fitting."
     ]
    }
   ],
   "source": [
    "score(deep_train_pipeline, X_test, y_test_continuous,0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1160,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "X has a different shape than during fitting.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1160-9cddf076b3d9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mxs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinspace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdeep_train_pipeline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test_continuous\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0mys_attained\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mys_potential\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1160-9cddf076b3d9>\u001b[0m in \u001b[0;36mscore\u001b[0;34m(est, X, y, cutoff)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcutoff\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.75\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0;31m# Invest if over 50% confident\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0my_pred_bool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbool_arr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcutoff\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/utils/metaestimators.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;31m# lambda, but not partial, allows help() to work with update_wrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m         \u001b[0;31m# update the docstring of the returned function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0mupdate_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X, **predict_params)\u001b[0m\n\u001b[1;32m    312\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 314\u001b[0;31m                 \u001b[0mXt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    315\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpredict_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/feature_selection/base.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     81\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"X has a different shape than during fitting.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msafe_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: X has a different shape than during fitting."
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def score(est, X, y, cutoff=0.75):\n",
    "    y_pred = est.predict(X)\n",
    "    # Invest if over 50% confident \n",
    "    y_pred_bool = bool_arr(y_pred, cutoff)\n",
    "    # Score correct if you make postive returns\n",
    "    y_true_bool = bool_arr(y, 0)\n",
    "    total = np.dot(y_pred_bool.reshape(X.shape[0],), y)\n",
    "    buy_all = np.dot(np.ones(X.shape[0]), y)\n",
    "    av = total/len(X)\n",
    "    buy_all_av = buy_all/len(X)\n",
    "#     print(confusion_matrix(y_true_bool, y_pred_bool))\n",
    "    return [av, buy_all_av]\n",
    "\n",
    "\n",
    "ys_attained = []\n",
    "ys_potential = []\n",
    "xs = []\n",
    "for i in np.linspace(0, 1):\n",
    "    sc = score(deep_train_pipeline, X_test, y_test_continuous,i)\n",
    "    ys_attained.append(sc[0])\n",
    "    ys_potential.append(sc[1])\n",
    "    xs.append(i)\n",
    "plt.plot(xs, ys_attained)\n",
    "plt.plot(xs, ys_potential)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bool_arr(deep_train_pipeline.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Factors to look into including: \n",
    "# \"BookValue\" = (Total Assets - Total Liabilities) / Number of shares outstanding\n",
    "# \"MarketCap\" = Market price per share * number of shares \n",
    "# \"DividendYield\" = Dividend / Market price per share \n",
    "# \"EarningsPerShare\" \n",
    "# \"PERatio2\" = Market price per share / earning per share \n",
    "# \"priceBook\" = Market price per share / ((Total Assets - Total Liabilities) / Number of shares outstanding)\n",
    "# \"PriceSales\" = MarketCap / Revenue \n",
    "# \"Ask\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# vocab = list(text_word_counts.steps[2][1].vocabulary_.keys())\n",
    "# import operator\n",
    "# iv_dict = [[vocab[i],-float(f)] for i,f in enumerate(clf.coef_)]\n",
    "# most_important_terms = sorted(iv_dict, key=operator.itemgetter(1))[0:100]\n",
    "# print(most_important_terms)\n",
    "# most_important_vocab = dict(most_important_terms).keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from joblib import Memory\n",
    "# %mkdir cachedir\n",
    "# location = './cachedir'\n",
    "# memory = Memory(location, verbose=0)\n",
    "# stock_name_date_mapping = memory.cache(stock_name_date_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # print(text_word_counts.steps[1][1].vocabulary_)\n",
    "# def get_excepted_stock_names(limit=-1):\n",
    "#     esn = {}\n",
    "#     for i, file in tqdm(enumerate(glob.glob('filing_texts/*')[0:limit])):\n",
    "#         stock_name = stock_name_from_filename(file)\n",
    "#         file_data = open(file, 'r').read()\n",
    "#         try:\n",
    "#             with open(file) as open_file:\n",
    "#                 file_data = [next(open_file) for x in range(3)]\n",
    "#             file_data = ''.join(file_data)\n",
    "#             time_data = file_data.split(\"\\n\")[2][5:14]\n",
    "#         except:\n",
    "#             esn[i] = stock_name\n",
    "#             continue\n",
    "#     return esn\n",
    "\n",
    "# def get_x_mask(X, y, filenames):\n",
    "#     no_date_indices = []\n",
    "#     if X.shape[0] != len(y):\n",
    "#         stock_names = FilenamesToStockNamesTransformer().transform(filenames)\n",
    "#         for i, stock_name in enumerate(stock_names): \n",
    "#             try:\n",
    "#                 date = spm[stock_name]['date']\n",
    "#             except:\n",
    "#                 no_date_indices.append(i)\n",
    "#         mask = np.ones(X.shape[0], dtype=bool)\n",
    "#         mask[no_date_indices] = False\n",
    "#         X_mask = X[mask]\n",
    "#     else:\n",
    "#         X_mask = X\n",
    "#     return X_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
