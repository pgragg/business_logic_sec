{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 627,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: filing_texts: File exists\n",
      "mkdir: prices: File exists\n"
     ]
    }
   ],
   "source": [
    "% mkdir filing_texts\n",
    "% mkdir prices "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 681,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, TfidfTransformer, CountVectorizer\n",
    "import glob\n",
    "import os \n",
    "import pandas as pd\n",
    "import datetime\n",
    "from scipy.stats import iqr\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import metrics\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "import math\n",
    "\n",
    "spm = None\n",
    "english_words = None\n",
    "excepted_stock_names = []\n",
    "limit = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 629,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.layers import Dense, Dropout, RepeatVector, BatchNormalization, Convolution1D, Flatten, Lambda, Permute, MaxPooling1D, AlphaDropout\n",
    "from keras.models import Sequential\n",
    "from keras.utils import to_categorical\n",
    "import keras.backend as K\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import load_model\n",
    "from tqdm import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 650,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DateRange():\n",
    "    def __init__(self, start_int, end_int):\n",
    "        self.start_int = start_int\n",
    "        self.end_int = end_int\n",
    "        self.range = (range(start_int, end_int))\n",
    "\n",
    "    def transform(self, date):\n",
    "        output = {}\n",
    "        dates = []\n",
    "        for delta in self.range:\n",
    "            date_delta = datetime.timedelta(days=delta)\n",
    "            date_string = datetime.datetime.strptime(date, '%Y-%m-%d').date()\n",
    "            dates.append(str(date_string + date_delta))\n",
    "        return dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 697,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SNDM():\n",
    "    def __init__(self, limit=-1):\n",
    "        self.limit = limit\n",
    "        self.sndm = self.__get_sndm()\n",
    "\n",
    "    def __get_sndm(self):\n",
    "        spm = self.__stock_name_date_mapping()\n",
    "        output = {}\n",
    "        for stock_name in spm.keys():\n",
    "            numerical_prices = 0\n",
    "            for date in DateRange(-3,2).transform(spm[stock_name]['date']):\n",
    "                try:\n",
    "                    isnan = math.isnan(PriceData(stock_name).on_date(date, 'open'))\n",
    "                except:\n",
    "                    isnan = True\n",
    "                if not (isnan):\n",
    "                    numerical_prices += 1 \n",
    "            # Delete stock_name from spm if numerical prices under a threshold. (nans)\n",
    "            if numerical_prices >= 3:\n",
    "                output[stock_name] = spm[stock_name]\n",
    "        return output\n",
    "    \n",
    "    def __stock_name_date_mapping(self):\n",
    "        spm = {}\n",
    "        for i, file in enumerate(glob.glob('filing_texts/*')[0:self.limit]):\n",
    "            try:\n",
    "                stock_name = self.__stock_name_from_filename(file)\n",
    "                file_data = open(file, 'r').read()[:200]\n",
    "                time_data = file_data.split('<ACCEPTANCE-DATETIME>')[1].split('\\\\n')[0][:8]\n",
    "                year = time_data[0:4]\n",
    "                month = time_data[4:6]\n",
    "                day = time_data[6:8]\n",
    "                date_stamp = (\"%s-%s-%s\" %(year, month, day))\n",
    "                spm[stock_name] = {'date': date_stamp}\n",
    "            except:\n",
    "                print(\"No data for \", stock_name)\n",
    "                excepted_stock_names.append(stock_name)\n",
    "                continue\n",
    "        return spm\n",
    "    def __stock_name_from_filename(filename):\n",
    "        return filename.split('/')[-1].split('_')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 656,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class PriceData:\n",
    "    def __init__(self, stock_name):\n",
    "        self.stock_name = stock_name\n",
    "        try:\n",
    "            self.price_data = pd.read_csv('prices/' + stock_name + '.csv')\n",
    "        except:\n",
    "            self.price_data = pd.DataFrame.from_dict({})\n",
    "\n",
    "    def on_date(self, date, market_time = 'open'): \n",
    "        try:\n",
    "            return float(self.price_data.loc[self.price_data['date'] == date][market_time])\n",
    "        except: \n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 657,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FilenamesToStockNamesTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        return None\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def fit_transform(self, X, y=None):\n",
    "        return self.fit(X, y).transform(X)\n",
    "        \n",
    "    def transform(self, X):\n",
    "        output = []\n",
    "        for filename in X:\n",
    "            stock_name = self.__stock_name_from_filename(filename)\n",
    "            output.append(stock_name)\n",
    "        return output\n",
    "    def __stock_name_from_filename(self, filename):\n",
    "        return filename.split('/')[-1].split('_')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 717,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Used in y pipeline\n",
    "class SpmToFileNamesTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        return None\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def fit_transform(self, X, y=None):\n",
    "        return self.fit(X, y).transform(X)\n",
    "    def transform(self, X):\n",
    "        output = []\n",
    "        for key in X.keys():\n",
    "            date = X[key]['date']\n",
    "            output.append(f'filing_texts/{key}_{date}')\n",
    "        return output\n",
    "    \n",
    "class SpmToStockNamesTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        return None\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def fit_transform(self, X, y=None):\n",
    "        return self.fit(X, y).transform(X)\n",
    "        \n",
    "    def transform(self, X):\n",
    "        return X.keys()\n",
    "    \n",
    "class StockNamesToFileNamesTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        return None\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def fit_transform(self, X, y=None):\n",
    "        return self.fit(X, y).transform(X)\n",
    "        \n",
    "    def transform(self, X):\n",
    "        return [f'filing_texts/{stock_name}' for stock_name in X]\n",
    "    \n",
    "# class MapStockNamesToDatesTransformer(BaseEstimator, TransformerMixin):\n",
    "#     def __init__(self, start_int, end_int):\n",
    "#         self.start_int = start_int\n",
    "#         self.end_int = end_int\n",
    "#         self.range = (range(start_int, end_int))\n",
    "    \n",
    "#     def fit(self, X, y=None):\n",
    "#         return self\n",
    "    \n",
    "#     def fit_transform(self, X, y=None):\n",
    "#         return self.fit(X, y).transform(X)\n",
    "        \n",
    "#     def transform(self, X):\n",
    "#         output = {}\n",
    "#         for i, stock_name in enumerate(X):\n",
    "#             try:\n",
    "#                 date = spm[stock_name]['date']\n",
    "#             except:\n",
    "#                 continue # Ignore stocks which don't have a date\n",
    "#             dates = []\n",
    "#             for delta in self.range:\n",
    "#                 date_delta = datetime.timedelta(days=delta)\n",
    "#                 date_string = datetime.datetime.strptime(date, '%Y-%m-%d').date()\n",
    "#                 dates.append(str(date_string + date_delta))\n",
    "#             output[stock_name] = dates\n",
    "#         return output\n",
    "# class StockNameDatesMapToPricesListTransformer(BaseEstimator, TransformerMixin):\n",
    "#     def __init__(self):\n",
    "#         return None\n",
    "    \n",
    "#     def fit(self, X, y=None):\n",
    "#         return self\n",
    "    \n",
    "#     def fit_transform(self, X, y=None):\n",
    "#         return self.fit(X, y).transform(X)\n",
    "        \n",
    "#     def transform(self, X):\n",
    "#         output = []\n",
    "#         for stock_name in X.keys():\n",
    "#             prices = []\n",
    "#             for date in X[stock_name]:\n",
    "#                 prices.append(PriceData(stock_name).on_date(date, 'open'))\n",
    "#             output.append(prices)\n",
    "#         return np.array(output)\n",
    "    \n",
    "# class LabelsTransform(BaseEstimator, TransformerMixin):\n",
    "#     # Returns the interquartile-range and median.\n",
    "#     def __init__(self):\n",
    "#         return None\n",
    "        \n",
    "#     def fit(self, X, y=None):\n",
    "#         return self\n",
    "    \n",
    "#     def fit_transform(self, X, y=None):\n",
    "#         return self.fit(X, y).transform(X)\n",
    "        \n",
    "#     def transform(self, X):\n",
    "#         # ldcom = last_day_change_over_median\n",
    "#         print(X.shape)\n",
    "#         ldcom = []\n",
    "#         for prices in X:\n",
    "#             this_median = np.median(prices[0:-3])\n",
    "#             ldcom.append(((prices[-1]-this_median)/this_median))\n",
    "#         return np.array(ldcom)\n",
    "            \n",
    "class StatisticalMeasuresTransformer(BaseEstimator, TransformerMixin):\n",
    "    # Returns the interquartile-range and median.\n",
    "    def __init__(self):\n",
    "        return None\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        # ldcom = last_day_change_over_median\n",
    "        \n",
    "        output = []\n",
    "        self.iqr_var = []\n",
    "        self.median = []\n",
    "        for prices in X:\n",
    "            this_iqr = iqr(prices[0:-3])\n",
    "            this_median = np.median(prices[0:-3])\n",
    "            self.iqr_var.append(this_iqr)\n",
    "            self.median.append(this_median)\n",
    "        return self\n",
    "    \n",
    "    def fit_transform(self, X, y=None):\n",
    "        return self.fit(X, y).transform(X)\n",
    "        \n",
    "    def transform(self, X):\n",
    "        X_output = []\n",
    "        for i, prices in enumerate(X):\n",
    "            stats   = []\n",
    "            iqr_var = self.iqr_var[i] or iqr(prices)\n",
    "            median  = self.median[i]  or np.median(prices)\n",
    "            \n",
    "            stats.append(iqr_var)\n",
    "            stats.append(median)\n",
    "            \n",
    "            X_output.append(stats)\n",
    "        return np.array(X_output)\n",
    "class SparseToArray(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        return None\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def fit_transform(self, X, y=None):\n",
    "        return self.fit(X, y).transform(X)\n",
    "        \n",
    "    def transform(self, X):\n",
    "        return np.array(X.toarray()) #[ar.toarray() for ar in X]\n",
    "    \n",
    "class ReadFiles(BaseEstimator, TransformerMixin): \n",
    "    def __init__(self):\n",
    "        return None\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def fit_transform(self, X, y=None):\n",
    "        return self.fit(X, y).transform(X)\n",
    "        \n",
    "    def transform(self, X):\n",
    "        return (open(filename, 'r').read() for filename in tqdm(X))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 659,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_filenames(limit = -1):\n",
    "    filenames = []\n",
    "    directory_files = glob.glob('filing_texts/*')\n",
    "    excepted_files = [('filing_texts/' + sn) for sn in excepted_stock_names]\n",
    "    filenames_with_data = [x for x in directory_files[:limit] if x not in excepted_files]\n",
    "    for filename in filenames_with_data:\n",
    "        filenames.append(filename)\n",
    "    return filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 774,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spm = SNDM().sndm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 828,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class StockNamesToLabelsTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, start_int, end_int):\n",
    "        self.filing_int = start_int * -1\n",
    "        self.start_int = start_int\n",
    "        self.end_int = end_int\n",
    "        self.range = (range(start_int, end_int))\n",
    "    # Not implemented since only used to generate labels\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    # Not implemented since only used to generate labels\n",
    "    def fit_transform(self, X, y=None):\n",
    "        return self.fit(X, y).transform(X)\n",
    "        \n",
    "    def transform(self, X):\n",
    "        h = {}\n",
    "        for i, stock_name in enumerate(X):\n",
    "            try:\n",
    "                date = spm[stock_name]['date']\n",
    "            except:\n",
    "                continue # Ignore stocks which don't have a date\n",
    "            dates = []\n",
    "            for delta in self.range:\n",
    "                date_delta = datetime.timedelta(days=delta)\n",
    "                date_string = datetime.datetime.strptime(date, '%Y-%m-%d').date()\n",
    "                dates.append(str(date_string + date_delta))\n",
    "            h[stock_name] = dates\n",
    "        \n",
    "        ldcom = []\n",
    "        for stock_name in h.keys():\n",
    "            earliest_price_after_filing = None\n",
    "            hist_p = []\n",
    "            for i, date in enumerate(h[stock_name]):\n",
    "                price = PriceData(stock_name).on_date(date, 'open')\n",
    "                if price and not math.isnan(price):\n",
    "                    if i > self.filing_int:\n",
    "                        earliest_price_after_filing = earliest_price_after_filing or price\n",
    "                    else:\n",
    "                        hist_p.append(price)\n",
    "            # Closing price on day of filing\n",
    "            price_close_filing = PriceData(stock_name).on_date(h[stock_name][self.filing_int], 'close')\n",
    "            # Use either the next open day of trading or the close price on day of filing\n",
    "            comparison_price = earliest_price_after_filing or price_close_filing\n",
    "            # Remove nans from historical prices before taking the mean\n",
    "            this_mean = np.mean(hist_p)\n",
    "            ldcom.append(((comparison_price-this_mean)/this_mean))\n",
    "        return np.array(ldcom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 829,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32.78\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.01682823])"
      ]
     },
     "execution_count": 829,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "StockNamesToLabelsTransformer(-5,5).transform(['XOXO'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 831,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used to build other pipelines\n",
    "from sklearn.preprocessing import Imputer\n",
    "\n",
    "# prices_pipeline = Pipeline([\n",
    "#     ('spm_to_filenames', SpmToFileNamesTransformer()),\n",
    "#     ('filenames_to_stock_names', FilenamesToStockNamesTransformer()),\n",
    "#     ('stock_names_to_dates', MapStockNamesToDatesTransformer(-5, 2)),\n",
    "#     ('dates_to_prices_transformer', StockNameDatesMapToPricesListTransformer()),\n",
    "#     ('imputer', Imputer(axis=1))\n",
    "# ])\n",
    "\n",
    "labels_pipeline = Pipeline([\n",
    "    ('spm_to_filenames', SpmToFileNamesTransformer()),\n",
    "    ('filenames_to_stock_names', FilenamesToStockNamesTransformer()),\n",
    "    ('dates_to_prices_transformer', StockNamesToLabelsTransformer(-5,2))\n",
    "])\n",
    "\n",
    "# Used as the final y values \n",
    "# labels_pipeline = Pipeline([\n",
    "#     ('prices_pipeline', prices_pipeline),\n",
    "#     ('labels_transform', LabelsTransform())\n",
    "# ])\n",
    "\n",
    "# Used for training and test set features\n",
    "stock_stats = Pipeline([\n",
    "    ('prices_pipeline', prices_pipeline),\n",
    "    ('stats_transform', StatisticalMeasuresTransformer())\n",
    "])\n",
    "\n",
    "# Used for training and test set features \n",
    "# ('stock_names_to_file_names', StockNamesToFileNamesTransformer()),\n",
    "text_word_counts = Pipeline([\n",
    "    ('spm_to_file_names', SpmToFileNamesTransformer()),\n",
    "    ('read_files', ReadFiles()),\n",
    "    ('vect', TfidfVectorizer(\n",
    "                token_pattern=r\"[a-zA-Z]+\", \n",
    "                min_df = 0.10,\n",
    "                max_df = 0.80,\n",
    "                stop_words = 'english',\n",
    "                max_features=9000,\n",
    "                ngram_range=(1, 1))),\n",
    "    ('sparse_to_array', SparseToArray()),\n",
    "    ('std_scaler', StandardScaler()),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 834,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = text_word_counts.fit_transform(spm)\n",
    "y = labels_pipeline.fit_transform(spm)\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 833,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train_continuous, y_test_continuous = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1033,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bool_arr(arr, limit=0.5):\n",
    "    y_bool = []\n",
    "    for num in arr:\n",
    "        if num >= limit:\n",
    "            y_bool.append(1)\n",
    "        else:\n",
    "            y_bool.append(0)\n",
    "    return np.array(y_bool)\n",
    "y_train = bool_arr(y_train_continuous, 0.05)\n",
    "y_test = bool_arr(y_test_continuous, 0.00)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1034,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.34210526315789475"
      ]
     },
     "execution_count": 1034,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectFpr\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "\n",
    "svc_train_pipeline = Pipeline([\n",
    "    ('reduce_false_pos', SelectFpr(alpha=0.1)),\n",
    "    ('svc', GridSearchCV(estimator=SVC(gamma='auto'), param_grid=dict(C=np.logspace(-1,3)), n_jobs=1))\n",
    "])\n",
    "svc_train_pipeline.fit(X_train, y_train)\n",
    "svc_train_pipeline.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1035,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Piper/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:194: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SVC(C=100, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
       "  kernel='rbf', max_iter=-1, probability=False, random_state=None,\n",
       "  shrinking=True, tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 1035,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "clf = SVC(C=100)\n",
    "# clf = RandomForestRegressor(n_estimators=200, max_depth=3, verbose=1, n_jobs=2)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1036,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import regularizers\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1041,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DeepEstimator():\n",
    "    def __init__(self):\n",
    "        return None\n",
    "    def fit(self, X, y=None):\n",
    "        class_weight = {0: 0.1,\n",
    "                1: 0.9}\n",
    "        self.model = self.__define_model(X)\n",
    "        self.model.fit(X, y, class_weight=class_weight)\n",
    "        sched = [[0.0001, 2], [0.001, 20], [0.01, 2], [0.1, 2], [0.5, 1], [0.1, 5], [0.01, 20], [0.001, 40], [0.0001, 80], [0.00005, 120]]\n",
    "        for lr, epochs in sched:\n",
    "            m1.optimizer.lr = lr\n",
    "            m1.fit(np.array(X_train), np.array(y_train), epochs=epochs,  batch_size=64, validation_data=(np.array(X_test), np.array(y_test)))\n",
    "        return self\n",
    "    def predict(self, X, y=None):\n",
    "        return self.model.predict(X)\n",
    "    def __define_model(self, X):\n",
    "        shape = X.shape[1]\n",
    "        model = Sequential([\n",
    "            BatchNormalization(input_shape=(shape,)),\n",
    "            Dense(25, activation='relu', kernel_regularizer=regularizers.l1(0.01)),\n",
    "            Dropout(0.8),\n",
    "            BatchNormalization(),\n",
    "            Dense(1, activation='sigmoid')   \n",
    "        ])\n",
    "\n",
    "        model.compile(optimizer='adam',\n",
    "                      loss='binary_crossentropy',\n",
    "                      metrics=['accuracy'])\n",
    "        return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1044,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "150/150 [==============================] - 2s 14ms/step - loss: 16.1550 - acc: 0.5200\n",
      "Train on 150 samples, validate on 38 samples\n",
      "Epoch 1/2\n",
      "150/150 [==============================] - 0s 548us/step - loss: 0.3882 - acc: 0.9933 - val_loss: 3.2588 - val_acc: 0.4211\n",
      "Epoch 2/2\n",
      "150/150 [==============================] - 0s 539us/step - loss: 0.3984 - acc: 0.9867 - val_loss: 3.2462 - val_acc: 0.4211\n",
      "Train on 150 samples, validate on 38 samples\n",
      "Epoch 1/20\n",
      "150/150 [==============================] - 0s 403us/step - loss: 0.3824 - acc: 0.9933 - val_loss: 3.2289 - val_acc: 0.4211\n",
      "Epoch 2/20\n",
      "150/150 [==============================] - 0s 303us/step - loss: 0.3790 - acc: 0.9867 - val_loss: 3.2454 - val_acc: 0.4211\n",
      "Epoch 3/20\n",
      "150/150 [==============================] - 0s 457us/step - loss: 0.3629 - acc: 0.9867 - val_loss: 3.2566 - val_acc: 0.4211\n",
      "Epoch 4/20\n",
      "150/150 [==============================] - 0s 511us/step - loss: 0.3626 - acc: 0.9933 - val_loss: 3.2594 - val_acc: 0.4211\n",
      "Epoch 5/20\n",
      "150/150 [==============================] - 0s 384us/step - loss: 0.3381 - acc: 0.9933 - val_loss: 3.2521 - val_acc: 0.4211\n",
      "Epoch 6/20\n",
      "150/150 [==============================] - 0s 639us/step - loss: 0.3411 - acc: 0.9933 - val_loss: 3.2435 - val_acc: 0.4211\n",
      "Epoch 7/20\n",
      "150/150 [==============================] - 0s 312us/step - loss: 0.3630 - acc: 0.9800 - val_loss: 3.2224 - val_acc: 0.4211\n",
      "Epoch 8/20\n",
      "150/150 [==============================] - 0s 447us/step - loss: 0.3193 - acc: 0.9933 - val_loss: 3.2516 - val_acc: 0.4211\n",
      "Epoch 9/20\n",
      "150/150 [==============================] - 0s 439us/step - loss: 0.3212 - acc: 0.9800 - val_loss: 3.3272 - val_acc: 0.4211\n",
      "Epoch 10/20\n",
      "150/150 [==============================] - 0s 405us/step - loss: 0.3607 - acc: 0.9733 - val_loss: 3.3076 - val_acc: 0.4211\n",
      "Epoch 11/20\n",
      "150/150 [==============================] - 0s 454us/step - loss: 0.3115 - acc: 0.9800 - val_loss: 3.2585 - val_acc: 0.4211\n",
      "Epoch 12/20\n",
      "150/150 [==============================] - 0s 421us/step - loss: 0.3074 - acc: 0.9933 - val_loss: 3.2296 - val_acc: 0.4211\n",
      "Epoch 13/20\n",
      "150/150 [==============================] - 0s 542us/step - loss: 0.3301 - acc: 0.9733 - val_loss: 3.1854 - val_acc: 0.4211\n",
      "Epoch 14/20\n",
      "150/150 [==============================] - 0s 431us/step - loss: 0.3291 - acc: 0.9667 - val_loss: 3.1281 - val_acc: 0.3947\n",
      "Epoch 15/20\n",
      "150/150 [==============================] - 0s 480us/step - loss: 0.3185 - acc: 0.9933 - val_loss: 3.1401 - val_acc: 0.3947\n",
      "Epoch 16/20\n",
      "150/150 [==============================] - 0s 485us/step - loss: 0.3525 - acc: 0.9867 - val_loss: 3.1628 - val_acc: 0.3947\n",
      "Epoch 17/20\n",
      "150/150 [==============================] - 0s 556us/step - loss: 0.3587 - acc: 0.9867 - val_loss: 3.2207 - val_acc: 0.3947\n",
      "Epoch 18/20\n",
      "150/150 [==============================] - 0s 392us/step - loss: 0.3552 - acc: 1.0000 - val_loss: 3.2719 - val_acc: 0.3947\n",
      "Epoch 19/20\n",
      "150/150 [==============================] - 0s 488us/step - loss: 0.3715 - acc: 0.9867 - val_loss: 3.2768 - val_acc: 0.3947\n",
      "Epoch 20/20\n",
      "150/150 [==============================] - 0s 608us/step - loss: 0.3649 - acc: 0.9933 - val_loss: 3.2621 - val_acc: 0.3947\n",
      "Train on 150 samples, validate on 38 samples\n",
      "Epoch 1/2\n",
      "150/150 [==============================] - 0s 449us/step - loss: 0.4021 - acc: 0.9867 - val_loss: 3.2492 - val_acc: 0.3947\n",
      "Epoch 2/2\n",
      "150/150 [==============================] - 0s 513us/step - loss: 0.3679 - acc: 0.9933 - val_loss: 3.2415 - val_acc: 0.4211\n",
      "Train on 150 samples, validate on 38 samples\n",
      "Epoch 1/2\n",
      "150/150 [==============================] - 0s 658us/step - loss: 0.3968 - acc: 0.9733 - val_loss: 3.2713 - val_acc: 0.4211\n",
      "Epoch 2/2\n",
      "150/150 [==============================] - 0s 514us/step - loss: 0.4049 - acc: 0.9733 - val_loss: 3.3201 - val_acc: 0.4211\n",
      "Train on 150 samples, validate on 38 samples\n",
      "Epoch 1/1\n",
      "150/150 [==============================] - 0s 525us/step - loss: 0.3696 - acc: 0.9867 - val_loss: 3.2951 - val_acc: 0.4211\n",
      "Train on 150 samples, validate on 38 samples\n",
      "Epoch 1/5\n",
      "150/150 [==============================] - 0s 566us/step - loss: 0.4561 - acc: 0.9600 - val_loss: 3.2559 - val_acc: 0.3947\n",
      "Epoch 2/5\n",
      "150/150 [==============================] - 0s 461us/step - loss: 0.3866 - acc: 0.9733 - val_loss: 3.2678 - val_acc: 0.3684\n",
      "Epoch 3/5\n",
      "150/150 [==============================] - 0s 579us/step - loss: 0.3829 - acc: 0.9867 - val_loss: 3.2795 - val_acc: 0.3684\n",
      "Epoch 4/5\n",
      "150/150 [==============================] - 0s 482us/step - loss: 0.4334 - acc: 0.9600 - val_loss: 3.2421 - val_acc: 0.3684\n",
      "Epoch 5/5\n",
      "150/150 [==============================] - 0s 382us/step - loss: 0.4169 - acc: 0.9667 - val_loss: 3.2399 - val_acc: 0.3684\n",
      "Train on 150 samples, validate on 38 samples\n",
      "Epoch 1/20\n",
      "150/150 [==============================] - 0s 454us/step - loss: 0.4480 - acc: 0.9600 - val_loss: 3.2446 - val_acc: 0.3947\n",
      "Epoch 2/20\n",
      "150/150 [==============================] - 0s 378us/step - loss: 0.4094 - acc: 0.9933 - val_loss: 3.2360 - val_acc: 0.3947\n",
      "Epoch 3/20\n",
      "150/150 [==============================] - 0s 471us/step - loss: 0.4782 - acc: 0.9600 - val_loss: 3.2382 - val_acc: 0.3947\n",
      "Epoch 4/20\n",
      "150/150 [==============================] - 0s 376us/step - loss: 0.4486 - acc: 0.9867 - val_loss: 3.3204 - val_acc: 0.3947\n",
      "Epoch 5/20\n",
      "150/150 [==============================] - 0s 407us/step - loss: 0.4849 - acc: 0.9733 - val_loss: 3.3617 - val_acc: 0.3947\n",
      "Epoch 6/20\n",
      "150/150 [==============================] - 0s 374us/step - loss: 0.5025 - acc: 0.9800 - val_loss: 3.3584 - val_acc: 0.3947\n",
      "Epoch 7/20\n",
      "150/150 [==============================] - 0s 559us/step - loss: 0.4858 - acc: 0.9867 - val_loss: 3.3630 - val_acc: 0.3947\n",
      "Epoch 8/20\n",
      "150/150 [==============================] - 0s 566us/step - loss: 0.5207 - acc: 0.9733 - val_loss: 3.3498 - val_acc: 0.3947\n",
      "Epoch 9/20\n",
      "150/150 [==============================] - 0s 374us/step - loss: 0.4610 - acc: 0.9933 - val_loss: 3.3225 - val_acc: 0.3947\n",
      "Epoch 10/20\n",
      "150/150 [==============================] - 0s 510us/step - loss: 0.4644 - acc: 0.9800 - val_loss: 3.2898 - val_acc: 0.3947\n",
      "Epoch 11/20\n",
      "150/150 [==============================] - 0s 430us/step - loss: 0.4166 - acc: 1.0000 - val_loss: 3.2797 - val_acc: 0.4211\n",
      "Epoch 12/20\n",
      "150/150 [==============================] - 0s 388us/step - loss: 0.4164 - acc: 0.9800 - val_loss: 3.2915 - val_acc: 0.4211\n",
      "Epoch 13/20\n",
      "150/150 [==============================] - 0s 387us/step - loss: 0.4123 - acc: 0.9733 - val_loss: 3.3023 - val_acc: 0.4211\n",
      "Epoch 14/20\n",
      "150/150 [==============================] - 0s 409us/step - loss: 0.3891 - acc: 0.9867 - val_loss: 3.2866 - val_acc: 0.3947\n",
      "Epoch 15/20\n",
      "150/150 [==============================] - 0s 398us/step - loss: 0.3878 - acc: 0.9933 - val_loss: 3.2896 - val_acc: 0.4211\n",
      "Epoch 16/20\n",
      "150/150 [==============================] - 0s 426us/step - loss: 0.4012 - acc: 0.9800 - val_loss: 3.2711 - val_acc: 0.3947\n",
      "Epoch 17/20\n",
      "150/150 [==============================] - 0s 394us/step - loss: 0.3830 - acc: 0.9800 - val_loss: 3.2317 - val_acc: 0.3947\n",
      "Epoch 18/20\n",
      "150/150 [==============================] - 0s 412us/step - loss: 0.3889 - acc: 0.9867 - val_loss: 3.2296 - val_acc: 0.3947\n",
      "Epoch 19/20\n",
      "150/150 [==============================] - 0s 482us/step - loss: 0.3908 - acc: 0.9800 - val_loss: 3.2052 - val_acc: 0.3684\n",
      "Epoch 20/20\n",
      "150/150 [==============================] - 0s 495us/step - loss: 0.3719 - acc: 0.9933 - val_loss: 3.1647 - val_acc: 0.3684\n",
      "Train on 150 samples, validate on 38 samples\n",
      "Epoch 1/40\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.4014 - acc: 0.9733 - val_loss: 3.1112 - val_acc: 0.3947\n",
      "Epoch 2/40\n",
      "150/150 [==============================] - 0s 666us/step - loss: 0.4005 - acc: 0.9867 - val_loss: 3.1340 - val_acc: 0.3947\n",
      "Epoch 3/40\n",
      "150/150 [==============================] - 0s 684us/step - loss: 0.3844 - acc: 0.9867 - val_loss: 3.1397 - val_acc: 0.3947\n",
      "Epoch 4/40\n",
      "150/150 [==============================] - 0s 587us/step - loss: 0.3766 - acc: 0.9867 - val_loss: 3.1576 - val_acc: 0.3947\n",
      "Epoch 5/40\n",
      "150/150 [==============================] - 0s 448us/step - loss: 0.3676 - acc: 0.9933 - val_loss: 3.1726 - val_acc: 0.3947\n",
      "Epoch 6/40\n",
      "150/150 [==============================] - 0s 591us/step - loss: 0.3750 - acc: 0.9867 - val_loss: 3.2012 - val_acc: 0.4211\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/40\n",
      "150/150 [==============================] - 0s 544us/step - loss: 0.4086 - acc: 0.9733 - val_loss: 3.2210 - val_acc: 0.4211\n",
      "Epoch 8/40\n",
      "150/150 [==============================] - 0s 768us/step - loss: 0.3863 - acc: 0.9733 - val_loss: 3.2891 - val_acc: 0.4211\n",
      "Epoch 9/40\n",
      "150/150 [==============================] - 0s 579us/step - loss: 0.4229 - acc: 0.9800 - val_loss: 3.2873 - val_acc: 0.4211\n",
      "Epoch 10/40\n",
      "150/150 [==============================] - 0s 615us/step - loss: 0.4162 - acc: 0.9867 - val_loss: 3.2832 - val_acc: 0.4211\n",
      "Epoch 11/40\n",
      "150/150 [==============================] - 0s 477us/step - loss: 0.4248 - acc: 0.9867 - val_loss: 3.2666 - val_acc: 0.4211\n",
      "Epoch 12/40\n",
      "150/150 [==============================] - 0s 450us/step - loss: 0.4610 - acc: 0.9867 - val_loss: 3.2593 - val_acc: 0.4211\n",
      "Epoch 13/40\n",
      "150/150 [==============================] - 0s 462us/step - loss: 0.4647 - acc: 0.9867 - val_loss: 3.2751 - val_acc: 0.3947\n",
      "Epoch 14/40\n",
      "150/150 [==============================] - 0s 600us/step - loss: 0.4581 - acc: 0.9933 - val_loss: 3.2969 - val_acc: 0.3684\n",
      "Epoch 15/40\n",
      "150/150 [==============================] - 0s 744us/step - loss: 0.4500 - acc: 1.0000 - val_loss: 3.2741 - val_acc: 0.3684\n",
      "Epoch 16/40\n",
      "150/150 [==============================] - 0s 476us/step - loss: 0.4942 - acc: 0.9667 - val_loss: 3.3190 - val_acc: 0.3684\n",
      "Epoch 17/40\n",
      "150/150 [==============================] - 0s 510us/step - loss: 0.4552 - acc: 0.9933 - val_loss: 3.3519 - val_acc: 0.3947\n",
      "Epoch 18/40\n",
      "150/150 [==============================] - 0s 595us/step - loss: 0.5045 - acc: 0.9667 - val_loss: 3.3800 - val_acc: 0.3947\n",
      "Epoch 19/40\n",
      "150/150 [==============================] - 0s 629us/step - loss: 0.4856 - acc: 0.9800 - val_loss: 3.3538 - val_acc: 0.3947\n",
      "Epoch 20/40\n",
      "150/150 [==============================] - 0s 792us/step - loss: 0.5063 - acc: 0.9733 - val_loss: 3.3147 - val_acc: 0.3947\n",
      "Epoch 21/40\n",
      "150/150 [==============================] - 0s 613us/step - loss: 0.6339 - acc: 0.9333 - val_loss: 3.2996 - val_acc: 0.3947\n",
      "Epoch 22/40\n",
      "150/150 [==============================] - 0s 610us/step - loss: 0.5118 - acc: 0.9600 - val_loss: 3.2865 - val_acc: 0.3947\n",
      "Epoch 23/40\n",
      "150/150 [==============================] - 0s 907us/step - loss: 0.5225 - acc: 0.9867 - val_loss: 3.2957 - val_acc: 0.3947\n",
      "Epoch 24/40\n",
      "150/150 [==============================] - 0s 686us/step - loss: 0.5280 - acc: 0.9800 - val_loss: 3.3008 - val_acc: 0.3947\n",
      "Epoch 25/40\n",
      "150/150 [==============================] - 0s 807us/step - loss: 0.5009 - acc: 0.9733 - val_loss: 3.3362 - val_acc: 0.3947\n",
      "Epoch 26/40\n",
      "150/150 [==============================] - 0s 654us/step - loss: 0.5848 - acc: 0.9667 - val_loss: 3.3617 - val_acc: 0.3947\n",
      "Epoch 27/40\n",
      "150/150 [==============================] - 0s 636us/step - loss: 0.5235 - acc: 0.9867 - val_loss: 3.3420 - val_acc: 0.3947\n",
      "Epoch 28/40\n",
      "150/150 [==============================] - 0s 364us/step - loss: 0.4746 - acc: 0.9933 - val_loss: 3.3313 - val_acc: 0.3947\n",
      "Epoch 29/40\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.4913 - acc: 0.984 - 0s 503us/step - loss: 0.4799 - acc: 0.9933 - val_loss: 3.3301 - val_acc: 0.3947\n",
      "Epoch 30/40\n",
      "150/150 [==============================] - 0s 426us/step - loss: 0.4714 - acc: 0.9800 - val_loss: 3.3183 - val_acc: 0.3947\n",
      "Epoch 31/40\n",
      "150/150 [==============================] - 0s 452us/step - loss: 0.4687 - acc: 0.9800 - val_loss: 3.3318 - val_acc: 0.3947\n",
      "Epoch 32/40\n",
      "150/150 [==============================] - 0s 635us/step - loss: 0.4648 - acc: 0.9800 - val_loss: 3.3151 - val_acc: 0.3947\n",
      "Epoch 33/40\n",
      "150/150 [==============================] - 0s 667us/step - loss: 0.4762 - acc: 0.9733 - val_loss: 3.3247 - val_acc: 0.3947\n",
      "Epoch 34/40\n",
      "150/150 [==============================] - 0s 515us/step - loss: 0.4355 - acc: 0.9867 - val_loss: 3.3102 - val_acc: 0.3947\n",
      "Epoch 35/40\n",
      "150/150 [==============================] - 0s 727us/step - loss: 0.5047 - acc: 0.9533 - val_loss: 3.2652 - val_acc: 0.3947\n",
      "Epoch 36/40\n",
      "150/150 [==============================] - 0s 838us/step - loss: 0.4492 - acc: 0.9867 - val_loss: 3.2736 - val_acc: 0.4211\n",
      "Epoch 37/40\n",
      "150/150 [==============================] - 0s 587us/step - loss: 0.4671 - acc: 0.9733 - val_loss: 3.2856 - val_acc: 0.4211\n",
      "Epoch 38/40\n",
      "150/150 [==============================] - 0s 516us/step - loss: 0.4851 - acc: 0.9800 - val_loss: 3.2912 - val_acc: 0.4211\n",
      "Epoch 39/40\n",
      "150/150 [==============================] - 0s 517us/step - loss: 0.5349 - acc: 0.9667 - val_loss: 3.3011 - val_acc: 0.4211\n",
      "Epoch 40/40\n",
      "150/150 [==============================] - 0s 863us/step - loss: 0.4949 - acc: 0.9800 - val_loss: 3.2886 - val_acc: 0.4211\n",
      "Train on 150 samples, validate on 38 samples\n",
      "Epoch 1/80\n",
      "150/150 [==============================] - 0s 590us/step - loss: 0.5171 - acc: 0.9867 - val_loss: 3.3035 - val_acc: 0.4211\n",
      "Epoch 2/80\n",
      "150/150 [==============================] - 0s 443us/step - loss: 0.5306 - acc: 0.9800 - val_loss: 3.3433 - val_acc: 0.4211\n",
      "Epoch 3/80\n",
      "150/150 [==============================] - 0s 459us/step - loss: 0.4843 - acc: 0.9800 - val_loss: 3.3287 - val_acc: 0.4211\n",
      "Epoch 4/80\n",
      "150/150 [==============================] - 0s 434us/step - loss: 0.4903 - acc: 0.9800 - val_loss: 3.2744 - val_acc: 0.4211\n",
      "Epoch 5/80\n",
      "150/150 [==============================] - 0s 682us/step - loss: 0.5043 - acc: 0.9733 - val_loss: 3.2832 - val_acc: 0.4211\n",
      "Epoch 6/80\n",
      "150/150 [==============================] - 0s 535us/step - loss: 0.4576 - acc: 0.9867 - val_loss: 3.2569 - val_acc: 0.4211\n",
      "Epoch 7/80\n",
      "150/150 [==============================] - 0s 588us/step - loss: 0.4857 - acc: 0.9667 - val_loss: 3.2296 - val_acc: 0.3947\n",
      "Epoch 8/80\n",
      "150/150 [==============================] - 0s 490us/step - loss: 0.4603 - acc: 0.9867 - val_loss: 3.2185 - val_acc: 0.3947\n",
      "Epoch 9/80\n",
      "150/150 [==============================] - 0s 464us/step - loss: 0.4375 - acc: 0.9733 - val_loss: 3.2108 - val_acc: 0.3947\n",
      "Epoch 10/80\n",
      "150/150 [==============================] - 0s 588us/step - loss: 0.4381 - acc: 0.9800 - val_loss: 3.1962 - val_acc: 0.3947\n",
      "Epoch 11/80\n",
      "150/150 [==============================] - 0s 505us/step - loss: 0.3949 - acc: 0.9933 - val_loss: 3.1666 - val_acc: 0.3947\n",
      "Epoch 12/80\n",
      "150/150 [==============================] - 0s 469us/step - loss: 0.4026 - acc: 0.9800 - val_loss: 3.1439 - val_acc: 0.3947\n",
      "Epoch 13/80\n",
      "150/150 [==============================] - 0s 682us/step - loss: 0.3679 - acc: 0.9933 - val_loss: 3.1371 - val_acc: 0.3947\n",
      "Epoch 14/80\n",
      "150/150 [==============================] - 0s 456us/step - loss: 0.3705 - acc: 0.9933 - val_loss: 3.1370 - val_acc: 0.3947\n",
      "Epoch 15/80\n",
      "150/150 [==============================] - 0s 471us/step - loss: 0.3725 - acc: 0.9867 - val_loss: 3.1569 - val_acc: 0.3947\n",
      "Epoch 16/80\n",
      "150/150 [==============================] - 0s 618us/step - loss: 0.3764 - acc: 0.9867 - val_loss: 3.1404 - val_acc: 0.3947\n",
      "Epoch 17/80\n",
      "150/150 [==============================] - 0s 636us/step - loss: 0.3826 - acc: 0.9867 - val_loss: 3.0857 - val_acc: 0.4211\n",
      "Epoch 18/80\n",
      "150/150 [==============================] - 0s 553us/step - loss: 0.3727 - acc: 0.9867 - val_loss: 3.0683 - val_acc: 0.4474\n",
      "Epoch 19/80\n",
      "150/150 [==============================] - 0s 842us/step - loss: 0.4309 - acc: 0.9733 - val_loss: 3.0399 - val_acc: 0.4211\n",
      "Epoch 20/80\n",
      "150/150 [==============================] - 0s 728us/step - loss: 0.4521 - acc: 0.9800 - val_loss: 3.0779 - val_acc: 0.4211\n",
      "Epoch 21/80\n",
      "150/150 [==============================] - 0s 561us/step - loss: 0.3917 - acc: 0.9867 - val_loss: 3.1487 - val_acc: 0.4474\n",
      "Epoch 22/80\n",
      "150/150 [==============================] - 0s 549us/step - loss: 0.4953 - acc: 0.9533 - val_loss: 3.2441 - val_acc: 0.4474\n",
      "Epoch 23/80\n",
      "150/150 [==============================] - 0s 478us/step - loss: 0.4254 - acc: 0.9667 - val_loss: 3.2342 - val_acc: 0.4474\n",
      "Epoch 24/80\n",
      "150/150 [==============================] - 0s 468us/step - loss: 0.4506 - acc: 0.9800 - val_loss: 3.2359 - val_acc: 0.4474\n",
      "Epoch 25/80\n",
      "150/150 [==============================] - 0s 683us/step - loss: 0.4938 - acc: 0.9467 - val_loss: 3.2115 - val_acc: 0.4474\n",
      "Epoch 26/80\n",
      "150/150 [==============================] - 0s 698us/step - loss: 0.4318 - acc: 0.9800 - val_loss: 3.1499 - val_acc: 0.4474\n",
      "Epoch 27/80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150/150 [==============================] - 0s 464us/step - loss: 0.4161 - acc: 0.9867 - val_loss: 3.1406 - val_acc: 0.4211\n",
      "Epoch 28/80\n",
      "150/150 [==============================] - 0s 604us/step - loss: 0.3941 - acc: 0.9867 - val_loss: 3.1506 - val_acc: 0.3947\n",
      "Epoch 29/80\n",
      "150/150 [==============================] - 0s 499us/step - loss: 0.3959 - acc: 0.9867 - val_loss: 3.1769 - val_acc: 0.3947\n",
      "Epoch 30/80\n",
      "150/150 [==============================] - 0s 677us/step - loss: 0.3691 - acc: 0.9933 - val_loss: 3.1622 - val_acc: 0.3947\n",
      "Epoch 31/80\n",
      "150/150 [==============================] - 0s 655us/step - loss: 0.3620 - acc: 0.9867 - val_loss: 3.1208 - val_acc: 0.3947\n",
      "Epoch 32/80\n",
      "150/150 [==============================] - 0s 543us/step - loss: 0.4000 - acc: 0.9600 - val_loss: 3.0863 - val_acc: 0.3947\n",
      "Epoch 33/80\n",
      "150/150 [==============================] - 0s 636us/step - loss: 0.3776 - acc: 0.9933 - val_loss: 3.0521 - val_acc: 0.3684\n",
      "Epoch 34/80\n",
      "150/150 [==============================] - 0s 557us/step - loss: 0.3992 - acc: 0.9800 - val_loss: 3.0414 - val_acc: 0.3947\n",
      "Epoch 35/80\n",
      "150/150 [==============================] - 0s 610us/step - loss: 0.4015 - acc: 0.9867 - val_loss: 3.0231 - val_acc: 0.3947\n",
      "Epoch 36/80\n",
      "150/150 [==============================] - 0s 539us/step - loss: 0.4208 - acc: 0.9667 - val_loss: 3.0491 - val_acc: 0.3947\n",
      "Epoch 37/80\n",
      "150/150 [==============================] - 0s 500us/step - loss: 0.3792 - acc: 0.9933 - val_loss: 3.1744 - val_acc: 0.3947\n",
      "Epoch 38/80\n",
      "150/150 [==============================] - 0s 631us/step - loss: 0.3974 - acc: 0.9933 - val_loss: 3.2503 - val_acc: 0.3947\n",
      "Epoch 39/80\n",
      "150/150 [==============================] - 0s 523us/step - loss: 0.4219 - acc: 0.9733 - val_loss: 3.2996 - val_acc: 0.3947\n",
      "Epoch 40/80\n",
      "150/150 [==============================] - 0s 512us/step - loss: 0.4425 - acc: 0.9667 - val_loss: 3.3202 - val_acc: 0.3947\n",
      "Epoch 41/80\n",
      "150/150 [==============================] - 0s 490us/step - loss: 0.4971 - acc: 0.9733 - val_loss: 3.3961 - val_acc: 0.3947\n",
      "Epoch 42/80\n",
      "150/150 [==============================] - 0s 552us/step - loss: 0.5497 - acc: 0.9600 - val_loss: 3.5263 - val_acc: 0.3684\n",
      "Epoch 43/80\n",
      "150/150 [==============================] - 0s 655us/step - loss: 0.5636 - acc: 0.9733 - val_loss: 3.5682 - val_acc: 0.3947\n",
      "Epoch 44/80\n",
      "150/150 [==============================] - 0s 656us/step - loss: 0.6606 - acc: 0.9400 - val_loss: 3.5974 - val_acc: 0.4211\n",
      "Epoch 45/80\n",
      "150/150 [==============================] - 0s 645us/step - loss: 0.6171 - acc: 0.9467 - val_loss: 3.5456 - val_acc: 0.3947\n",
      "Epoch 46/80\n",
      "150/150 [==============================] - 0s 953us/step - loss: 0.6023 - acc: 0.9600 - val_loss: 3.5339 - val_acc: 0.3947\n",
      "Epoch 47/80\n",
      "150/150 [==============================] - 0s 490us/step - loss: 0.5985 - acc: 0.9667 - val_loss: 3.4946 - val_acc: 0.3947\n",
      "Epoch 48/80\n",
      "150/150 [==============================] - 0s 553us/step - loss: 0.6094 - acc: 0.9733 - val_loss: 3.4602 - val_acc: 0.3947\n",
      "Epoch 49/80\n",
      "150/150 [==============================] - 0s 721us/step - loss: 0.5542 - acc: 0.9800 - val_loss: 3.4969 - val_acc: 0.3947\n",
      "Epoch 50/80\n",
      "150/150 [==============================] - 0s 499us/step - loss: 0.5953 - acc: 0.9667 - val_loss: 3.4578 - val_acc: 0.3947\n",
      "Epoch 51/80\n",
      "150/150 [==============================] - 0s 556us/step - loss: 0.5816 - acc: 0.9667 - val_loss: 3.4741 - val_acc: 0.4211\n",
      "Epoch 52/80\n",
      "150/150 [==============================] - 0s 757us/step - loss: 0.5422 - acc: 0.9800 - val_loss: 3.4852 - val_acc: 0.4211\n",
      "Epoch 53/80\n",
      "150/150 [==============================] - 0s 678us/step - loss: 0.5941 - acc: 0.9533 - val_loss: 3.4960 - val_acc: 0.3947\n",
      "Epoch 54/80\n",
      "150/150 [==============================] - 0s 674us/step - loss: 0.5387 - acc: 0.9800 - val_loss: 3.5145 - val_acc: 0.3947\n",
      "Epoch 55/80\n",
      "150/150 [==============================] - 0s 466us/step - loss: 0.5213 - acc: 0.9867 - val_loss: 3.4737 - val_acc: 0.3947\n",
      "Epoch 56/80\n",
      "150/150 [==============================] - 0s 500us/step - loss: 0.5131 - acc: 0.9867 - val_loss: 3.4322 - val_acc: 0.3947\n",
      "Epoch 57/80\n",
      "150/150 [==============================] - 0s 529us/step - loss: 0.4851 - acc: 0.9867 - val_loss: 3.3926 - val_acc: 0.3947\n",
      "Epoch 58/80\n",
      "150/150 [==============================] - 0s 744us/step - loss: 0.4891 - acc: 0.9867 - val_loss: 3.3739 - val_acc: 0.3947\n",
      "Epoch 59/80\n",
      "150/150 [==============================] - 0s 598us/step - loss: 0.4799 - acc: 0.9667 - val_loss: 3.3457 - val_acc: 0.3947\n",
      "Epoch 60/80\n",
      "150/150 [==============================] - 0s 638us/step - loss: 0.4497 - acc: 0.9800 - val_loss: 3.3202 - val_acc: 0.3947\n",
      "Epoch 61/80\n",
      "150/150 [==============================] - 0s 672us/step - loss: 0.4448 - acc: 0.9800 - val_loss: 3.2987 - val_acc: 0.3947\n",
      "Epoch 62/80\n",
      "150/150 [==============================] - 0s 669us/step - loss: 0.4180 - acc: 0.9867 - val_loss: 3.3106 - val_acc: 0.3947\n",
      "Epoch 63/80\n",
      "150/150 [==============================] - 0s 688us/step - loss: 0.5227 - acc: 0.9667 - val_loss: 3.3080 - val_acc: 0.3947\n",
      "Epoch 64/80\n",
      "150/150 [==============================] - 0s 512us/step - loss: 0.4846 - acc: 0.9733 - val_loss: 3.2461 - val_acc: 0.3947\n",
      "Epoch 65/80\n",
      "150/150 [==============================] - 0s 396us/step - loss: 0.4648 - acc: 0.9867 - val_loss: 3.2326 - val_acc: 0.3947\n",
      "Epoch 66/80\n",
      "150/150 [==============================] - 0s 397us/step - loss: 0.5095 - acc: 0.9800 - val_loss: 3.2940 - val_acc: 0.3947\n",
      "Epoch 67/80\n",
      "150/150 [==============================] - 0s 466us/step - loss: 0.5341 - acc: 0.9667 - val_loss: 3.3744 - val_acc: 0.3947\n",
      "Epoch 68/80\n",
      "150/150 [==============================] - 0s 498us/step - loss: 0.5379 - acc: 0.9733 - val_loss: 3.4348 - val_acc: 0.3947\n",
      "Epoch 69/80\n",
      "150/150 [==============================] - 0s 441us/step - loss: 0.5592 - acc: 0.9800 - val_loss: 3.4441 - val_acc: 0.3947\n",
      "Epoch 70/80\n",
      "150/150 [==============================] - 0s 521us/step - loss: 0.5922 - acc: 0.9533 - val_loss: 3.3963 - val_acc: 0.3947\n",
      "Epoch 71/80\n",
      "150/150 [==============================] - 0s 524us/step - loss: 0.5664 - acc: 0.9800 - val_loss: 3.3538 - val_acc: 0.3684\n",
      "Epoch 72/80\n",
      "150/150 [==============================] - 0s 514us/step - loss: 0.6524 - acc: 0.9533 - val_loss: 3.3925 - val_acc: 0.3684\n",
      "Epoch 73/80\n",
      "150/150 [==============================] - 0s 489us/step - loss: 0.5547 - acc: 0.9667 - val_loss: 3.4521 - val_acc: 0.3684\n",
      "Epoch 74/80\n",
      "150/150 [==============================] - 0s 432us/step - loss: 0.6187 - acc: 0.9533 - val_loss: 3.4840 - val_acc: 0.3684\n",
      "Epoch 75/80\n",
      "150/150 [==============================] - 0s 417us/step - loss: 0.5561 - acc: 0.9867 - val_loss: 3.4709 - val_acc: 0.3684\n",
      "Epoch 76/80\n",
      "150/150 [==============================] - 0s 595us/step - loss: 0.5142 - acc: 0.9867 - val_loss: 3.4628 - val_acc: 0.3684\n",
      "Epoch 77/80\n",
      "150/150 [==============================] - 0s 479us/step - loss: 0.5121 - acc: 0.9867 - val_loss: 3.4563 - val_acc: 0.3684\n",
      "Epoch 78/80\n",
      "150/150 [==============================] - 0s 465us/step - loss: 0.5085 - acc: 0.9867 - val_loss: 3.4368 - val_acc: 0.3684\n",
      "Epoch 79/80\n",
      "150/150 [==============================] - 0s 421us/step - loss: 0.5329 - acc: 0.9667 - val_loss: 3.4134 - val_acc: 0.3684\n",
      "Epoch 80/80\n",
      "150/150 [==============================] - 0s 490us/step - loss: 0.5294 - acc: 0.9733 - val_loss: 3.3844 - val_acc: 0.3684\n",
      "Train on 150 samples, validate on 38 samples\n",
      "Epoch 1/120\n",
      "150/150 [==============================] - 0s 513us/step - loss: 0.4656 - acc: 0.9933 - val_loss: 3.3687 - val_acc: 0.3684\n",
      "Epoch 2/120\n",
      "150/150 [==============================] - 0s 475us/step - loss: 0.4767 - acc: 0.9867 - val_loss: 3.3733 - val_acc: 0.3684\n",
      "Epoch 3/120\n",
      "150/150 [==============================] - 0s 459us/step - loss: 0.4519 - acc: 0.9800 - val_loss: 3.3787 - val_acc: 0.3684\n",
      "Epoch 4/120\n",
      "150/150 [==============================] - 0s 433us/step - loss: 0.4636 - acc: 0.9733 - val_loss: 3.4481 - val_acc: 0.3684\n",
      "Epoch 5/120\n",
      "150/150 [==============================] - 0s 403us/step - loss: 0.4889 - acc: 0.9800 - val_loss: 3.5949 - val_acc: 0.3947\n",
      "Epoch 6/120\n",
      "150/150 [==============================] - 0s 451us/step - loss: 0.5524 - acc: 0.9733 - val_loss: 3.6622 - val_acc: 0.3947\n",
      "Epoch 7/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150/150 [==============================] - 0s 450us/step - loss: 0.5405 - acc: 0.9600 - val_loss: 3.6459 - val_acc: 0.3947\n",
      "Epoch 8/120\n",
      "150/150 [==============================] - 0s 406us/step - loss: 0.5494 - acc: 0.9800 - val_loss: 3.4786 - val_acc: 0.4211\n",
      "Epoch 9/120\n",
      "150/150 [==============================] - 0s 486us/step - loss: 0.5497 - acc: 0.9800 - val_loss: 3.3582 - val_acc: 0.3947\n",
      "Epoch 10/120\n",
      "150/150 [==============================] - 0s 367us/step - loss: 0.5911 - acc: 0.9733 - val_loss: 3.2840 - val_acc: 0.3947\n",
      "Epoch 11/120\n",
      "150/150 [==============================] - 0s 405us/step - loss: 0.5928 - acc: 0.9800 - val_loss: 3.2561 - val_acc: 0.3947\n",
      "Epoch 12/120\n",
      "150/150 [==============================] - 0s 471us/step - loss: 0.5757 - acc: 0.9733 - val_loss: 3.2225 - val_acc: 0.4211\n",
      "Epoch 13/120\n",
      "150/150 [==============================] - 0s 528us/step - loss: 0.5838 - acc: 0.9800 - val_loss: 3.1961 - val_acc: 0.3947\n",
      "Epoch 14/120\n",
      "150/150 [==============================] - 0s 427us/step - loss: 0.5410 - acc: 0.9933 - val_loss: 3.1799 - val_acc: 0.3947\n",
      "Epoch 15/120\n",
      "150/150 [==============================] - 0s 515us/step - loss: 0.5305 - acc: 0.9867 - val_loss: 3.1805 - val_acc: 0.3947\n",
      "Epoch 16/120\n",
      "150/150 [==============================] - 0s 519us/step - loss: 0.5182 - acc: 0.9800 - val_loss: 3.1976 - val_acc: 0.3947\n",
      "Epoch 17/120\n",
      "150/150 [==============================] - 0s 422us/step - loss: 0.5124 - acc: 0.9867 - val_loss: 3.2302 - val_acc: 0.3947\n",
      "Epoch 18/120\n",
      "150/150 [==============================] - 0s 580us/step - loss: 0.4984 - acc: 0.9800 - val_loss: 3.2490 - val_acc: 0.3947\n",
      "Epoch 19/120\n",
      "150/150 [==============================] - 0s 519us/step - loss: 0.5163 - acc: 0.9800 - val_loss: 3.2493 - val_acc: 0.3947\n",
      "Epoch 20/120\n",
      "150/150 [==============================] - 0s 454us/step - loss: 0.4716 - acc: 0.9867 - val_loss: 3.2098 - val_acc: 0.3947\n",
      "Epoch 21/120\n",
      "150/150 [==============================] - 0s 467us/step - loss: 0.4675 - acc: 0.9867 - val_loss: 3.1693 - val_acc: 0.3947\n",
      "Epoch 22/120\n",
      "150/150 [==============================] - 0s 425us/step - loss: 0.4684 - acc: 0.9667 - val_loss: 3.1326 - val_acc: 0.3947\n",
      "Epoch 23/120\n",
      "150/150 [==============================] - 0s 451us/step - loss: 0.4719 - acc: 0.9800 - val_loss: 3.1438 - val_acc: 0.3947\n",
      "Epoch 24/120\n",
      "150/150 [==============================] - 0s 449us/step - loss: 0.4523 - acc: 0.9867 - val_loss: 3.1582 - val_acc: 0.3947\n",
      "Epoch 25/120\n",
      "150/150 [==============================] - 0s 509us/step - loss: 0.4371 - acc: 1.0000 - val_loss: 3.1636 - val_acc: 0.3947\n",
      "Epoch 26/120\n",
      "150/150 [==============================] - 0s 526us/step - loss: 0.4738 - acc: 0.9933 - val_loss: 3.1856 - val_acc: 0.3947\n",
      "Epoch 27/120\n",
      "150/150 [==============================] - 0s 470us/step - loss: 0.4920 - acc: 0.9733 - val_loss: 3.1971 - val_acc: 0.3947\n",
      "Epoch 28/120\n",
      "150/150 [==============================] - 0s 568us/step - loss: 0.4938 - acc: 0.9867 - val_loss: 3.2026 - val_acc: 0.3947\n",
      "Epoch 29/120\n",
      "150/150 [==============================] - 0s 461us/step - loss: 0.4790 - acc: 0.9867 - val_loss: 3.2401 - val_acc: 0.3947\n",
      "Epoch 30/120\n",
      "150/150 [==============================] - 0s 407us/step - loss: 0.5395 - acc: 0.9667 - val_loss: 3.2065 - val_acc: 0.3947\n",
      "Epoch 31/120\n",
      "150/150 [==============================] - 0s 410us/step - loss: 0.5592 - acc: 0.9800 - val_loss: 3.2092 - val_acc: 0.3947\n",
      "Epoch 32/120\n",
      "150/150 [==============================] - 0s 389us/step - loss: 0.5607 - acc: 0.9667 - val_loss: 3.2034 - val_acc: 0.3947\n",
      "Epoch 33/120\n",
      "150/150 [==============================] - 0s 553us/step - loss: 0.5834 - acc: 0.9600 - val_loss: 3.2231 - val_acc: 0.4211\n",
      "Epoch 34/120\n",
      "150/150 [==============================] - 0s 564us/step - loss: 0.5594 - acc: 0.9800 - val_loss: 3.2456 - val_acc: 0.4474\n",
      "Epoch 35/120\n",
      "150/150 [==============================] - 0s 495us/step - loss: 0.5802 - acc: 0.9600 - val_loss: 3.2711 - val_acc: 0.4474\n",
      "Epoch 36/120\n",
      "150/150 [==============================] - 0s 473us/step - loss: 0.5986 - acc: 0.9667 - val_loss: 3.2604 - val_acc: 0.4474\n",
      "Epoch 37/120\n",
      "150/150 [==============================] - 0s 452us/step - loss: 0.5629 - acc: 0.9867 - val_loss: 3.2331 - val_acc: 0.4211\n",
      "Epoch 38/120\n",
      "150/150 [==============================] - 0s 591us/step - loss: 0.6027 - acc: 0.9533 - val_loss: 3.2274 - val_acc: 0.4211\n",
      "Epoch 39/120\n",
      "150/150 [==============================] - 0s 502us/step - loss: 0.5306 - acc: 0.9733 - val_loss: 3.2111 - val_acc: 0.4211\n",
      "Epoch 40/120\n",
      "150/150 [==============================] - 0s 434us/step - loss: 0.4973 - acc: 1.0000 - val_loss: 3.2324 - val_acc: 0.4211\n",
      "Epoch 41/120\n",
      "150/150 [==============================] - 0s 534us/step - loss: 0.5379 - acc: 0.9667 - val_loss: 3.2404 - val_acc: 0.4211\n",
      "Epoch 42/120\n",
      "150/150 [==============================] - 0s 448us/step - loss: 0.5323 - acc: 0.9733 - val_loss: 3.2341 - val_acc: 0.3947\n",
      "Epoch 43/120\n",
      "150/150 [==============================] - 0s 543us/step - loss: 0.5057 - acc: 0.9667 - val_loss: 3.2807 - val_acc: 0.3947\n",
      "Epoch 44/120\n",
      "150/150 [==============================] - 0s 470us/step - loss: 0.5346 - acc: 0.9667 - val_loss: 3.3205 - val_acc: 0.3684\n",
      "Epoch 45/120\n",
      "150/150 [==============================] - 0s 421us/step - loss: 0.4760 - acc: 1.0000 - val_loss: 3.3390 - val_acc: 0.3684\n",
      "Epoch 46/120\n",
      "150/150 [==============================] - 0s 461us/step - loss: 0.5631 - acc: 0.9600 - val_loss: 3.3510 - val_acc: 0.3684\n",
      "Epoch 47/120\n",
      "150/150 [==============================] - 0s 453us/step - loss: 0.5133 - acc: 0.9933 - val_loss: 3.3419 - val_acc: 0.3684\n",
      "Epoch 48/120\n",
      "150/150 [==============================] - 0s 454us/step - loss: 0.5122 - acc: 1.0000 - val_loss: 3.3217 - val_acc: 0.3684\n",
      "Epoch 49/120\n",
      "150/150 [==============================] - 0s 449us/step - loss: 0.5526 - acc: 0.9733 - val_loss: 3.2766 - val_acc: 0.3684\n",
      "Epoch 50/120\n",
      "150/150 [==============================] - 0s 401us/step - loss: 0.5118 - acc: 0.9800 - val_loss: 3.2582 - val_acc: 0.3684\n",
      "Epoch 51/120\n",
      "150/150 [==============================] - 0s 405us/step - loss: 0.5073 - acc: 0.9800 - val_loss: 3.2465 - val_acc: 0.3684\n",
      "Epoch 52/120\n",
      "150/150 [==============================] - 0s 412us/step - loss: 0.4772 - acc: 0.9933 - val_loss: 3.2681 - val_acc: 0.3684\n",
      "Epoch 53/120\n",
      "150/150 [==============================] - 0s 428us/step - loss: 0.5237 - acc: 0.9667 - val_loss: 3.2677 - val_acc: 0.3684\n",
      "Epoch 54/120\n",
      "150/150 [==============================] - 0s 412us/step - loss: 0.4643 - acc: 0.9933 - val_loss: 3.3105 - val_acc: 0.3684\n",
      "Epoch 55/120\n",
      "150/150 [==============================] - 0s 418us/step - loss: 0.5048 - acc: 0.9800 - val_loss: 3.3319 - val_acc: 0.3947\n",
      "Epoch 56/120\n",
      "150/150 [==============================] - 0s 466us/step - loss: 0.5125 - acc: 0.9800 - val_loss: 3.3703 - val_acc: 0.3947\n",
      "Epoch 57/120\n",
      "150/150 [==============================] - 0s 454us/step - loss: 0.5425 - acc: 0.9733 - val_loss: 3.4441 - val_acc: 0.3947\n",
      "Epoch 58/120\n",
      "150/150 [==============================] - 0s 425us/step - loss: 0.5287 - acc: 0.9867 - val_loss: 3.4738 - val_acc: 0.3947\n",
      "Epoch 59/120\n",
      "150/150 [==============================] - 0s 410us/step - loss: 0.5641 - acc: 0.9733 - val_loss: 3.5156 - val_acc: 0.3947\n",
      "Epoch 60/120\n",
      "150/150 [==============================] - 0s 424us/step - loss: 0.5939 - acc: 0.9800 - val_loss: 3.4938 - val_acc: 0.4211\n",
      "Epoch 61/120\n",
      "150/150 [==============================] - 0s 493us/step - loss: 0.5533 - acc: 0.9667 - val_loss: 3.4771 - val_acc: 0.4211\n",
      "Epoch 62/120\n",
      "150/150 [==============================] - 0s 395us/step - loss: 0.5627 - acc: 0.9667 - val_loss: 3.4526 - val_acc: 0.3947\n",
      "Epoch 63/120\n",
      "150/150 [==============================] - 0s 412us/step - loss: 0.4977 - acc: 0.9933 - val_loss: 3.4347 - val_acc: 0.3947\n",
      "Epoch 64/120\n",
      "150/150 [==============================] - 0s 407us/step - loss: 0.5617 - acc: 0.9533 - val_loss: 3.4309 - val_acc: 0.3947\n",
      "Epoch 65/120\n",
      "150/150 [==============================] - 0s 427us/step - loss: 0.4805 - acc: 0.9800 - val_loss: 3.4369 - val_acc: 0.3684\n",
      "Epoch 66/120\n",
      "150/150 [==============================] - 0s 460us/step - loss: 0.4754 - acc: 0.9867 - val_loss: 3.4436 - val_acc: 0.3684\n",
      "Epoch 67/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150/150 [==============================] - 0s 442us/step - loss: 0.4553 - acc: 0.9867 - val_loss: 3.4407 - val_acc: 0.3684\n",
      "Epoch 68/120\n",
      "150/150 [==============================] - 0s 417us/step - loss: 0.4278 - acc: 0.9933 - val_loss: 3.4219 - val_acc: 0.3684\n",
      "Epoch 69/120\n",
      "150/150 [==============================] - 0s 381us/step - loss: 0.4118 - acc: 0.9933 - val_loss: 3.3925 - val_acc: 0.3947\n",
      "Epoch 70/120\n",
      "150/150 [==============================] - 0s 493us/step - loss: 0.4637 - acc: 0.9600 - val_loss: 3.3310 - val_acc: 0.3947\n",
      "Epoch 71/120\n",
      "150/150 [==============================] - 0s 468us/step - loss: 0.4015 - acc: 0.9867 - val_loss: 3.3016 - val_acc: 0.3947\n",
      "Epoch 72/120\n",
      "150/150 [==============================] - 0s 473us/step - loss: 0.4486 - acc: 0.9533 - val_loss: 3.2451 - val_acc: 0.3947\n",
      "Epoch 73/120\n",
      "150/150 [==============================] - 0s 482us/step - loss: 0.4260 - acc: 0.9800 - val_loss: 3.1485 - val_acc: 0.3684\n",
      "Epoch 74/120\n",
      "150/150 [==============================] - 0s 421us/step - loss: 0.4120 - acc: 0.9933 - val_loss: 3.1515 - val_acc: 0.3947\n",
      "Epoch 75/120\n",
      "150/150 [==============================] - 0s 432us/step - loss: 0.4793 - acc: 0.9600 - val_loss: 3.1259 - val_acc: 0.3947\n",
      "Epoch 76/120\n",
      "150/150 [==============================] - 0s 516us/step - loss: 0.4765 - acc: 0.9733 - val_loss: 3.1263 - val_acc: 0.3947\n",
      "Epoch 77/120\n",
      "150/150 [==============================] - 0s 439us/step - loss: 0.4801 - acc: 0.9600 - val_loss: 3.1278 - val_acc: 0.3947\n",
      "Epoch 78/120\n",
      "150/150 [==============================] - 0s 464us/step - loss: 0.5166 - acc: 0.9733 - val_loss: 3.1320 - val_acc: 0.3947\n",
      "Epoch 79/120\n",
      "150/150 [==============================] - 0s 453us/step - loss: 0.4853 - acc: 0.9933 - val_loss: 3.1052 - val_acc: 0.3947\n",
      "Epoch 80/120\n",
      "150/150 [==============================] - 0s 410us/step - loss: 0.5095 - acc: 0.9733 - val_loss: 3.1379 - val_acc: 0.3947\n",
      "Epoch 81/120\n",
      "150/150 [==============================] - 0s 560us/step - loss: 0.4868 - acc: 0.9867 - val_loss: 3.1656 - val_acc: 0.3947\n",
      "Epoch 82/120\n",
      "150/150 [==============================] - 0s 461us/step - loss: 0.5097 - acc: 0.9733 - val_loss: 3.1565 - val_acc: 0.3947\n",
      "Epoch 83/120\n",
      "150/150 [==============================] - 0s 461us/step - loss: 0.4785 - acc: 0.9933 - val_loss: 3.1359 - val_acc: 0.3947\n",
      "Epoch 84/120\n",
      "150/150 [==============================] - 0s 465us/step - loss: 0.4682 - acc: 1.0000 - val_loss: 3.1202 - val_acc: 0.3684\n",
      "Epoch 85/120\n",
      "150/150 [==============================] - 0s 461us/step - loss: 0.4895 - acc: 0.9800 - val_loss: 3.0911 - val_acc: 0.3684\n",
      "Epoch 86/120\n",
      "150/150 [==============================] - 0s 481us/step - loss: 0.4363 - acc: 1.0000 - val_loss: 3.0455 - val_acc: 0.3684\n",
      "Epoch 87/120\n",
      "150/150 [==============================] - 0s 469us/step - loss: 0.4488 - acc: 0.9800 - val_loss: 3.0270 - val_acc: 0.3684\n",
      "Epoch 88/120\n",
      "150/150 [==============================] - 0s 421us/step - loss: 0.4382 - acc: 0.9800 - val_loss: 3.0179 - val_acc: 0.3947\n",
      "Epoch 89/120\n",
      "150/150 [==============================] - 0s 403us/step - loss: 0.3909 - acc: 1.0000 - val_loss: 3.0216 - val_acc: 0.3947\n",
      "Epoch 90/120\n",
      "150/150 [==============================] - 0s 382us/step - loss: 0.4033 - acc: 0.9867 - val_loss: 3.0462 - val_acc: 0.3947\n",
      "Epoch 91/120\n",
      "150/150 [==============================] - 0s 393us/step - loss: 0.3926 - acc: 0.9933 - val_loss: 3.0614 - val_acc: 0.3947\n",
      "Epoch 92/120\n",
      "150/150 [==============================] - 0s 479us/step - loss: 0.3877 - acc: 0.9867 - val_loss: 3.0679 - val_acc: 0.3947\n",
      "Epoch 93/120\n",
      "150/150 [==============================] - 0s 458us/step - loss: 0.3815 - acc: 0.9933 - val_loss: 3.0658 - val_acc: 0.3947\n",
      "Epoch 94/120\n",
      "150/150 [==============================] - 0s 441us/step - loss: 0.3659 - acc: 0.9933 - val_loss: 3.0801 - val_acc: 0.3947\n",
      "Epoch 95/120\n",
      "150/150 [==============================] - 0s 441us/step - loss: 0.4343 - acc: 0.9667 - val_loss: 3.0732 - val_acc: 0.3947\n",
      "Epoch 96/120\n",
      "150/150 [==============================] - 0s 407us/step - loss: 0.4088 - acc: 0.9733 - val_loss: 3.0919 - val_acc: 0.4211\n",
      "Epoch 97/120\n",
      "150/150 [==============================] - 0s 514us/step - loss: 0.4124 - acc: 0.9800 - val_loss: 3.1045 - val_acc: 0.3947\n",
      "Epoch 98/120\n",
      "150/150 [==============================] - 0s 450us/step - loss: 0.4229 - acc: 0.9800 - val_loss: 3.1201 - val_acc: 0.4211\n",
      "Epoch 99/120\n",
      "150/150 [==============================] - 0s 404us/step - loss: 0.4205 - acc: 0.9800 - val_loss: 3.1443 - val_acc: 0.4211\n",
      "Epoch 100/120\n",
      "150/150 [==============================] - 0s 417us/step - loss: 0.5061 - acc: 0.9533 - val_loss: 3.1787 - val_acc: 0.3947\n",
      "Epoch 101/120\n",
      "150/150 [==============================] - 0s 469us/step - loss: 0.4266 - acc: 0.9867 - val_loss: 3.2355 - val_acc: 0.3684\n",
      "Epoch 102/120\n",
      "150/150 [==============================] - 0s 515us/step - loss: 0.5056 - acc: 0.9600 - val_loss: 3.2808 - val_acc: 0.3684\n",
      "Epoch 103/120\n",
      "150/150 [==============================] - 0s 489us/step - loss: 0.5008 - acc: 0.9733 - val_loss: 3.2846 - val_acc: 0.3421\n",
      "Epoch 104/120\n",
      "150/150 [==============================] - 0s 436us/step - loss: 0.4716 - acc: 0.9800 - val_loss: 3.2777 - val_acc: 0.3684\n",
      "Epoch 105/120\n",
      "150/150 [==============================] - 0s 440us/step - loss: 0.4946 - acc: 0.9600 - val_loss: 3.2532 - val_acc: 0.3684\n",
      "Epoch 106/120\n",
      "150/150 [==============================] - 0s 459us/step - loss: 0.5039 - acc: 0.9733 - val_loss: 3.2263 - val_acc: 0.3684\n",
      "Epoch 107/120\n",
      "150/150 [==============================] - 0s 457us/step - loss: 0.5809 - acc: 0.9400 - val_loss: 3.2107 - val_acc: 0.3684\n",
      "Epoch 108/120\n",
      "150/150 [==============================] - 0s 410us/step - loss: 0.5355 - acc: 0.9800 - val_loss: 3.2169 - val_acc: 0.3684\n",
      "Epoch 109/120\n",
      "150/150 [==============================] - 0s 422us/step - loss: 0.5448 - acc: 0.9600 - val_loss: 3.2424 - val_acc: 0.3684\n",
      "Epoch 110/120\n",
      "150/150 [==============================] - 0s 496us/step - loss: 0.6095 - acc: 0.9467 - val_loss: 3.3032 - val_acc: 0.3684\n",
      "Epoch 111/120\n",
      "150/150 [==============================] - 0s 474us/step - loss: 0.5763 - acc: 0.9667 - val_loss: 3.1997 - val_acc: 0.3684\n",
      "Epoch 112/120\n",
      "150/150 [==============================] - 0s 458us/step - loss: 0.5592 - acc: 0.9800 - val_loss: 3.1870 - val_acc: 0.3684\n",
      "Epoch 113/120\n",
      "150/150 [==============================] - 0s 454us/step - loss: 0.5575 - acc: 0.9800 - val_loss: 3.2213 - val_acc: 0.3684\n",
      "Epoch 114/120\n",
      "150/150 [==============================] - 0s 431us/step - loss: 0.6254 - acc: 0.9733 - val_loss: 3.2320 - val_acc: 0.3684\n",
      "Epoch 115/120\n",
      "150/150 [==============================] - 0s 497us/step - loss: 0.5744 - acc: 0.9733 - val_loss: 3.2594 - val_acc: 0.3684\n",
      "Epoch 116/120\n",
      "150/150 [==============================] - 0s 467us/step - loss: 0.5861 - acc: 0.9733 - val_loss: 3.2923 - val_acc: 0.3684\n",
      "Epoch 117/120\n",
      "150/150 [==============================] - 0s 459us/step - loss: 0.5666 - acc: 0.9667 - val_loss: 3.3200 - val_acc: 0.3421\n",
      "Epoch 118/120\n",
      "150/150 [==============================] - 0s 446us/step - loss: 0.5357 - acc: 0.9867 - val_loss: 3.3979 - val_acc: 0.3684\n",
      "Epoch 119/120\n",
      "150/150 [==============================] - 0s 473us/step - loss: 0.5734 - acc: 0.9733 - val_loss: 3.4681 - val_acc: 0.3684\n",
      "Epoch 120/120\n",
      "150/150 [==============================] - 0s 439us/step - loss: 0.5815 - acc: 0.9600 - val_loss: 3.4749 - val_acc: 0.3684\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('reduce_false_pos', SelectFpr(alpha=0.75, score_func=<function f_classif at 0x1a2e6e78c8>)), ('deep', <__main__.DeepEstimator object at 0x1a1a6662b0>)])"
      ]
     },
     "execution_count": 1044,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deep_train_pipeline = Pipeline([\n",
    "    ('reduce_false_pos', SelectFpr(alpha=0.75)),\n",
    "    ('deep', DeepEstimator())\n",
    "])\n",
    "deep_train_pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1045,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1039e8da0>]"
      ]
     },
     "execution_count": 1045,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAD8CAYAAABkbJM/AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a1a59aeb8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def score(est, X, y, cutoff=0.75):\n",
    "    y_pred = est.predict(X)\n",
    "    # Invest if over 50% confident \n",
    "    y_pred_bool = bool_arr(y_pred, cutoff)\n",
    "    # Score correct if you make postive returns\n",
    "    y_true_bool = bool_arr(y, 0)\n",
    "    total = np.dot(y_pred_bool.reshape(X.shape[0],), y)\n",
    "    buy_all = np.dot(np.ones(X.shape[0]), y)\n",
    "    av = total/len(X)\n",
    "    buy_all_av = buy_all/len(X)\n",
    "#     print(confusion_matrix(y_true_bool, y_pred_bool))\n",
    "    return [av, buy_all_av]\n",
    "\n",
    "\n",
    "ys_attained = []\n",
    "ys_potential = []\n",
    "xs = []\n",
    "for i in np.linspace(0, 1):\n",
    "    sc = score(deep_train_pipeline, X_test, y_test_continuous,i)\n",
    "    ys_attained.append(sc[0])\n",
    "    ys_potential.append(sc[1])\n",
    "    xs.append(i)\n",
    "plt.plot(xs, ys_attained)\n",
    "plt.plot(xs, ys_potential)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1026,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1,\n",
       "       1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1])"
      ]
     },
     "execution_count": 1026,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bool_arr(deep_train_pipeline.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Factors to look into including: \n",
    "# \"BookValue\" = (Total Assets - Total Liabilities) / Number of shares outstanding\n",
    "# \"MarketCap\" = Market price per share * number of shares \n",
    "# \"DividendYield\" = Dividend / Market price per share \n",
    "# \"EarningsPerShare\" \n",
    "# \"PERatio2\" = Market price per share / earning per share \n",
    "# \"priceBook\" = Market price per share / ((Total Assets - Total Liabilities) / Number of shares outstanding)\n",
    "# \"PriceSales\" = MarketCap / Revenue \n",
    "# \"Ask\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# vocab = list(text_word_counts.steps[2][1].vocabulary_.keys())\n",
    "# import operator\n",
    "# iv_dict = [[vocab[i],-float(f)] for i,f in enumerate(clf.coef_)]\n",
    "# most_important_terms = sorted(iv_dict, key=operator.itemgetter(1))[0:100]\n",
    "# print(most_important_terms)\n",
    "# most_important_vocab = dict(most_important_terms).keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from joblib import Memory\n",
    "# %mkdir cachedir\n",
    "# location = './cachedir'\n",
    "# memory = Memory(location, verbose=0)\n",
    "# stock_name_date_mapping = memory.cache(stock_name_date_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # print(text_word_counts.steps[1][1].vocabulary_)\n",
    "# def get_excepted_stock_names(limit=-1):\n",
    "#     esn = {}\n",
    "#     for i, file in tqdm(enumerate(glob.glob('filing_texts/*')[0:limit])):\n",
    "#         stock_name = stock_name_from_filename(file)\n",
    "#         file_data = open(file, 'r').read()\n",
    "#         try:\n",
    "#             with open(file) as open_file:\n",
    "#                 file_data = [next(open_file) for x in range(3)]\n",
    "#             file_data = ''.join(file_data)\n",
    "#             time_data = file_data.split(\"\\n\")[2][5:14]\n",
    "#         except:\n",
    "#             esn[i] = stock_name\n",
    "#             continue\n",
    "#     return esn\n",
    "\n",
    "# def get_x_mask(X, y, filenames):\n",
    "#     no_date_indices = []\n",
    "#     if X.shape[0] != len(y):\n",
    "#         stock_names = FilenamesToStockNamesTransformer().transform(filenames)\n",
    "#         for i, stock_name in enumerate(stock_names): \n",
    "#             try:\n",
    "#                 date = spm[stock_name]['date']\n",
    "#             except:\n",
    "#                 no_date_indices.append(i)\n",
    "#         mask = np.ones(X.shape[0], dtype=bool)\n",
    "#         mask[no_date_indices] = False\n",
    "#         X_mask = X[mask]\n",
    "#     else:\n",
    "#         X_mask = X\n",
    "#     return X_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
