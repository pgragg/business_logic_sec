{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, TfidfTransformer, CountVectorizer\n",
    "import glob\n",
    "import os \n",
    "import pandas as pd\n",
    "import datetime\n",
    "from scipy.stats import iqr\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import metrics\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "spm = None\n",
    "english_words = None\n",
    "excepted_stock_names = []\n",
    "limit = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.layers import Dense, Dropout, RepeatVector, BatchNormalization, Convolution1D, Flatten, Lambda, Permute, MaxPooling1D, AlphaDropout\n",
    "from keras.models import Sequential\n",
    "from keras.utils import to_categorical\n",
    "import keras.backend as K\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import load_model\n",
    "from tqdm import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def stock_name_from_filename(filename):\n",
    "    return filename.split('/')[-1].split('_')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def stock_name_date_mapping(limit=-1):\n",
    "    spm = {}\n",
    "    for i, file in enumerate(glob.glob('filing_texts/*')[0:limit]):\n",
    "        try:\n",
    "            stock_name = stock_name_from_filename(file)\n",
    "            file_data = open(file, 'r').read()[:200]\n",
    "            time_data = file_data.split('<ACCEPTANCE-DATETIME>')[1].split('\\\\n')[0][:8]\n",
    "            year = time_data[0:4]\n",
    "            month = time_data[4:6]\n",
    "            day = time_data[6:8]\n",
    "            date_stamp = (\"%s-%s-%s\" %(year, month, day))\n",
    "            spm[stock_name] = {'date': date_stamp}\n",
    "        except:\n",
    "            print(\"No data for \", stock_name)\n",
    "            excepted_stock_names.append(stock_name)\n",
    "            continue\n",
    "    return spm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class PriceData:\n",
    "    def __init__(self, stock_name):\n",
    "        self.stock_name = stock_name\n",
    "        try:\n",
    "            self.price_data = pd.read_csv('prices/' + stock_name + '.csv')\n",
    "        except:\n",
    "            self.price_data = pd.DataFrame.from_dict({})\n",
    "\n",
    "    def on_date(self, date, market_time = 'open'): \n",
    "        try:\n",
    "            return float(self.price_data.loc[self.price_data['date'] == date][market_time])\n",
    "        except: \n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FilenamesToStockNamesTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        return None\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def fit_transform(self, X, y=None):\n",
    "        return self.fit(X, y).transform(X)\n",
    "        \n",
    "    def transform(self, X):\n",
    "        output = []\n",
    "        for filename in X:\n",
    "            stock_name = self.__stock_name_from_filename(filename)\n",
    "            output.append(stock_name)\n",
    "        return output\n",
    "    def __stock_name_from_filename(self, filename):\n",
    "        return filename.split('/')[-1].split('_')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Used in y pipeline\n",
    "class SpmToFileNamesTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        return None\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def fit_transform(self, X, y=None):\n",
    "        return self.fit(X, y).transform(X)\n",
    "    def transform(self, X):\n",
    "        output = []\n",
    "        for key in X.keys():\n",
    "            date = X[key]['date']\n",
    "            output.append(f'filing_texts/{key}_{date}')\n",
    "        return output\n",
    "    \n",
    "class SpmToStockNamesTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        return None\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def fit_transform(self, X, y=None):\n",
    "        return self.fit(X, y).transform(X)\n",
    "        \n",
    "    def transform(self, X):\n",
    "        return X.keys()\n",
    "    \n",
    "class StockNamesToFileNamesTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        return None\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def fit_transform(self, X, y=None):\n",
    "        return self.fit(X, y).transform(X)\n",
    "        \n",
    "    def transform(self, X):\n",
    "        return [f'filing_texts/{stock_name}' for stock_name in X]\n",
    "    \n",
    "class MapStockNamesToDatesTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, start_int, end_int):\n",
    "        self.start_int = start_int\n",
    "        self.end_int = end_int\n",
    "        self.range = (range(start_int, end_int))\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def fit_transform(self, X, y=None):\n",
    "        return self.fit(X, y).transform(X)\n",
    "        \n",
    "    def transform(self, X):\n",
    "        output = {}\n",
    "        for i, stock_name in enumerate(X):\n",
    "            try:\n",
    "                date = spm[stock_name]['date']\n",
    "            except:\n",
    "                continue # Ignore stocks which don't have a date\n",
    "            dates = []\n",
    "            for delta in self.range:\n",
    "                date_delta = datetime.timedelta(days=delta)\n",
    "                date_string = datetime.datetime.strptime(date, '%Y-%m-%d').date()\n",
    "                dates.append(str(date_string + date_delta))\n",
    "            output[stock_name] = dates\n",
    "        return output\n",
    "class StockNameDatesMapToPricesListTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        return None\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def fit_transform(self, X, y=None):\n",
    "        return self.fit(X, y).transform(X)\n",
    "        \n",
    "    def transform(self, X):\n",
    "        output = []\n",
    "        for stock_name in X.keys():\n",
    "            prices = []\n",
    "            for date in X[stock_name]:\n",
    "                prices.append(PriceData(stock_name).on_date(date, 'close'))\n",
    "            output.append(prices)\n",
    "        return np.array(output)\n",
    "class LabelsTransform(BaseEstimator, TransformerMixin):\n",
    "    # Returns the interquartile-range and median.\n",
    "    def __init__(self):\n",
    "        return None\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def fit_transform(self, X, y=None):\n",
    "        self.fit(X, y).transform(X)\n",
    "        \n",
    "    def transform(self, X):\n",
    "        # ldcom = last_day_change_over_median\n",
    "        ldcom = []\n",
    "        for prices in X:\n",
    "            this_median = np.median(prices[0:-3])\n",
    "            ldcom.append(((prices[-1]-this_median)/this_median))\n",
    "        return np.array(ldcom)\n",
    "            \n",
    "class StatisticalMeasuresTransformer(BaseEstimator, TransformerMixin):\n",
    "    # Returns the interquartile-range and median.\n",
    "    def __init__(self):\n",
    "        return None\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        # ldcom = last_day_change_over_median\n",
    "        \n",
    "        output = []\n",
    "        self.iqr_var = []\n",
    "        self.median = []\n",
    "        for prices in X:\n",
    "            this_iqr = iqr(prices[0:-3])\n",
    "            this_median = np.median(prices[0:-3])\n",
    "            self.iqr_var.append(this_iqr)\n",
    "            self.median.append(this_median)\n",
    "        return self\n",
    "    \n",
    "    def fit_transform(self, X, y=None):\n",
    "        return self.fit(X, y).transform(X)\n",
    "        \n",
    "    def transform(self, X):\n",
    "        X_output = []\n",
    "        for i, prices in enumerate(X):\n",
    "            stats   = []\n",
    "            iqr_var = self.iqr_var[i] or iqr(prices)\n",
    "            median  = self.median[i]  or np.median(prices)\n",
    "            \n",
    "            stats.append(iqr_var)\n",
    "            stats.append(median)\n",
    "            \n",
    "            X_output.append(stats)\n",
    "        return np.array(X_output)\n",
    "class SparseToArray(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        return None\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def fit_transform(self, X, y=None):\n",
    "        return self.fit(X, y).transform(X)\n",
    "        \n",
    "    def transform(self, X):\n",
    "        return np.array(X.toarray()) #[ar.toarray() for ar in X]\n",
    "    \n",
    "class ReadFiles(BaseEstimator, TransformerMixin): \n",
    "    def __init__(self):\n",
    "        return None\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def fit_transform(self, X, y=None):\n",
    "        return self.fit(X, y).transform(X)\n",
    "        \n",
    "    def transform(self, X):\n",
    "        return (open(filename, 'r').read() for filename in tqdm(X))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_filenames(limit = -1):\n",
    "    filenames = []\n",
    "    directory_files = glob.glob('filing_texts/*')\n",
    "    excepted_files = [('filing_texts/' + sn) for sn in excepted_stock_names]\n",
    "    filenames_with_data = [x for x in directory_files[:limit] if x not in excepted_files]\n",
    "    for filename in filenames_with_data:\n",
    "        filenames.append(filename)\n",
    "    return filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spm = stock_name_date_mapping(limit)\n",
    "\n",
    "filenames = get_filenames(limit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used to build other pipelines\n",
    "\n",
    "prices_pipeline = Pipeline([\n",
    "    ('spm_to_filenames', SpmToFileNamesTransformer()),\n",
    "    ('filenames_to_stock_names', FilenamesToStockNamesTransformer()),\n",
    "    ('stock_names_to_dates', MapStockNamesToDatesTransformer(-3, 2)),\n",
    "    ('dates_to_prices_transformer', StockNameDatesMapToPricesListTransformer()),\n",
    "    ('imputer', SimpleImputer())\n",
    "])\n",
    "\n",
    "# Used as the final y values \n",
    "labels_pipeline = Pipeline([\n",
    "    ('prices_pipeline', prices_pipeline),\n",
    "    ('labels_transform', LabelsTransform())\n",
    "])\n",
    "\n",
    "# Used for training and test set features\n",
    "stock_stats = Pipeline([\n",
    "    ('prices_pipeline', prices_pipeline),\n",
    "    ('stats_transform', StatisticalMeasuresTransformer())\n",
    "])\n",
    "\n",
    "# Used for training and test set features \n",
    "# ('stock_names_to_file_names', StockNamesToFileNamesTransformer()),\n",
    "text_word_counts = Pipeline([\n",
    "    ('spm_to_file_names', SpmToFileNamesTransformer()),\n",
    "    ('read_files', ReadFiles()),\n",
    "    ('vect', TfidfVectorizer(\n",
    "                token_pattern=r\"[a-zA-Z]+\", \n",
    "                min_df = 0.10,\n",
    "                max_df = 0.80,\n",
    "                stop_words = 'english',\n",
    "                max_features=9000,\n",
    "                ngram_range=(1, 1))),\n",
    "    ('sparse_to_array', SparseToArray()),\n",
    "    ('std_scaler', StandardScaler()),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/192 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  1%|          | 2/192 [00:00<00:19,  9.60it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  6%|▌         | 11/192 [00:00<00:05, 32.84it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  7%|▋         | 14/192 [00:01<00:23,  7.57it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 10%|█         | 20/192 [00:02<00:17,  9.69it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 13%|█▎        | 25/192 [00:02<00:15, 10.75it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 17%|█▋        | 32/192 [00:02<00:12, 13.16it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 19%|█▉        | 36/192 [00:02<00:12, 12.10it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 21%|██        | 40/192 [00:03<00:11, 12.68it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 22%|██▏       | 43/192 [00:03<00:11, 12.71it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 27%|██▋       | 51/192 [00:03<00:09, 14.61it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 29%|██▉       | 56/192 [00:03<00:08, 15.58it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 32%|███▏      | 61/192 [00:03<00:08, 16.27it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 34%|███▍      | 65/192 [00:03<00:07, 16.57it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 36%|███▌      | 69/192 [00:06<00:11, 10.86it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 40%|████      | 77/192 [00:06<00:10, 11.27it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 43%|████▎     | 83/192 [00:07<00:09, 11.85it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 46%|████▌     | 88/192 [00:07<00:08, 11.90it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 49%|████▉     | 95/192 [00:07<00:07, 12.66it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 53%|█████▎    | 101/192 [00:07<00:06, 13.26it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 55%|█████▌    | 106/192 [00:07<00:06, 13.58it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 57%|█████▋    | 110/192 [00:08<00:06, 12.73it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 59%|█████▉    | 114/192 [00:09<00:06, 12.30it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 61%|██████    | 117/192 [00:09<00:06, 12.43it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 65%|██████▌   | 125/192 [00:09<00:05, 13.07it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 67%|██████▋   | 129/192 [00:12<00:06, 10.25it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 69%|██████▉   | 132/192 [00:13<00:06,  9.80it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 72%|███████▏  | 138/192 [00:13<00:05, 10.17it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 76%|███████▌  | 145/192 [00:13<00:04, 10.60it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 79%|███████▊  | 151/192 [00:14<00:03, 10.56it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 81%|████████  | 155/192 [00:21<00:05,  7.17it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 83%|████████▎ | 159/192 [00:21<00:04,  7.29it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 84%|████████▍ | 162/192 [00:25<00:04,  6.26it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 90%|█████████ | 173/192 [00:25<00:02,  6.66it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 93%|█████████▎| 178/192 [00:26<00:02,  6.79it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 95%|█████████▍| 182/192 [00:26<00:01,  6.91it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 97%|█████████▋| 187/192 [00:26<00:00,  7.06it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████| 192/192 [00:26<00:00,  7.23it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "192\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'NoneType' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-443-995b9c2da886>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels_pipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
     ]
    }
   ],
   "source": [
    "X = text_word_counts.fit_transform(spm)\n",
    "y = labels_pipeline.fit_transform(spm)\n",
    "print(len(X))\n",
    "print(len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(192, 3700)\n",
      "(192,)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train_continuous, y_test_continuous = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bool_arr(arr):\n",
    "    y_bool = []\n",
    "    for num in arr:\n",
    "        if num >= 0.1:\n",
    "            y_bool.append(1)\n",
    "        else:\n",
    "            y_bool.append(0)\n",
    "    return np.array(y_bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectFpr\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "\n",
    "train_pipeline = Pipeline([\n",
    "    ('reduce_false_pos', SelectFpr(alpha=0.33)),\n",
    "    ('svc', GridSearchCV(estimator=SVC(gamma='auto'), param_grid=dict(C=np.logspace(-1,3)), n_jobs=1))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train = bool_arr(y_train_continuous)\n",
    "y_test = bool_arr(y_test_continuous)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Piper/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_search.py:734: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('reduce_false_pos', SelectFpr(alpha=0.33, score_func=<function f_classif at 0x1a2e6e78c8>)), ('svc', GridSearchCV(cv=None, error_score='raise-deprecating',\n",
       "       estimator=SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kerne...   pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0))])"
      ]
     },
     "execution_count": 533,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.58974358974358976"
      ]
     },
     "execution_count": 534,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_pipeline.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22.899769244214269, 43.727389956429228]\n"
     ]
    }
   ],
   "source": [
    "# train_pipeline.predict(X_test)\n",
    "print(economic_score(X_test, y_test_continuous, train_pipeline))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.941176470588\n",
      "0.589743589744\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Piper/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:194: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "clf = SVC(C=100)\n",
    "# clf = RandomForestRegressor(n_estimators=200, max_depth=3, verbose=1, n_jobs=2)\n",
    "clf.fit(lil_x, y_train)\n",
    "\n",
    "print(clf.score(select_fpr.transform(X_train), y_train))\n",
    "print(clf.score(select_fpr.transform(X_test), y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def economic_score(feat, labels, clf):\n",
    "    total = np.dot(m1.predict(feat).reshape(feat.shape[0],), labels)\n",
    "    buy_all = np.dot(np.ones(feat.shape[0]), labels)\n",
    "    av = total/len(feat)\n",
    "    buy_all_av = buy_all/len(feat)\n",
    "    return [av, buy_all_av]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[90.120230661697605, 185.02618425699143]\n",
      "[22.899769244214269, 43.727389956429228]\n",
      "[22.899769244214269, 43.727389956429228]\n"
     ]
    }
   ],
   "source": [
    "print(economic_score(X_train, y_train_continuous, m1))\n",
    "print(economic_score(X_test, y_test_continuous, m1))\n",
    "print(economic_score(X_test, y_test_continuous, clf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Factors to look into including: \n",
    "# \"BookValue\" = (Total Assets - Total Liabilities) / Number of shares outstanding\n",
    "# \"MarketCap\" = Market price per share * number of shares \n",
    "# \"DividendYield\" = Dividend / Market price per share \n",
    "# \"EarningsPerShare\" \n",
    "# \"PERatio2\" = Market price per share / earning per share \n",
    "# \"priceBook\" = Market price per share / ((Total Assets - Total Liabilities) / Number of shares outstanding)\n",
    "# \"PriceSales\" = MarketCap / Revenue \n",
    "# \"Ask\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bool_arr(arr):\n",
    "    y_bool = []\n",
    "    for num in arr:\n",
    "        if num >= 0.1:\n",
    "            y_bool.append(1)\n",
    "        else:\n",
    "            y_bool.append(0)\n",
    "    return np.array(y_bool)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.models.Sequential at 0x1a1eb12978>"
      ]
     },
     "execution_count": 335,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras import regularizers\n",
    "\n",
    "shape = X_train.shape[1]\n",
    "\n",
    "m1 = Sequential([\n",
    "    BatchNormalization(input_shape=(shape,)),\n",
    "    Dense(35, activation='relu', kernel_regularizer=regularizers.l1(0.01)),\n",
    "    Dropout(0.5),\n",
    "    BatchNormalization(),\n",
    "    Dense(1, activation='sigmoid')   \n",
    "])\n",
    "\n",
    "m1.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "m1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 153 samples, validate on 39 samples\n",
      "Epoch 1/2\n",
      "153/153 [==============================] - 1s 4ms/step - loss: 26.7506 - acc: 0.4641 - val_loss: 26.6236 - val_acc: 0.4103\n",
      "Epoch 2/2\n",
      "153/153 [==============================] - 0s 390us/step - loss: 26.4335 - acc: 0.5229 - val_loss: 26.3072 - val_acc: 0.4359\n",
      "Train on 153 samples, validate on 39 samples\n",
      "Epoch 1/20\n",
      "153/153 [==============================] - 0s 307us/step - loss: 26.0977 - acc: 0.4967 - val_loss: 26.0022 - val_acc: 0.4359\n",
      "Epoch 2/20\n",
      "153/153 [==============================] - 0s 402us/step - loss: 25.7170 - acc: 0.5556 - val_loss: 25.7036 - val_acc: 0.4359\n",
      "Epoch 3/20\n",
      "153/153 [==============================] - 0s 385us/step - loss: 25.4699 - acc: 0.6013 - val_loss: 25.4087 - val_acc: 0.4615\n",
      "Epoch 4/20\n",
      "153/153 [==============================] - 0s 474us/step - loss: 25.2408 - acc: 0.5359 - val_loss: 25.1165 - val_acc: 0.4615\n",
      "Epoch 5/20\n",
      "153/153 [==============================] - 0s 384us/step - loss: 24.9465 - acc: 0.5163 - val_loss: 24.8244 - val_acc: 0.4359\n",
      "Epoch 6/20\n",
      "153/153 [==============================] - 0s 484us/step - loss: 24.5593 - acc: 0.5752 - val_loss: 24.5375 - val_acc: 0.4615\n",
      "Epoch 7/20\n",
      "153/153 [==============================] - 0s 322us/step - loss: 24.3123 - acc: 0.5817 - val_loss: 24.2555 - val_acc: 0.4615\n",
      "Epoch 8/20\n",
      "153/153 [==============================] - 0s 287us/step - loss: 24.0748 - acc: 0.5425 - val_loss: 23.9796 - val_acc: 0.4615\n",
      "Epoch 9/20\n",
      "153/153 [==============================] - 0s 333us/step - loss: 23.7458 - acc: 0.6013 - val_loss: 23.7032 - val_acc: 0.4359\n",
      "Epoch 10/20\n",
      "153/153 [==============================] - 0s 354us/step - loss: 23.4103 - acc: 0.6667 - val_loss: 23.4328 - val_acc: 0.4615\n",
      "Epoch 11/20\n",
      "153/153 [==============================] - 0s 416us/step - loss: 23.2240 - acc: 0.6209 - val_loss: 23.1625 - val_acc: 0.4615\n",
      "Epoch 12/20\n",
      "153/153 [==============================] - 0s 297us/step - loss: 22.9141 - acc: 0.6275 - val_loss: 22.8984 - val_acc: 0.4615\n",
      "Epoch 13/20\n",
      "153/153 [==============================] - 0s 416us/step - loss: 22.6244 - acc: 0.6601 - val_loss: 22.6346 - val_acc: 0.4359\n",
      "Epoch 14/20\n",
      "153/153 [==============================] - 0s 374us/step - loss: 22.3289 - acc: 0.6863 - val_loss: 22.3737 - val_acc: 0.4615\n",
      "Epoch 15/20\n",
      "153/153 [==============================] - 0s 502us/step - loss: 22.0825 - acc: 0.6667 - val_loss: 22.1179 - val_acc: 0.4615\n",
      "Epoch 16/20\n",
      "153/153 [==============================] - 0s 590us/step - loss: 21.9187 - acc: 0.6340 - val_loss: 21.8596 - val_acc: 0.4615\n",
      "Epoch 17/20\n",
      "153/153 [==============================] - 0s 287us/step - loss: 21.5512 - acc: 0.7124 - val_loss: 21.6061 - val_acc: 0.5385\n",
      "Epoch 18/20\n",
      "153/153 [==============================] - 0s 373us/step - loss: 21.3207 - acc: 0.6732 - val_loss: 21.3556 - val_acc: 0.5385\n",
      "Epoch 19/20\n",
      "153/153 [==============================] - 0s 287us/step - loss: 21.0887 - acc: 0.6928 - val_loss: 21.1069 - val_acc: 0.5385\n",
      "Epoch 20/20\n",
      "153/153 [==============================] - 0s 281us/step - loss: 20.8017 - acc: 0.7124 - val_loss: 20.8614 - val_acc: 0.5128\n",
      "Train on 153 samples, validate on 39 samples\n",
      "Epoch 1/2\n",
      "153/153 [==============================] - 0s 370us/step - loss: 20.5550 - acc: 0.7059 - val_loss: 20.6202 - val_acc: 0.5128\n",
      "Epoch 2/2\n",
      "153/153 [==============================] - 0s 266us/step - loss: 20.3024 - acc: 0.7190 - val_loss: 20.3793 - val_acc: 0.4872\n",
      "Train on 153 samples, validate on 39 samples\n",
      "Epoch 1/2\n",
      "153/153 [==============================] - 0s 365us/step - loss: 20.0405 - acc: 0.7647 - val_loss: 20.1376 - val_acc: 0.5128\n",
      "Epoch 2/2\n",
      "153/153 [==============================] - 0s 400us/step - loss: 19.7901 - acc: 0.7124 - val_loss: 19.8945 - val_acc: 0.5128\n",
      "Train on 153 samples, validate on 39 samples\n",
      "Epoch 1/1\n",
      "153/153 [==============================] - 0s 591us/step - loss: 19.5809 - acc: 0.7124 - val_loss: 19.6482 - val_acc: 0.5128\n",
      "Train on 153 samples, validate on 39 samples\n",
      "Epoch 1/5\n",
      "153/153 [==============================] - 0s 388us/step - loss: 19.2951 - acc: 0.7124 - val_loss: 19.4044 - val_acc: 0.4615\n",
      "Epoch 2/5\n",
      "153/153 [==============================] - 0s 553us/step - loss: 19.0406 - acc: 0.7908 - val_loss: 19.1645 - val_acc: 0.4872\n",
      "Epoch 3/5\n",
      "153/153 [==============================] - 0s 723us/step - loss: 18.8129 - acc: 0.7451 - val_loss: 18.9242 - val_acc: 0.4615\n",
      "Epoch 4/5\n",
      "153/153 [==============================] - 0s 476us/step - loss: 18.5959 - acc: 0.7124 - val_loss: 18.6876 - val_acc: 0.4615\n",
      "Epoch 5/5\n",
      "153/153 [==============================] - 0s 598us/step - loss: 18.3869 - acc: 0.6928 - val_loss: 18.4572 - val_acc: 0.4872\n",
      "Train on 153 samples, validate on 39 samples\n",
      "Epoch 1/20\n",
      "153/153 [==============================] - 0s 417us/step - loss: 18.1375 - acc: 0.7320 - val_loss: 18.2272 - val_acc: 0.4872\n",
      "Epoch 2/20\n",
      "153/153 [==============================] - 0s 310us/step - loss: 17.9008 - acc: 0.7647 - val_loss: 18.0007 - val_acc: 0.4872\n",
      "Epoch 3/20\n",
      "153/153 [==============================] - 0s 376us/step - loss: 17.6094 - acc: 0.8170 - val_loss: 17.7753 - val_acc: 0.4872\n",
      "Epoch 4/20\n",
      "153/153 [==============================] - 0s 427us/step - loss: 17.4137 - acc: 0.7778 - val_loss: 17.5502 - val_acc: 0.4872\n",
      "Epoch 5/20\n",
      "153/153 [==============================] - 0s 504us/step - loss: 17.1575 - acc: 0.7778 - val_loss: 17.3234 - val_acc: 0.4872\n",
      "Epoch 6/20\n",
      "153/153 [==============================] - 0s 622us/step - loss: 17.0121 - acc: 0.7516 - val_loss: 17.1037 - val_acc: 0.4872\n",
      "Epoch 7/20\n",
      "153/153 [==============================] - 0s 510us/step - loss: 16.7924 - acc: 0.7320 - val_loss: 16.8836 - val_acc: 0.4615\n",
      "Epoch 8/20\n",
      "153/153 [==============================] - 0s 540us/step - loss: 16.5226 - acc: 0.7843 - val_loss: 16.6627 - val_acc: 0.4615\n",
      "Epoch 9/20\n",
      "153/153 [==============================] - 0s 308us/step - loss: 16.3054 - acc: 0.7778 - val_loss: 16.4422 - val_acc: 0.4359\n",
      "Epoch 10/20\n",
      "153/153 [==============================] - 0s 271us/step - loss: 16.0819 - acc: 0.8170 - val_loss: 16.2294 - val_acc: 0.4615\n",
      "Epoch 11/20\n",
      "153/153 [==============================] - 0s 319us/step - loss: 15.8569 - acc: 0.7647 - val_loss: 16.0195 - val_acc: 0.4615\n",
      "Epoch 12/20\n",
      "153/153 [==============================] - 0s 274us/step - loss: 15.6472 - acc: 0.8170 - val_loss: 15.8134 - val_acc: 0.4872\n",
      "Epoch 13/20\n",
      "153/153 [==============================] - 0s 274us/step - loss: 15.4325 - acc: 0.7908 - val_loss: 15.6088 - val_acc: 0.4872\n",
      "Epoch 14/20\n",
      "153/153 [==============================] - 0s 410us/step - loss: 15.1940 - acc: 0.8235 - val_loss: 15.4037 - val_acc: 0.4872\n",
      "Epoch 15/20\n",
      "153/153 [==============================] - 0s 374us/step - loss: 15.0227 - acc: 0.8039 - val_loss: 15.2001 - val_acc: 0.4872\n",
      "Epoch 16/20\n",
      "153/153 [==============================] - 0s 326us/step - loss: 14.7788 - acc: 0.8301 - val_loss: 14.9970 - val_acc: 0.5128\n",
      "Epoch 17/20\n",
      "153/153 [==============================] - 0s 384us/step - loss: 14.5432 - acc: 0.8627 - val_loss: 14.7948 - val_acc: 0.5128\n",
      "Epoch 18/20\n",
      "153/153 [==============================] - 0s 345us/step - loss: 14.3924 - acc: 0.8366 - val_loss: 14.5969 - val_acc: 0.4872\n",
      "Epoch 19/20\n",
      "153/153 [==============================] - 0s 375us/step - loss: 14.1720 - acc: 0.8170 - val_loss: 14.4042 - val_acc: 0.5385\n",
      "Epoch 20/20\n",
      "153/153 [==============================] - 0s 356us/step - loss: 13.9945 - acc: 0.8170 - val_loss: 14.2132 - val_acc: 0.5385\n",
      "Train on 153 samples, validate on 39 samples\n",
      "Epoch 1/40\n",
      "153/153 [==============================] - 0s 332us/step - loss: 13.7803 - acc: 0.8366 - val_loss: 14.0209 - val_acc: 0.5385\n",
      "Epoch 2/40\n",
      "153/153 [==============================] - 0s 328us/step - loss: 13.5709 - acc: 0.8693 - val_loss: 13.8297 - val_acc: 0.4872\n",
      "Epoch 3/40\n",
      "153/153 [==============================] - 0s 321us/step - loss: 13.4072 - acc: 0.8039 - val_loss: 13.6398 - val_acc: 0.4872\n",
      "Epoch 4/40\n",
      "153/153 [==============================] - 0s 318us/step - loss: 13.2557 - acc: 0.8170 - val_loss: 13.4504 - val_acc: 0.5128\n",
      "Epoch 5/40\n",
      "153/153 [==============================] - 0s 323us/step - loss: 13.0383 - acc: 0.8497 - val_loss: 13.2646 - val_acc: 0.4872\n",
      "Epoch 6/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "153/153 [==============================] - 0s 372us/step - loss: 12.7932 - acc: 0.8627 - val_loss: 13.0789 - val_acc: 0.4872\n",
      "Epoch 7/40\n",
      "153/153 [==============================] - 0s 325us/step - loss: 12.6430 - acc: 0.8301 - val_loss: 12.8934 - val_acc: 0.4872\n",
      "Epoch 8/40\n",
      "153/153 [==============================] - 0s 427us/step - loss: 12.5073 - acc: 0.8301 - val_loss: 12.7098 - val_acc: 0.4872\n",
      "Epoch 9/40\n",
      "153/153 [==============================] - 0s 514us/step - loss: 12.2785 - acc: 0.8693 - val_loss: 12.5347 - val_acc: 0.5385\n",
      "Epoch 10/40\n",
      "153/153 [==============================] - 0s 537us/step - loss: 12.0889 - acc: 0.8497 - val_loss: 12.3591 - val_acc: 0.5128\n",
      "Epoch 11/40\n",
      "153/153 [==============================] - 0s 576us/step - loss: 11.9075 - acc: 0.8824 - val_loss: 12.1828 - val_acc: 0.5385\n",
      "Epoch 12/40\n",
      "153/153 [==============================] - 0s 434us/step - loss: 11.7418 - acc: 0.8627 - val_loss: 12.0067 - val_acc: 0.4872\n",
      "Epoch 13/40\n",
      "153/153 [==============================] - 0s 327us/step - loss: 11.5620 - acc: 0.8497 - val_loss: 11.8349 - val_acc: 0.4872\n",
      "Epoch 14/40\n",
      "153/153 [==============================] - 0s 248us/step - loss: 11.4078 - acc: 0.8366 - val_loss: 11.6664 - val_acc: 0.4615\n",
      "Epoch 15/40\n",
      "153/153 [==============================] - 0s 243us/step - loss: 11.2411 - acc: 0.8366 - val_loss: 11.5012 - val_acc: 0.4615\n",
      "Epoch 16/40\n",
      "153/153 [==============================] - 0s 288us/step - loss: 11.0165 - acc: 0.8889 - val_loss: 11.3363 - val_acc: 0.5128\n",
      "Epoch 17/40\n",
      "153/153 [==============================] - 0s 364us/step - loss: 10.8487 - acc: 0.9020 - val_loss: 11.1761 - val_acc: 0.5128\n",
      "Epoch 18/40\n",
      "153/153 [==============================] - 0s 386us/step - loss: 10.7026 - acc: 0.8627 - val_loss: 11.0109 - val_acc: 0.4872\n",
      "Epoch 19/40\n",
      "153/153 [==============================] - 0s 422us/step - loss: 10.5252 - acc: 0.8824 - val_loss: 10.8451 - val_acc: 0.4872\n",
      "Epoch 20/40\n",
      "153/153 [==============================] - 0s 333us/step - loss: 10.3611 - acc: 0.8693 - val_loss: 10.6841 - val_acc: 0.4872\n",
      "Epoch 21/40\n",
      "153/153 [==============================] - 0s 353us/step - loss: 10.2215 - acc: 0.8627 - val_loss: 10.5249 - val_acc: 0.5128\n",
      "Epoch 22/40\n",
      "153/153 [==============================] - 0s 355us/step - loss: 10.0415 - acc: 0.8824 - val_loss: 10.3659 - val_acc: 0.5128\n",
      "Epoch 23/40\n",
      "153/153 [==============================] - 0s 338us/step - loss: 9.8986 - acc: 0.8627 - val_loss: 10.2095 - val_acc: 0.5385\n",
      "Epoch 24/40\n",
      "153/153 [==============================] - 0s 301us/step - loss: 9.7039 - acc: 0.9020 - val_loss: 10.0554 - val_acc: 0.5385\n",
      "Epoch 25/40\n",
      "153/153 [==============================] - 0s 313us/step - loss: 9.5960 - acc: 0.9020 - val_loss: 9.9057 - val_acc: 0.5128\n",
      "Epoch 26/40\n",
      "153/153 [==============================] - 0s 302us/step - loss: 9.4217 - acc: 0.8954 - val_loss: 9.7568 - val_acc: 0.5128\n",
      "Epoch 27/40\n",
      "153/153 [==============================] - 0s 292us/step - loss: 9.2262 - acc: 0.9216 - val_loss: 9.6094 - val_acc: 0.5128\n",
      "Epoch 28/40\n",
      "153/153 [==============================] - 0s 313us/step - loss: 9.1224 - acc: 0.9085 - val_loss: 9.4633 - val_acc: 0.5128\n",
      "Epoch 29/40\n",
      "153/153 [==============================] - 0s 515us/step - loss: 8.9666 - acc: 0.9150 - val_loss: 9.3215 - val_acc: 0.5385\n",
      "Epoch 30/40\n",
      "153/153 [==============================] - 0s 586us/step - loss: 8.8454 - acc: 0.9020 - val_loss: 9.1830 - val_acc: 0.5385\n",
      "Epoch 31/40\n",
      "153/153 [==============================] - 0s 335us/step - loss: 8.7163 - acc: 0.8824 - val_loss: 9.0473 - val_acc: 0.5128\n",
      "Epoch 32/40\n",
      "153/153 [==============================] - 0s 373us/step - loss: 8.5356 - acc: 0.9281 - val_loss: 8.9133 - val_acc: 0.5128\n",
      "Epoch 33/40\n",
      "153/153 [==============================] - 0s 560us/step - loss: 8.4336 - acc: 0.9020 - val_loss: 8.7788 - val_acc: 0.5385\n",
      "Epoch 34/40\n",
      "153/153 [==============================] - 0s 432us/step - loss: 8.2798 - acc: 0.9150 - val_loss: 8.6476 - val_acc: 0.5385\n",
      "Epoch 35/40\n",
      "153/153 [==============================] - 0s 438us/step - loss: 8.1586 - acc: 0.8889 - val_loss: 8.5203 - val_acc: 0.5385\n",
      "Epoch 36/40\n",
      "153/153 [==============================] - 0s 467us/step - loss: 8.0094 - acc: 0.9020 - val_loss: 8.3939 - val_acc: 0.4872\n",
      "Epoch 37/40\n",
      "153/153 [==============================] - 0s 625us/step - loss: 7.8722 - acc: 0.9020 - val_loss: 8.2628 - val_acc: 0.5128\n",
      "Epoch 38/40\n",
      "153/153 [==============================] - 0s 377us/step - loss: 7.7143 - acc: 0.9281 - val_loss: 8.1342 - val_acc: 0.5128\n",
      "Epoch 39/40\n",
      "153/153 [==============================] - 0s 405us/step - loss: 7.6117 - acc: 0.9085 - val_loss: 8.0090 - val_acc: 0.5128\n",
      "Epoch 40/40\n",
      "153/153 [==============================] - 0s 530us/step - loss: 7.4950 - acc: 0.9085 - val_loss: 7.8840 - val_acc: 0.5128\n",
      "Train on 153 samples, validate on 39 samples\n",
      "Epoch 1/80\n",
      "153/153 [==============================] - 0s 529us/step - loss: 7.3324 - acc: 0.9412 - val_loss: 7.7575 - val_acc: 0.5128\n",
      "Epoch 2/80\n",
      "153/153 [==============================] - 0s 548us/step - loss: 7.2253 - acc: 0.9281 - val_loss: 7.6375 - val_acc: 0.5385\n",
      "Epoch 3/80\n",
      "153/153 [==============================] - 0s 482us/step - loss: 7.1085 - acc: 0.9216 - val_loss: 7.5292 - val_acc: 0.4872\n",
      "Epoch 4/80\n",
      "153/153 [==============================] - 0s 444us/step - loss: 6.9750 - acc: 0.9346 - val_loss: 7.4191 - val_acc: 0.4872\n",
      "Epoch 5/80\n",
      "153/153 [==============================] - 0s 611us/step - loss: 6.8889 - acc: 0.9020 - val_loss: 7.3062 - val_acc: 0.4872\n",
      "Epoch 6/80\n",
      "153/153 [==============================] - 0s 529us/step - loss: 6.7491 - acc: 0.9216 - val_loss: 7.1960 - val_acc: 0.5128\n",
      "Epoch 7/80\n",
      "153/153 [==============================] - 0s 446us/step - loss: 6.6296 - acc: 0.9346 - val_loss: 7.0854 - val_acc: 0.5128\n",
      "Epoch 8/80\n",
      "153/153 [==============================] - 0s 366us/step - loss: 6.5228 - acc: 0.9346 - val_loss: 6.9771 - val_acc: 0.4872\n",
      "Epoch 9/80\n",
      "153/153 [==============================] - 0s 323us/step - loss: 6.4421 - acc: 0.9150 - val_loss: 6.8713 - val_acc: 0.4872\n",
      "Epoch 10/80\n",
      "153/153 [==============================] - 0s 364us/step - loss: 6.3106 - acc: 0.9346 - val_loss: 6.7646 - val_acc: 0.4872\n",
      "Epoch 11/80\n",
      "153/153 [==============================] - 0s 380us/step - loss: 6.2223 - acc: 0.9085 - val_loss: 6.6591 - val_acc: 0.4872\n",
      "Epoch 12/80\n",
      "153/153 [==============================] - 0s 373us/step - loss: 6.1052 - acc: 0.9216 - val_loss: 6.5566 - val_acc: 0.5128\n",
      "Epoch 13/80\n",
      "153/153 [==============================] - 0s 376us/step - loss: 6.0081 - acc: 0.9216 - val_loss: 6.4655 - val_acc: 0.5641\n",
      "Epoch 14/80\n",
      "153/153 [==============================] - 0s 340us/step - loss: 5.9293 - acc: 0.9346 - val_loss: 6.3737 - val_acc: 0.5641\n",
      "Epoch 15/80\n",
      "153/153 [==============================] - 0s 408us/step - loss: 5.8162 - acc: 0.9216 - val_loss: 6.2831 - val_acc: 0.5385\n",
      "Epoch 16/80\n",
      "153/153 [==============================] - 0s 428us/step - loss: 5.7438 - acc: 0.8954 - val_loss: 6.1915 - val_acc: 0.5128\n",
      "Epoch 17/80\n",
      "153/153 [==============================] - 0s 321us/step - loss: 5.6104 - acc: 0.9412 - val_loss: 6.0992 - val_acc: 0.5128\n",
      "Epoch 18/80\n",
      "153/153 [==============================] - 0s 361us/step - loss: 5.5394 - acc: 0.8954 - val_loss: 6.0139 - val_acc: 0.5128\n",
      "Epoch 19/80\n",
      "153/153 [==============================] - 0s 441us/step - loss: 5.4129 - acc: 0.9346 - val_loss: 5.9268 - val_acc: 0.5128\n",
      "Epoch 20/80\n",
      "153/153 [==============================] - 0s 490us/step - loss: 5.3414 - acc: 0.9281 - val_loss: 5.8413 - val_acc: 0.5128\n",
      "Epoch 21/80\n",
      "153/153 [==============================] - 0s 842us/step - loss: 5.2436 - acc: 0.9150 - val_loss: 5.7558 - val_acc: 0.5128\n",
      "Epoch 22/80\n",
      "153/153 [==============================] - 0s 610us/step - loss: 5.1750 - acc: 0.9281 - val_loss: 5.6670 - val_acc: 0.5385\n",
      "Epoch 23/80\n",
      "153/153 [==============================] - 0s 559us/step - loss: 5.0467 - acc: 0.9412 - val_loss: 5.5763 - val_acc: 0.5385\n",
      "Epoch 24/80\n",
      "153/153 [==============================] - 0s 338us/step - loss: 4.9733 - acc: 0.9673 - val_loss: 5.4906 - val_acc: 0.5385\n",
      "Epoch 25/80\n",
      "153/153 [==============================] - 0s 362us/step - loss: 4.8992 - acc: 0.9346 - val_loss: 5.4113 - val_acc: 0.5385\n",
      "Epoch 26/80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "153/153 [==============================] - 0s 451us/step - loss: 4.7967 - acc: 0.9542 - val_loss: 5.3325 - val_acc: 0.5385\n",
      "Epoch 27/80\n",
      "153/153 [==============================] - 0s 555us/step - loss: 4.7408 - acc: 0.9608 - val_loss: 5.2635 - val_acc: 0.5128\n",
      "Epoch 28/80\n",
      "153/153 [==============================] - 0s 566us/step - loss: 4.6578 - acc: 0.9281 - val_loss: 5.2027 - val_acc: 0.5128\n",
      "Epoch 29/80\n",
      "153/153 [==============================] - 0s 493us/step - loss: 4.6073 - acc: 0.9346 - val_loss: 5.1451 - val_acc: 0.5128\n",
      "Epoch 30/80\n",
      "153/153 [==============================] - 0s 346us/step - loss: 4.5278 - acc: 0.9281 - val_loss: 5.0838 - val_acc: 0.5128\n",
      "Epoch 31/80\n",
      "153/153 [==============================] - 0s 349us/step - loss: 4.4376 - acc: 0.9477 - val_loss: 5.0089 - val_acc: 0.5128\n",
      "Epoch 32/80\n",
      "153/153 [==============================] - 0s 712us/step - loss: 4.3862 - acc: 0.9412 - val_loss: 4.9335 - val_acc: 0.5385\n",
      "Epoch 33/80\n",
      "153/153 [==============================] - 0s 456us/step - loss: 4.2871 - acc: 0.9804 - val_loss: 4.8570 - val_acc: 0.5385\n",
      "Epoch 34/80\n",
      "153/153 [==============================] - 0s 652us/step - loss: 4.2666 - acc: 0.9150 - val_loss: 4.7858 - val_acc: 0.5385\n",
      "Epoch 35/80\n",
      "153/153 [==============================] - 0s 651us/step - loss: 4.1637 - acc: 0.9739 - val_loss: 4.7259 - val_acc: 0.5641\n",
      "Epoch 36/80\n",
      "153/153 [==============================] - 0s 380us/step - loss: 4.1163 - acc: 0.9412 - val_loss: 4.6724 - val_acc: 0.5897\n",
      "Epoch 37/80\n",
      "153/153 [==============================] - 0s 340us/step - loss: 4.0644 - acc: 0.9477 - val_loss: 4.6225 - val_acc: 0.5641\n",
      "Epoch 38/80\n",
      "153/153 [==============================] - 0s 377us/step - loss: 4.0331 - acc: 0.9216 - val_loss: 4.5695 - val_acc: 0.5641\n",
      "Epoch 39/80\n",
      "153/153 [==============================] - 0s 361us/step - loss: 3.9777 - acc: 0.9542 - val_loss: 4.5133 - val_acc: 0.5385\n",
      "Epoch 40/80\n",
      "153/153 [==============================] - 0s 343us/step - loss: 3.9053 - acc: 0.9412 - val_loss: 4.4588 - val_acc: 0.5385\n",
      "Epoch 41/80\n",
      "153/153 [==============================] - 0s 352us/step - loss: 3.8281 - acc: 0.9412 - val_loss: 4.4023 - val_acc: 0.5385\n",
      "Epoch 42/80\n",
      "153/153 [==============================] - 0s 478us/step - loss: 3.7685 - acc: 0.9477 - val_loss: 4.3456 - val_acc: 0.5385\n",
      "Epoch 43/80\n",
      "153/153 [==============================] - 0s 550us/step - loss: 3.6779 - acc: 0.9869 - val_loss: 4.3007 - val_acc: 0.5385\n",
      "Epoch 44/80\n",
      "153/153 [==============================] - 0s 386us/step - loss: 3.6795 - acc: 0.9608 - val_loss: 4.2525 - val_acc: 0.5385\n",
      "Epoch 45/80\n",
      "153/153 [==============================] - 0s 355us/step - loss: 3.6264 - acc: 0.9542 - val_loss: 4.2013 - val_acc: 0.5385\n",
      "Epoch 46/80\n",
      "153/153 [==============================] - 0s 342us/step - loss: 3.5744 - acc: 0.9542 - val_loss: 4.1540 - val_acc: 0.5385\n",
      "Epoch 47/80\n",
      "153/153 [==============================] - 0s 341us/step - loss: 3.5292 - acc: 0.9412 - val_loss: 4.1158 - val_acc: 0.5128\n",
      "Epoch 48/80\n",
      "153/153 [==============================] - 0s 371us/step - loss: 3.4753 - acc: 0.9542 - val_loss: 4.0851 - val_acc: 0.5385\n",
      "Epoch 49/80\n",
      "153/153 [==============================] - 0s 367us/step - loss: 3.4340 - acc: 0.9542 - val_loss: 4.0497 - val_acc: 0.4872\n",
      "Epoch 50/80\n",
      "153/153 [==============================] - 0s 370us/step - loss: 3.4002 - acc: 0.9542 - val_loss: 4.0030 - val_acc: 0.4872\n",
      "Epoch 51/80\n",
      "153/153 [==============================] - 0s 341us/step - loss: 3.3491 - acc: 0.9673 - val_loss: 3.9564 - val_acc: 0.5128\n",
      "Epoch 52/80\n",
      "153/153 [==============================] - 0s 324us/step - loss: 3.3322 - acc: 0.9477 - val_loss: 3.9093 - val_acc: 0.5128\n",
      "Epoch 53/80\n",
      "153/153 [==============================] - 0s 319us/step - loss: 3.2543 - acc: 0.9477 - val_loss: 3.8667 - val_acc: 0.4872\n",
      "Epoch 54/80\n",
      "153/153 [==============================] - 0s 437us/step - loss: 3.2336 - acc: 0.9346 - val_loss: 3.8291 - val_acc: 0.4872\n",
      "Epoch 55/80\n",
      "153/153 [==============================] - 0s 349us/step - loss: 3.1662 - acc: 0.9739 - val_loss: 3.7982 - val_acc: 0.4615\n",
      "Epoch 56/80\n",
      "153/153 [==============================] - 0s 335us/step - loss: 3.1293 - acc: 0.9608 - val_loss: 3.7539 - val_acc: 0.5128\n",
      "Epoch 57/80\n",
      "153/153 [==============================] - 0s 346us/step - loss: 3.0852 - acc: 0.9673 - val_loss: 3.7185 - val_acc: 0.5128\n",
      "Epoch 58/80\n",
      "153/153 [==============================] - 0s 421us/step - loss: 3.0736 - acc: 0.9346 - val_loss: 3.6750 - val_acc: 0.5385\n",
      "Epoch 59/80\n",
      "153/153 [==============================] - 0s 288us/step - loss: 3.0247 - acc: 0.9804 - val_loss: 3.6208 - val_acc: 0.5641\n",
      "Epoch 60/80\n",
      "153/153 [==============================] - 0s 353us/step - loss: 2.9827 - acc: 0.9739 - val_loss: 3.5754 - val_acc: 0.5641\n",
      "Epoch 61/80\n",
      "153/153 [==============================] - 0s 359us/step - loss: 2.9613 - acc: 0.9673 - val_loss: 3.5383 - val_acc: 0.5385\n",
      "Epoch 62/80\n",
      "153/153 [==============================] - 0s 388us/step - loss: 2.9288 - acc: 0.9412 - val_loss: 3.5051 - val_acc: 0.5641\n",
      "Epoch 63/80\n",
      "153/153 [==============================] - 0s 335us/step - loss: 2.8999 - acc: 0.9346 - val_loss: 3.4828 - val_acc: 0.5385\n",
      "Epoch 64/80\n",
      "153/153 [==============================] - 0s 399us/step - loss: 2.8571 - acc: 0.9346 - val_loss: 3.4794 - val_acc: 0.4872\n",
      "Epoch 65/80\n",
      "153/153 [==============================] - 0s 427us/step - loss: 2.8417 - acc: 0.9608 - val_loss: 3.4556 - val_acc: 0.4872\n",
      "Epoch 66/80\n",
      "153/153 [==============================] - 0s 355us/step - loss: 2.8100 - acc: 0.9739 - val_loss: 3.4140 - val_acc: 0.4872\n",
      "Epoch 67/80\n",
      "153/153 [==============================] - 0s 386us/step - loss: 2.7637 - acc: 0.9608 - val_loss: 3.3722 - val_acc: 0.4872\n",
      "Epoch 68/80\n",
      "153/153 [==============================] - 0s 369us/step - loss: 2.7313 - acc: 0.9673 - val_loss: 3.3273 - val_acc: 0.4615\n",
      "Epoch 69/80\n",
      "153/153 [==============================] - 0s 401us/step - loss: 2.6942 - acc: 0.9739 - val_loss: 3.2959 - val_acc: 0.5128\n",
      "Epoch 70/80\n",
      "153/153 [==============================] - 0s 304us/step - loss: 2.6762 - acc: 0.9804 - val_loss: 3.2861 - val_acc: 0.5128\n",
      "Epoch 71/80\n",
      "153/153 [==============================] - 0s 432us/step - loss: 2.6618 - acc: 0.9412 - val_loss: 3.2767 - val_acc: 0.4872\n",
      "Epoch 72/80\n",
      "153/153 [==============================] - 0s 408us/step - loss: 2.6637 - acc: 0.9412 - val_loss: 3.2618 - val_acc: 0.4872\n",
      "Epoch 73/80\n",
      "153/153 [==============================] - 0s 296us/step - loss: 2.6250 - acc: 0.9281 - val_loss: 3.2327 - val_acc: 0.4615\n",
      "Epoch 74/80\n",
      "153/153 [==============================] - 0s 336us/step - loss: 2.5616 - acc: 0.9673 - val_loss: 3.2081 - val_acc: 0.4359\n",
      "Epoch 75/80\n",
      "153/153 [==============================] - 0s 294us/step - loss: 2.5689 - acc: 0.9150 - val_loss: 3.1806 - val_acc: 0.4359\n",
      "Epoch 76/80\n",
      "153/153 [==============================] - 0s 316us/step - loss: 2.5020 - acc: 0.9739 - val_loss: 3.1577 - val_acc: 0.4872\n",
      "Epoch 77/80\n",
      "153/153 [==============================] - 0s 275us/step - loss: 2.4964 - acc: 0.9477 - val_loss: 3.1329 - val_acc: 0.4872\n",
      "Epoch 78/80\n",
      "153/153 [==============================] - 0s 345us/step - loss: 2.4447 - acc: 0.9673 - val_loss: 3.0877 - val_acc: 0.5128\n",
      "Epoch 79/80\n",
      "153/153 [==============================] - 0s 330us/step - loss: 2.4243 - acc: 0.9542 - val_loss: 3.0419 - val_acc: 0.5385\n",
      "Epoch 80/80\n",
      "153/153 [==============================] - 0s 307us/step - loss: 2.3754 - acc: 0.9673 - val_loss: 3.0217 - val_acc: 0.5128\n",
      "Train on 153 samples, validate on 39 samples\n",
      "Epoch 1/120\n",
      "153/153 [==============================] - 0s 323us/step - loss: 2.3863 - acc: 0.9673 - val_loss: 2.9980 - val_acc: 0.4872\n",
      "Epoch 2/120\n",
      "153/153 [==============================] - 0s 359us/step - loss: 2.3210 - acc: 0.9673 - val_loss: 2.9756 - val_acc: 0.5385\n",
      "Epoch 3/120\n",
      "153/153 [==============================] - 0s 355us/step - loss: 2.3211 - acc: 0.9542 - val_loss: 2.9619 - val_acc: 0.5385\n",
      "Epoch 4/120\n",
      "153/153 [==============================] - 0s 285us/step - loss: 2.2926 - acc: 0.9412 - val_loss: 2.9357 - val_acc: 0.5385\n",
      "Epoch 5/120\n",
      "153/153 [==============================] - 0s 371us/step - loss: 2.2938 - acc: 0.9281 - val_loss: 2.9012 - val_acc: 0.5385\n",
      "Epoch 6/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "153/153 [==============================] - 0s 368us/step - loss: 2.2315 - acc: 0.9608 - val_loss: 2.8661 - val_acc: 0.4872\n",
      "Epoch 7/120\n",
      "153/153 [==============================] - 0s 347us/step - loss: 2.2179 - acc: 0.9477 - val_loss: 2.8350 - val_acc: 0.5128\n",
      "Epoch 8/120\n",
      "153/153 [==============================] - 0s 413us/step - loss: 2.1951 - acc: 0.9608 - val_loss: 2.7868 - val_acc: 0.5385\n",
      "Epoch 9/120\n",
      "153/153 [==============================] - 0s 436us/step - loss: 2.1514 - acc: 0.9542 - val_loss: 2.7604 - val_acc: 0.5385\n",
      "Epoch 10/120\n",
      "153/153 [==============================] - 0s 384us/step - loss: 2.1703 - acc: 0.9477 - val_loss: 2.7639 - val_acc: 0.4615\n",
      "Epoch 11/120\n",
      "153/153 [==============================] - 0s 426us/step - loss: 2.1338 - acc: 0.9477 - val_loss: 2.7395 - val_acc: 0.4872\n",
      "Epoch 12/120\n",
      "153/153 [==============================] - 0s 457us/step - loss: 2.1034 - acc: 0.9608 - val_loss: 2.7105 - val_acc: 0.4615\n",
      "Epoch 13/120\n",
      "153/153 [==============================] - 0s 429us/step - loss: 2.0641 - acc: 0.9804 - val_loss: 2.6914 - val_acc: 0.5128\n",
      "Epoch 14/120\n",
      "153/153 [==============================] - 0s 373us/step - loss: 2.0878 - acc: 0.9281 - val_loss: 2.6778 - val_acc: 0.4872\n",
      "Epoch 15/120\n",
      "153/153 [==============================] - 0s 312us/step - loss: 2.0157 - acc: 0.9608 - val_loss: 2.7004 - val_acc: 0.5897\n",
      "Epoch 16/120\n",
      "153/153 [==============================] - 0s 327us/step - loss: 2.0425 - acc: 0.9412 - val_loss: 2.6836 - val_acc: 0.5897\n",
      "Epoch 17/120\n",
      "153/153 [==============================] - 0s 329us/step - loss: 2.0365 - acc: 0.9673 - val_loss: 2.6597 - val_acc: 0.5385\n",
      "Epoch 18/120\n",
      "153/153 [==============================] - 0s 355us/step - loss: 2.0245 - acc: 0.9412 - val_loss: 2.6572 - val_acc: 0.5641\n",
      "Epoch 19/120\n",
      "153/153 [==============================] - 0s 425us/step - loss: 2.0061 - acc: 0.9477 - val_loss: 2.6575 - val_acc: 0.5641\n",
      "Epoch 20/120\n",
      "153/153 [==============================] - 0s 340us/step - loss: 1.9832 - acc: 0.9673 - val_loss: 2.6339 - val_acc: 0.5128\n",
      "Epoch 21/120\n",
      "153/153 [==============================] - 0s 310us/step - loss: 1.9511 - acc: 0.9739 - val_loss: 2.6164 - val_acc: 0.5385\n",
      "Epoch 22/120\n",
      "153/153 [==============================] - 0s 431us/step - loss: 1.9795 - acc: 0.9477 - val_loss: 2.6026 - val_acc: 0.5128\n",
      "Epoch 23/120\n",
      "153/153 [==============================] - 0s 343us/step - loss: 1.9220 - acc: 0.9542 - val_loss: 2.5822 - val_acc: 0.4872\n",
      "Epoch 24/120\n",
      "153/153 [==============================] - 0s 353us/step - loss: 1.9130 - acc: 0.9281 - val_loss: 2.5614 - val_acc: 0.5128\n",
      "Epoch 25/120\n",
      "153/153 [==============================] - 0s 356us/step - loss: 1.8972 - acc: 0.9281 - val_loss: 2.5550 - val_acc: 0.4872\n",
      "Epoch 26/120\n",
      "153/153 [==============================] - 0s 346us/step - loss: 1.8634 - acc: 0.9608 - val_loss: 2.5349 - val_acc: 0.4872\n",
      "Epoch 27/120\n",
      "153/153 [==============================] - 0s 317us/step - loss: 1.8563 - acc: 0.9412 - val_loss: 2.5170 - val_acc: 0.4872\n",
      "Epoch 28/120\n",
      "153/153 [==============================] - 0s 347us/step - loss: 1.8612 - acc: 0.9412 - val_loss: 2.4761 - val_acc: 0.4872\n",
      "Epoch 29/120\n",
      "153/153 [==============================] - 0s 343us/step - loss: 1.8229 - acc: 0.9281 - val_loss: 2.4527 - val_acc: 0.4615\n",
      "Epoch 30/120\n",
      "153/153 [==============================] - 0s 350us/step - loss: 1.7552 - acc: 0.9739 - val_loss: 2.4296 - val_acc: 0.4872\n",
      "Epoch 31/120\n",
      "153/153 [==============================] - 0s 285us/step - loss: 1.7322 - acc: 0.9739 - val_loss: 2.4022 - val_acc: 0.5385\n",
      "Epoch 32/120\n",
      "153/153 [==============================] - 0s 296us/step - loss: 1.7190 - acc: 0.9673 - val_loss: 2.3753 - val_acc: 0.5128\n",
      "Epoch 33/120\n",
      "153/153 [==============================] - 0s 337us/step - loss: 1.6738 - acc: 0.9869 - val_loss: 2.3767 - val_acc: 0.4872\n",
      "Epoch 34/120\n",
      "153/153 [==============================] - 0s 326us/step - loss: 1.6786 - acc: 0.9542 - val_loss: 2.3568 - val_acc: 0.4872\n",
      "Epoch 35/120\n",
      "153/153 [==============================] - 0s 381us/step - loss: 1.6230 - acc: 0.9869 - val_loss: 2.3286 - val_acc: 0.4872\n",
      "Epoch 36/120\n",
      "153/153 [==============================] - 0s 307us/step - loss: 1.6051 - acc: 0.9804 - val_loss: 2.3010 - val_acc: 0.5385\n",
      "Epoch 37/120\n",
      "153/153 [==============================] - ETA: 0s - loss: 1.5632 - acc: 1.000 - 0s 339us/step - loss: 1.5783 - acc: 0.9804 - val_loss: 2.2800 - val_acc: 0.5385\n",
      "Epoch 38/120\n",
      "153/153 [==============================] - 0s 324us/step - loss: 1.5493 - acc: 0.9935 - val_loss: 2.2604 - val_acc: 0.5128\n",
      "Epoch 39/120\n",
      "153/153 [==============================] - 0s 342us/step - loss: 1.5286 - acc: 0.9739 - val_loss: 2.2533 - val_acc: 0.5128\n",
      "Epoch 40/120\n",
      "153/153 [==============================] - 0s 332us/step - loss: 1.5455 - acc: 0.9608 - val_loss: 2.2386 - val_acc: 0.4872\n",
      "Epoch 41/120\n",
      "153/153 [==============================] - 0s 365us/step - loss: 1.5248 - acc: 0.9608 - val_loss: 2.2345 - val_acc: 0.4615\n",
      "Epoch 42/120\n",
      "153/153 [==============================] - 0s 394us/step - loss: 1.5105 - acc: 0.9608 - val_loss: 2.2364 - val_acc: 0.5385\n",
      "Epoch 43/120\n",
      "153/153 [==============================] - 0s 361us/step - loss: 1.5215 - acc: 0.9673 - val_loss: 2.1849 - val_acc: 0.5128\n",
      "Epoch 44/120\n",
      "153/153 [==============================] - 0s 315us/step - loss: 1.4764 - acc: 0.9869 - val_loss: 2.1454 - val_acc: 0.5385\n",
      "Epoch 45/120\n",
      "153/153 [==============================] - 0s 427us/step - loss: 1.4691 - acc: 0.9739 - val_loss: 2.1280 - val_acc: 0.5385\n",
      "Epoch 46/120\n",
      "153/153 [==============================] - 0s 374us/step - loss: 1.4431 - acc: 0.9673 - val_loss: 2.1207 - val_acc: 0.5385\n",
      "Epoch 47/120\n",
      "153/153 [==============================] - 0s 327us/step - loss: 1.4394 - acc: 0.9869 - val_loss: 2.1133 - val_acc: 0.5385\n",
      "Epoch 48/120\n",
      "153/153 [==============================] - 0s 342us/step - loss: 1.4498 - acc: 0.9477 - val_loss: 2.1066 - val_acc: 0.4872\n",
      "Epoch 49/120\n",
      "153/153 [==============================] - 0s 307us/step - loss: 1.4651 - acc: 0.9477 - val_loss: 2.0916 - val_acc: 0.5128\n",
      "Epoch 50/120\n",
      "153/153 [==============================] - 0s 311us/step - loss: 1.4406 - acc: 0.9608 - val_loss: 2.0828 - val_acc: 0.5385\n",
      "Epoch 51/120\n",
      "153/153 [==============================] - 0s 304us/step - loss: 1.4441 - acc: 0.9608 - val_loss: 2.0750 - val_acc: 0.5128\n",
      "Epoch 52/120\n",
      "153/153 [==============================] - 0s 351us/step - loss: 1.4073 - acc: 0.9477 - val_loss: 2.0685 - val_acc: 0.4872\n",
      "Epoch 53/120\n",
      "153/153 [==============================] - 0s 343us/step - loss: 1.4137 - acc: 0.9412 - val_loss: 2.0582 - val_acc: 0.4872\n",
      "Epoch 54/120\n",
      "153/153 [==============================] - 0s 365us/step - loss: 1.3740 - acc: 0.9739 - val_loss: 2.0453 - val_acc: 0.5385\n",
      "Epoch 55/120\n",
      "153/153 [==============================] - 0s 373us/step - loss: 1.4041 - acc: 0.9281 - val_loss: 2.0288 - val_acc: 0.5385\n",
      "Epoch 56/120\n",
      "153/153 [==============================] - 0s 327us/step - loss: 1.3451 - acc: 0.9739 - val_loss: 2.0114 - val_acc: 0.5128\n",
      "Epoch 57/120\n",
      "153/153 [==============================] - 0s 641us/step - loss: 1.3315 - acc: 0.9608 - val_loss: 1.9918 - val_acc: 0.4872\n",
      "Epoch 58/120\n",
      "153/153 [==============================] - 0s 436us/step - loss: 1.3059 - acc: 0.9542 - val_loss: 1.9900 - val_acc: 0.5128\n",
      "Epoch 59/120\n",
      "153/153 [==============================] - 0s 636us/step - loss: 1.2983 - acc: 0.9804 - val_loss: 2.0036 - val_acc: 0.5641\n",
      "Epoch 60/120\n",
      "153/153 [==============================] - 0s 299us/step - loss: 1.2992 - acc: 0.9608 - val_loss: 2.0107 - val_acc: 0.5641\n",
      "Epoch 61/120\n",
      "153/153 [==============================] - 0s 337us/step - loss: 1.2604 - acc: 0.9935 - val_loss: 2.0160 - val_acc: 0.5641\n",
      "Epoch 62/120\n",
      "153/153 [==============================] - 0s 657us/step - loss: 1.2614 - acc: 0.9739 - val_loss: 1.9737 - val_acc: 0.5385\n",
      "Epoch 63/120\n",
      "153/153 [==============================] - 0s 685us/step - loss: 1.2374 - acc: 0.9608 - val_loss: 1.9351 - val_acc: 0.5385\n",
      "Epoch 64/120\n",
      "153/153 [==============================] - 0s 359us/step - loss: 1.1864 - acc: 0.9869 - val_loss: 1.9126 - val_acc: 0.5641\n",
      "Epoch 65/120\n",
      "153/153 [==============================] - 0s 317us/step - loss: 1.2232 - acc: 0.9542 - val_loss: 1.9042 - val_acc: 0.5128\n",
      "Epoch 66/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "153/153 [==============================] - 0s 310us/step - loss: 1.2227 - acc: 0.9477 - val_loss: 1.9350 - val_acc: 0.5641\n",
      "Epoch 67/120\n",
      "153/153 [==============================] - 0s 277us/step - loss: 1.2440 - acc: 0.9281 - val_loss: 1.9395 - val_acc: 0.5897\n",
      "Epoch 68/120\n",
      "153/153 [==============================] - 0s 319us/step - loss: 1.2629 - acc: 0.9216 - val_loss: 1.9601 - val_acc: 0.5897\n",
      "Epoch 69/120\n",
      "153/153 [==============================] - 0s 319us/step - loss: 1.2157 - acc: 0.9673 - val_loss: 1.9597 - val_acc: 0.5897\n",
      "Epoch 70/120\n",
      "153/153 [==============================] - 0s 381us/step - loss: 1.2407 - acc: 0.9608 - val_loss: 1.9808 - val_acc: 0.5128\n",
      "Epoch 71/120\n",
      "153/153 [==============================] - 0s 334us/step - loss: 1.2237 - acc: 0.9739 - val_loss: 1.9569 - val_acc: 0.5385\n",
      "Epoch 72/120\n",
      "153/153 [==============================] - 0s 353us/step - loss: 1.1995 - acc: 0.9673 - val_loss: 1.9513 - val_acc: 0.5385\n",
      "Epoch 73/120\n",
      "153/153 [==============================] - 0s 345us/step - loss: 1.2305 - acc: 0.9542 - val_loss: 1.9343 - val_acc: 0.5128\n",
      "Epoch 74/120\n",
      "153/153 [==============================] - 0s 306us/step - loss: 1.1938 - acc: 0.9542 - val_loss: 1.9086 - val_acc: 0.5128\n",
      "Epoch 75/120\n",
      "153/153 [==============================] - 0s 331us/step - loss: 1.1986 - acc: 0.9346 - val_loss: 1.8813 - val_acc: 0.4872\n",
      "Epoch 76/120\n",
      "153/153 [==============================] - 0s 348us/step - loss: 1.1653 - acc: 0.9673 - val_loss: 1.8842 - val_acc: 0.5385\n",
      "Epoch 77/120\n",
      "153/153 [==============================] - 0s 324us/step - loss: 1.1451 - acc: 0.9869 - val_loss: 1.8611 - val_acc: 0.5641\n",
      "Epoch 78/120\n",
      "153/153 [==============================] - 0s 412us/step - loss: 1.1240 - acc: 0.9804 - val_loss: 1.8137 - val_acc: 0.5128\n",
      "Epoch 79/120\n",
      "153/153 [==============================] - 0s 284us/step - loss: 1.1446 - acc: 0.9608 - val_loss: 1.8116 - val_acc: 0.5128\n",
      "Epoch 80/120\n",
      "153/153 [==============================] - 0s 301us/step - loss: 1.1062 - acc: 0.9542 - val_loss: 1.8038 - val_acc: 0.5128\n",
      "Epoch 81/120\n",
      "153/153 [==============================] - 0s 373us/step - loss: 1.1135 - acc: 0.9542 - val_loss: 1.7967 - val_acc: 0.5641\n",
      "Epoch 82/120\n",
      "153/153 [==============================] - 0s 319us/step - loss: 1.0713 - acc: 0.9739 - val_loss: 1.8078 - val_acc: 0.5897\n",
      "Epoch 83/120\n",
      "153/153 [==============================] - 0s 400us/step - loss: 1.1290 - acc: 0.9346 - val_loss: 1.8133 - val_acc: 0.5641\n",
      "Epoch 84/120\n",
      "153/153 [==============================] - 0s 332us/step - loss: 1.1039 - acc: 0.9346 - val_loss: 1.8393 - val_acc: 0.5128\n",
      "Epoch 85/120\n",
      "153/153 [==============================] - 0s 379us/step - loss: 1.0602 - acc: 0.9804 - val_loss: 1.8289 - val_acc: 0.5385\n",
      "Epoch 86/120\n",
      "153/153 [==============================] - 0s 342us/step - loss: 1.0601 - acc: 0.9608 - val_loss: 1.8105 - val_acc: 0.5897\n",
      "Epoch 87/120\n",
      "153/153 [==============================] - 0s 331us/step - loss: 1.0490 - acc: 0.9608 - val_loss: 1.8202 - val_acc: 0.5385\n",
      "Epoch 88/120\n",
      "153/153 [==============================] - 0s 340us/step - loss: 1.0378 - acc: 0.9477 - val_loss: 1.8120 - val_acc: 0.5897\n",
      "Epoch 89/120\n",
      "153/153 [==============================] - 0s 368us/step - loss: 1.0009 - acc: 0.9935 - val_loss: 1.7944 - val_acc: 0.6154\n",
      "Epoch 90/120\n",
      "153/153 [==============================] - 0s 359us/step - loss: 1.0132 - acc: 0.9804 - val_loss: 1.7946 - val_acc: 0.5897\n",
      "Epoch 91/120\n",
      "153/153 [==============================] - 0s 395us/step - loss: 0.9795 - acc: 0.9869 - val_loss: 1.8530 - val_acc: 0.5897\n",
      "Epoch 92/120\n",
      "153/153 [==============================] - 0s 400us/step - loss: 0.9927 - acc: 0.9412 - val_loss: 1.7896 - val_acc: 0.5897\n",
      "Epoch 93/120\n",
      "153/153 [==============================] - 0s 420us/step - loss: 0.9659 - acc: 0.9739 - val_loss: 1.7715 - val_acc: 0.5641\n",
      "Epoch 94/120\n",
      "153/153 [==============================] - 0s 312us/step - loss: 0.9600 - acc: 0.9739 - val_loss: 1.7576 - val_acc: 0.5641\n",
      "Epoch 95/120\n",
      "153/153 [==============================] - 0s 303us/step - loss: 0.9584 - acc: 0.9673 - val_loss: 1.7970 - val_acc: 0.5128\n",
      "Epoch 96/120\n",
      "153/153 [==============================] - 0s 371us/step - loss: 0.9572 - acc: 0.9673 - val_loss: 1.7904 - val_acc: 0.5128\n",
      "Epoch 97/120\n",
      "153/153 [==============================] - 0s 457us/step - loss: 0.9858 - acc: 0.9542 - val_loss: 1.7799 - val_acc: 0.5128\n",
      "Epoch 98/120\n",
      "153/153 [==============================] - 0s 386us/step - loss: 0.9472 - acc: 0.9869 - val_loss: 1.7536 - val_acc: 0.5641\n",
      "Epoch 99/120\n",
      "153/153 [==============================] - 0s 378us/step - loss: 0.9618 - acc: 0.9673 - val_loss: 1.7521 - val_acc: 0.4872\n",
      "Epoch 100/120\n",
      "153/153 [==============================] - 0s 361us/step - loss: 0.9371 - acc: 0.9673 - val_loss: 1.7413 - val_acc: 0.4359\n",
      "Epoch 101/120\n",
      "153/153 [==============================] - 0s 297us/step - loss: 0.9991 - acc: 0.9216 - val_loss: 1.7340 - val_acc: 0.4359\n",
      "Epoch 102/120\n",
      "153/153 [==============================] - 0s 381us/step - loss: 0.9485 - acc: 0.9739 - val_loss: 1.7273 - val_acc: 0.4615\n",
      "Epoch 103/120\n",
      "153/153 [==============================] - 0s 339us/step - loss: 0.9512 - acc: 0.9542 - val_loss: 1.6893 - val_acc: 0.4872\n",
      "Epoch 104/120\n",
      "153/153 [==============================] - 0s 334us/step - loss: 0.9422 - acc: 0.9608 - val_loss: 1.6641 - val_acc: 0.5897\n",
      "Epoch 105/120\n",
      "153/153 [==============================] - 0s 371us/step - loss: 0.9422 - acc: 0.9804 - val_loss: 1.6650 - val_acc: 0.5641\n",
      "Epoch 106/120\n",
      "153/153 [==============================] - 0s 358us/step - loss: 0.9508 - acc: 0.9608 - val_loss: 1.6710 - val_acc: 0.5385\n",
      "Epoch 107/120\n",
      "153/153 [==============================] - 0s 335us/step - loss: 0.9258 - acc: 0.9542 - val_loss: 1.6848 - val_acc: 0.5128\n",
      "Epoch 108/120\n",
      "153/153 [==============================] - 0s 352us/step - loss: 0.9055 - acc: 0.9608 - val_loss: 1.7005 - val_acc: 0.5641\n",
      "Epoch 109/120\n",
      "153/153 [==============================] - 0s 338us/step - loss: 0.9574 - acc: 0.9085 - val_loss: 1.6406 - val_acc: 0.5128\n",
      "Epoch 110/120\n",
      "153/153 [==============================] - 0s 311us/step - loss: 0.8864 - acc: 0.9804 - val_loss: 1.6409 - val_acc: 0.5385\n",
      "Epoch 111/120\n",
      "153/153 [==============================] - 0s 352us/step - loss: 0.8841 - acc: 0.9673 - val_loss: 1.6857 - val_acc: 0.5128\n",
      "Epoch 112/120\n",
      "153/153 [==============================] - 0s 351us/step - loss: 0.8758 - acc: 0.9673 - val_loss: 1.7082 - val_acc: 0.5385\n",
      "Epoch 113/120\n",
      "153/153 [==============================] - 0s 471us/step - loss: 0.8996 - acc: 0.9542 - val_loss: 1.7032 - val_acc: 0.4615\n",
      "Epoch 114/120\n",
      "153/153 [==============================] - 0s 326us/step - loss: 0.9163 - acc: 0.9477 - val_loss: 1.7115 - val_acc: 0.5385\n",
      "Epoch 115/120\n",
      "153/153 [==============================] - 0s 331us/step - loss: 0.8921 - acc: 0.9412 - val_loss: 1.6663 - val_acc: 0.5128\n",
      "Epoch 116/120\n",
      "153/153 [==============================] - 0s 307us/step - loss: 0.8655 - acc: 0.9673 - val_loss: 1.6783 - val_acc: 0.5385\n",
      "Epoch 117/120\n",
      "153/153 [==============================] - 0s 355us/step - loss: 0.8946 - acc: 0.9608 - val_loss: 1.6550 - val_acc: 0.5128\n",
      "Epoch 118/120\n",
      "153/153 [==============================] - 0s 304us/step - loss: 0.8988 - acc: 0.9542 - val_loss: 1.6508 - val_acc: 0.4872\n",
      "Epoch 119/120\n",
      "153/153 [==============================] - 0s 322us/step - loss: 0.8956 - acc: 0.9542 - val_loss: 1.6438 - val_acc: 0.5128\n",
      "Epoch 120/120\n",
      "153/153 [==============================] - 0s 362us/step - loss: 0.8712 - acc: 0.9477 - val_loss: 1.6331 - val_acc: 0.5385\n"
     ]
    }
   ],
   "source": [
    "# sched = [[0.0001, 1000]]\n",
    "sched = [[0.0001, 2], [0.001, 20], [0.01, 2], [0.1, 2], [0.5, 1], [0.1, 5], [0.01, 20], [0.001, 40], [0.0001, 80], [0.00005, 120]]\n",
    "\n",
    "for i in range(1):\n",
    "    for info in sched:\n",
    "        lr, epochs = info\n",
    "        m1.optimizer.lr = lr\n",
    "        m1.fit(np.array(X_train), np.array(y_train_bool), epochs=epochs,  batch_size=64, validation_data=(np.array(X_test), np.array(y_test_bool)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22.899769244214269"
      ]
     },
     "execution_count": 337,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "economic_score(X_test, y_test, m1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.341781988102027,\n",
       " 6.622165640631012,\n",
       " -0.436573573614911,\n",
       " 0.4068140965980122,\n",
       " -0.3204552325076958,\n",
       " 5.615243605155948,\n",
       " 0.2844426590633416,\n",
       " -0.7908905068043499,\n",
       " -0.4251771038750514,\n",
       " 0.736151415826052,\n",
       " 26.784023141654984,\n",
       " 219.9040054863566,\n",
       " -0.2094495480575452,\n",
       " 308.3766944652674,\n",
       " -0.9440548025535945,\n",
       " -0.4406530114550679,\n",
       " 0.05152572017007015,\n",
       " -0.8919876257041239,\n",
       " -0.10538475171729224,\n",
       " 0.29631114248398777,\n",
       " -0.685671118315194,\n",
       " 573.2031449275363,\n",
       " -0.30100250171132575,\n",
       " -0.9266159679220135,\n",
       " 391.393948697633,\n",
       " -0.9648233763443692,\n",
       " -0.5546952403764974,\n",
       " -0.7321554236118895,\n",
       " -0.6012213774010581,\n",
       " -0.6388810878406338,\n",
       " -0.13477359529591648,\n",
       " 1.6420390104027747,\n",
       " 181.17104851761937,\n",
       " 0.2695291234482818,\n",
       " -0.4423768500638972,\n",
       " -0.8860388391498305,\n",
       " -0.46980137033134683,\n",
       " 0.3490179078404973,\n",
       " 0.506567635807774]"
      ]
     },
     "execution_count": 344,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[float(y) for y in y_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# vocab = list(text_word_counts.steps[2][1].vocabulary_.keys())\n",
    "# import operator\n",
    "# iv_dict = [[vocab[i],-float(f)] for i,f in enumerate(clf.coef_)]\n",
    "# most_important_terms = sorted(iv_dict, key=operator.itemgetter(1))[0:100]\n",
    "# print(most_important_terms)\n",
    "# most_important_vocab = dict(most_important_terms).keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from joblib import Memory\n",
    "# %mkdir cachedir\n",
    "# location = './cachedir'\n",
    "# memory = Memory(location, verbose=0)\n",
    "# stock_name_date_mapping = memory.cache(stock_name_date_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # print(text_word_counts.steps[1][1].vocabulary_)\n",
    "# def get_excepted_stock_names(limit=-1):\n",
    "#     esn = {}\n",
    "#     for i, file in tqdm(enumerate(glob.glob('filing_texts/*')[0:limit])):\n",
    "#         stock_name = stock_name_from_filename(file)\n",
    "#         file_data = open(file, 'r').read()\n",
    "#         try:\n",
    "#             with open(file) as open_file:\n",
    "#                 file_data = [next(open_file) for x in range(3)]\n",
    "#             file_data = ''.join(file_data)\n",
    "#             time_data = file_data.split(\"\\n\")[2][5:14]\n",
    "#         except:\n",
    "#             esn[i] = stock_name\n",
    "#             continue\n",
    "#     return esn\n",
    "\n",
    "# def get_x_mask(X, y, filenames):\n",
    "#     no_date_indices = []\n",
    "#     if X.shape[0] != len(y):\n",
    "#         stock_names = FilenamesToStockNamesTransformer().transform(filenames)\n",
    "#         for i, stock_name in enumerate(stock_names): \n",
    "#             try:\n",
    "#                 date = spm[stock_name]['date']\n",
    "#             except:\n",
    "#                 no_date_indices.append(i)\n",
    "#         mask = np.ones(X.shape[0], dtype=bool)\n",
    "#         mask[no_date_indices] = False\n",
    "#         X_mask = X[mask]\n",
    "#     else:\n",
    "#         X_mask = X\n",
    "#     return X_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
