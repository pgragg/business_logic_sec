{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf \n",
    "import keras\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import glob\n",
    "import os \n",
    "import pandas as pd\n",
    "import datetime\n",
    "from scipy.stats import iqr\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import metrics\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import Imputer as SimpleImputer\n",
    "# from sklearn.impute import SimpleImputer\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2\n",
    "\n",
    "import math\n",
    "\n",
    "spm = None\n",
    "english_words = None\n",
    "excepted_stock_names = []\n",
    "limit = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.layers import Dense, Dropout, RepeatVector, BatchNormalization, Convolution1D, Flatten, Lambda, Permute, MaxPooling1D, AlphaDropout\n",
    "from keras.models import Sequential\n",
    "from keras.utils import to_categorical\n",
    "import keras.backend as K\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import load_model\n",
    "from tqdm import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3413, 50429)\n",
      "(3413,)\n"
     ]
    }
   ],
   "source": [
    "import pickle as pickle\n",
    "x_name = 'X_3413_p015_min_df_p5_max_df_1_7_ngram.p'\n",
    "y_name = 'y_3413.p'\n",
    "# pickle.dump(X, open( x_name, \"wb\" ))\n",
    "# pickle.dump(y, open( y_name, \"wb\" ))\n",
    "X = pickle.load(open(x_name, 'rb'))\n",
    "y = pickle.load(open(y_name, 'rb'))\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DateRange():\n",
    "    def __init__(self, start_int, end_int):\n",
    "        self.start_int = start_int\n",
    "        self.end_int = end_int\n",
    "        self.range = (range(start_int, end_int))\n",
    "\n",
    "    def transform(self, date):\n",
    "        output = {}\n",
    "        dates = []\n",
    "        for delta in self.range:\n",
    "            date_delta = datetime.timedelta(days=delta)\n",
    "            date_string = datetime.datetime.strptime(date, '%Y-%m-%d').date()\n",
    "            dates.append(str(date_string + date_delta))\n",
    "        return dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PriceData:\n",
    "    def __init__(self, stock_name):\n",
    "        self.stock_name = stock_name\n",
    "        try:\n",
    "            self.price_data = pd.read_csv('prices/' + stock_name + '.csv')\n",
    "        except:\n",
    "            self.price_data = pd.DataFrame.from_dict({})\n",
    "\n",
    "    def on_date(self, date, market_time = 'open'): \n",
    "        try:\n",
    "            return float(self.price_data.loc[self.price_data['date'] == date][market_time])\n",
    "        except: \n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class X_y_mapping():\n",
    "    def __init__(self, start_int, end_int):\n",
    "        self.filing_int = start_int * -1\n",
    "        self.start_int = start_int\n",
    "        self.end_int = end_int\n",
    "        self.range = (range(start_int, end_int))\n",
    "        \n",
    "    def __stock_name_from_filename(self, filename):\n",
    "        return filename.split('/')[-1]\n",
    "        \n",
    "    def transform(self):\n",
    "        stock_names = [self.__stock_name_from_filename(filename) for filename in glob.glob('proc_filing_texts/*')]\n",
    "        price_ranges = self.__get_price_ranges(stock_names)\n",
    "        X_y = {}\n",
    "        for stock_name in price_ranges.keys():\n",
    "            cp_ratio = self.__comparison_price_ratio(price_ranges, stock_name)\n",
    "            if cp_ratio and not math.isnan(cp_ratio):\n",
    "                X_y[stock_name] = cp_ratio\n",
    "        return X_y\n",
    "    \n",
    "    \n",
    "    def __get_price_ranges(self, stock_names):\n",
    "        h = {}\n",
    "        for i, stock_name in enumerate(stock_names):\n",
    "            try:\n",
    "                date = stock_name.split('_')[1]\n",
    "            except:\n",
    "                continue # Ignore stocks which don't have a date\n",
    "            dates = []\n",
    "            for delta in self.range:\n",
    "                date_delta = datetime.timedelta(days=delta)\n",
    "                date_string = datetime.datetime.strptime(date, '%Y-%m-%d').date()\n",
    "                dates.append(str(date_string + date_delta))\n",
    "            h[stock_name] = dates\n",
    "        return h\n",
    "    \n",
    "    def __comparison_price_ratio(self,h, stock_name_with_date):\n",
    "        earliest_price_after_filing = None\n",
    "        hist_p = []\n",
    "        stock_name = stock_name_with_date.split('_')[0]\n",
    "        for i, date in enumerate(h[stock_name_with_date]):\n",
    "            price = PriceData(stock_name).on_date(date, 'open')\n",
    "            if price and not math.isnan(price):\n",
    "                if i > self.filing_int:\n",
    "                    earliest_price_after_filing = earliest_price_after_filing or price\n",
    "                else:\n",
    "                    hist_p.append(price)\n",
    "        # Closing price on day of filing\n",
    "        price_close_filing = PriceData(stock_name).on_date(h[stock_name_with_date][self.filing_int], 'close')\n",
    "        # Use either the next open day of trading or the close price on day of filing\n",
    "        comparison_price = earliest_price_after_filing or price_close_filing\n",
    "        # Remove nans from historical prices before taking the mean\n",
    "        this_mean = np.mean(hist_p)\n",
    "        if comparison_price and this_mean:\n",
    "            return ((comparison_price-this_mean)/this_mean)\n",
    "        else:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piper/anaconda2/envs/sls_sec/lib/python3.6/site-packages/numpy/core/fromnumeric.py:2957: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/home/piper/anaconda2/envs/sls_sec/lib/python3.6/site-packages/numpy/core/_methods.py:80: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3413"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spm = X_y_mapping(-3,4).transform()\n",
    "len(spm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import operator\n",
    "# sorted(spm.items(), key=operator.itemgetter(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FilenamesToStockNamesTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        return None\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def fit_transform(self, X, y=None):\n",
    "        return self.fit(X, y).transform(X)\n",
    "        \n",
    "    def transform(self, X):\n",
    "        output = []\n",
    "        for filename in X:\n",
    "            stock_name = self.__stock_name_from_filename(filename)\n",
    "            output.append(stock_name)\n",
    "        return output\n",
    "    def __stock_name_from_filename(self, filename):\n",
    "        return filename.split('/')[-1].split('_')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Used in y pipeline\n",
    "class SpmToFileNamesTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        return None\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def fit_transform(self, X, y=None):\n",
    "        return self.fit(X, y).transform(X)\n",
    "    def transform(self, X):\n",
    "        output = []\n",
    "        for key in X.keys():\n",
    "            output.append(f'proc_filing_texts/{key}')\n",
    "        return output\n",
    "    \n",
    "class SpmToStockNamesTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        return None\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def fit_transform(self, X, y=None):\n",
    "        return self.fit(X, y).transform(X)\n",
    "        \n",
    "    def transform(self, X):\n",
    "        return X.keys()\n",
    "    \n",
    "class StockNamesToFileNamesTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        return None\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def fit_transform(self, X, y=None):\n",
    "        return self.fit(X, y).transform(X)\n",
    "        \n",
    "    def transform(self, X):\n",
    "        return [f'filing_texts/{stock_name}' for stock_name in X]\n",
    "    \n",
    "class StatisticalMeasuresTransformer(BaseEstimator, TransformerMixin):\n",
    "    # Returns the interquartile-range and median.\n",
    "    def __init__(self):\n",
    "        return None\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        # ldcom = last_day_change_over_median\n",
    "        \n",
    "        output = []\n",
    "        self.iqr_var = []\n",
    "        self.median = []\n",
    "        for prices in X:\n",
    "            this_iqr = iqr(prices[0:-3])\n",
    "            this_median = np.median(prices[0:-3])\n",
    "            self.iqr_var.append(this_iqr)\n",
    "            self.median.append(this_median)\n",
    "        return self\n",
    "    \n",
    "    def fit_transform(self, X, y=None):\n",
    "        return self.fit(X, y).transform(X)\n",
    "        \n",
    "    def transform(self, X):\n",
    "        X_output = []\n",
    "        for i, prices in enumerate(X):\n",
    "            stats   = []\n",
    "            iqr_var = self.iqr_var[i] or iqr(prices)\n",
    "            median  = self.median[i]  or np.median(prices)\n",
    "            \n",
    "            stats.append(iqr_var)\n",
    "            stats.append(median)\n",
    "            \n",
    "            X_output.append(stats)\n",
    "        return np.array(X_output)\n",
    "class SparseToArray(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        return None\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def fit_transform(self, X, y=None):\n",
    "        return self.fit(X, y).transform(X)\n",
    "        \n",
    "    def transform(self, X):\n",
    "        return np.array(X.toarray()) #[ar.toarray() for ar in X]\n",
    "    \n",
    "class ReadFiles(BaseEstimator, TransformerMixin): \n",
    "    def __init__(self):\n",
    "        return None\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def fit_transform(self, X, y=None):\n",
    "        return self.fit(X, y).transform(X)\n",
    "        \n",
    "    def transform(self, X):\n",
    "        return (open(filename, 'r').read() for filename in tqdm(X))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpmToContinuousLabelsTransformer(BaseEstimator, TransformerMixin): \n",
    "    def __init__(self):\n",
    "        return None\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def fit_transform(self, X, y=None):\n",
    "        return self.fit(X, y).transform(X)\n",
    "        \n",
    "    def transform(self, X):\n",
    "        return np.array(list(X.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used to build other pipelines\n",
    "from sklearn.preprocessing import Imputer\n",
    "\n",
    "labels_pipeline = Pipeline([\n",
    "    ('dates_to_prices_transformer', SpmToContinuousLabelsTransformer())\n",
    "])\n",
    "\n",
    "text_word_counts = Pipeline([\n",
    "    ('spm_to_file_names', SpmToFileNamesTransformer()),\n",
    "    ('read_files', ReadFiles()),\n",
    "    ('vect', TfidfVectorizer(\n",
    "                token_pattern=r\"[a-zA-Z]+\", \n",
    "                min_df = 0.015,\n",
    "                max_df = 0.5,\n",
    "                stop_words = 'english',\n",
    "                ngram_range=(1, 7))),\n",
    "    ('sparse_to_array', SparseToArray()),\n",
    "    ('std_scaler', StandardScaler(with_mean=False)),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_x_y():\n",
    "    X = text_word_counts.fit_transform(spm)\n",
    "    y = labels_pipeline.fit_transform(spm)\n",
    "    print(X.shape)\n",
    "    print(y.shape)\n",
    "    return [X, y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3413, 50429)\n",
      "(3413,)\n"
     ]
    }
   ],
   "source": [
    "import pickle as pickle\n",
    "x_name = 'X_3413_p015_min_df_p5_max_df_1_7_ngram.p'\n",
    "y_name = 'y_3413.p'\n",
    "# pickle.dump(X, open( x_name, \"wb\" ))\n",
    "# pickle.dump(y, open( y_name, \"wb\" ))\n",
    "X = pickle.load(open(x_name, 'rb'))\n",
    "y = pickle.load(open(y_name, 'rb'))\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weighted_return(prob_preds, threshold):\n",
    "    total = 0\n",
    "    amount_wagered = 0\n",
    "    bet_amount = 1\n",
    "    bets = 0\n",
    "    good_is = []\n",
    "    for i, confidence in enumerate(prob_preds):\n",
    "        if confidence >= threshold:\n",
    "            return_v = y_test_continuous[i]\n",
    "            good_is.append(i)\n",
    "#             print(f'{confidence} confidence, {return_v} return')\n",
    "            total += (bet_amount * confidence * return_v)\n",
    "            amount_wagered += bet_amount * confidence\n",
    "            bets += 1\n",
    "\n",
    "    if amount_wagered == 0:\n",
    "        print('No confident bets')\n",
    "    else:\n",
    "        av_return = total/float(amount_wagered)\n",
    "        print(f'{bets} bets, {total} return. {av_return} av return')\n",
    "        print(good_is)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unweighted_return(pred, threshold):\n",
    "    total = 0\n",
    "    amount_wagered = 0\n",
    "    bet_amount = 1\n",
    "    bets = 0\n",
    "    good_is = []\n",
    "    for i, confidence in enumerate(pred):\n",
    "        if confidence >= threshold:\n",
    "            return_v = y_test_continuous[i]\n",
    "            good_is.append(i)\n",
    "#             print(f'{confidence} confidence, {return_v} return')\n",
    "            total += (bet_amount * return_v)\n",
    "            amount_wagered += bet_amount\n",
    "            bets += 1\n",
    "\n",
    "    if amount_wagered == 0:\n",
    "        print('No confident bets')\n",
    "    else:\n",
    "        av_return = total/float(amount_wagered)\n",
    "        print(f'{bets} bets, {total} return. {av_return} av return')\n",
    "        print(good_is)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "ys = []\n",
    "for val in y:\n",
    "    if val <= 0.5:\n",
    "        ys.append(val)\n",
    "    else:\n",
    "        ys.append(0)\n",
    "y = ys\n",
    "\n",
    "X_train, X_test, y_train_continuous, y_test_continuous = train_test_split(X, y, test_size=0.2)\n",
    "def bool_arr(arr, limit=0.5):\n",
    "    y_bool = []\n",
    "    for num in arr:\n",
    "        if num >= limit:\n",
    "            y_bool.append(1)\n",
    "        else:\n",
    "            y_bool.append(0)\n",
    "    return np.array(y_bool)\n",
    "\n",
    "y_train = bool_arr(y_train_continuous, 0.00)\n",
    "y_test = bool_arr(y_test_continuous, 0.00)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fd4298bba58>]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "v = y_train_continuous\n",
    "v.sort()\n",
    "plt.plot(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.48462664714494874\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectFpr\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import RandomizedSearchCV, cross_val_score\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "lin_svc_train_pipeline = Pipeline([\n",
    "    ('reduce_false_pos', SelectFpr(alpha=0.5)),\n",
    "    ('svc', LinearSVC(\n",
    "        class_weight='balanced'))\n",
    "     ])\n",
    "\n",
    "# svc_train_pipeline = Pipeline([\n",
    "#     ('reduce_false_pos', SelectFpr(alpha=0.5)),\n",
    "#     ('svc', SVC( \n",
    "#             gamma='auto', \n",
    "#             kernel='sigmoid',\n",
    "#             probability=False,\n",
    "#             class_weight='balanced')\n",
    "#         )\n",
    "#     ])\n",
    "\n",
    "lin_svc_train_pipeline.fit(X_train, y_train)\n",
    "print(lin_svc_train_pipeline.score(X_test, y_test))\n",
    "svc_prob_preds = lin_svc_train_pipeline.decision_function(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Voting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PCA(copy=True, iterated_power='auto', n_components=None, random_state=None,\n",
       "  svd_solver='auto', tol=0.0, whiten=False)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA()\n",
    "pca.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.11378947e+03, 9.16755441e+02, 7.56607773e+02, ...,\n",
       "       9.89142878e-31, 8.27842393e-31, 5.97256964e-31])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca.explained_variance_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA()\n",
    "pca.fit(X_train)\n",
    "X_train_pca = pca.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibSVM][LibSVM]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('l_svc', SVC(C=1.0, cache_size=200, class_weight='balanced', coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='linear',\n",
       "  max_iter=-1, probability=True, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=1)), ('s_svc', SVC(C=1.0, cache_size=200, class_weig..._jobs=-1,\n",
       "            oob_score=False, random_state=None, verbose=1,\n",
       "            warm_start=False))],\n",
       "         flatten_transform=None, n_jobs=1, voting='soft', weights=None)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "s_svc = SVC(\n",
    "        gamma='auto', \n",
    "        kernel='poly',\n",
    "        probability=True,\n",
    "        class_weight='balanced',\n",
    "        verbose=1)\n",
    "\n",
    "l_svc = SVC(\n",
    "        gamma='auto', \n",
    "        kernel='linear',\n",
    "        probability=True,\n",
    "        class_weight='balanced',\n",
    "        verbose=1)\n",
    "\n",
    "rfc = RandomForestClassifier(\n",
    "    min_weight_fraction_leaf=0.02,\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")    \n",
    "\n",
    "vc = VotingClassifier(\n",
    "    [   ('l_svc', l_svc),\n",
    "        ('s_svc', s_svc),\n",
    "        ('rfc', rfc)\n",
    "    ],\n",
    "    voting='soft'\n",
    ")\n",
    "vc.fit(X_train_pca, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=10)]: Done   2 out of  10 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=10)]: Done  10 out of  10 | elapsed:    0.0s finished\n"
     ]
    }
   ],
   "source": [
    "X_test_pca = pca.transform(X_test)\n",
    "preds = vc.predict_proba(X_test_pca)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=10)]: Done   2 out of  10 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=10)]: Done  10 out of  10 | elapsed:    0.0s finished\n",
      "/home/piper/anaconda2/envs/sls_sec/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7860805860805861"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vc.score(X_train_pca, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=10)]: Done   2 out of  10 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=10)]: Done  10 out of  10 | elapsed:    0.0s finished\n",
      "/home/piper/anaconda2/envs/sls_sec/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.4699853587115666"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vc.score(X_test_pca, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=10)]: Done   2 out of  10 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=10)]: Done  10 out of  10 | elapsed:    0.0s finished\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 682 into shape (683,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-147-853b8bd84e2a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test_pca\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mnum\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mPlotter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test_continuous\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Desktop/sls_sec/all/utilities/plotter.py\u001b[0m in \u001b[0;36mplot_score\u001b[0;34m(self, y, num)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mthreshold\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinspace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m             \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m             \u001b[0mbuy_rec_ys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0mbuy_all_ys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/sls_sec/all/utilities/plotter.py\u001b[0m in \u001b[0;36mscore\u001b[0;34m(self, y, cutoff)\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0my_pred_bool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mUtility\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbool_arr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcutoff\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0mconfidence_weighted\u001b[0m    \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m         \u001b[0mbuy_rec\u001b[0m                \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred_bool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mbuy_all\u001b[0m                \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 682 into shape (683,)"
     ]
    }
   ],
   "source": [
    "from utilities.plotter import Plotter\n",
    "\n",
    "num = -1\n",
    "\n",
    "# Plotter().plot_score(vc, X_test_pca[:num], y_test[:num])\n",
    "\n",
    "preds = vc.predict_proba(X_test_pca[:num])[:, 1]\n",
    "Plotter(preds).plot_score(np.array(y_test_continuous))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = vc.predict_proba(X_train_pca[:num])[:, 1]\n",
    "Plotter(preds).plot_score(np.array(y_train_continuous))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.4933139542581251\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectFpr\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import RandomizedSearchCV, cross_val_score\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "param_distributions = dict(C=np.logspace(-1,1,num=10))\n",
    "\n",
    "svr_train_pipeline = Pipeline([\n",
    "    ('svc', RandomizedSearchCV(estimator=SVR(\n",
    "        gamma='auto', \n",
    "        kernel='rbf'), \n",
    "        scoring='neg_mean_absolute_error',\n",
    "        param_distributions=param_distributions))\n",
    "     ])\n",
    "svr_train_pipeline.fit(X_train, y_train_continuous)\n",
    "print(svr_train_pipeline.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 bets, 0.5321300286398862 return. 0.026606501431994313 av return\n",
      "[2, 3, 10, 19, 28, 30, 37, 42, 43, 49, 53, 58, 62, 67, 70, 75, 76, 89, 96, 98]\n",
      "20 bets, 0.5321300286398862 return. 0.026606501431994313 av return\n",
      "[2, 3, 10, 19, 28, 30, 37, 42, 43, 49, 53, 58, 62, 67, 70, 75, 76, 89, 96, 98]\n"
     ]
    }
   ],
   "source": [
    "svc_prob_preds = svc_train_pipeline.predict(X_test)\n",
    "get_weighted_return(svc_prob_preds, 0.03)\n",
    "get_unweighted_return(svc_prob_preds, 0.03)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 bets, 0.00398947972416052 return. 0.023264985931279722 av return\n",
      "[12, 19, 31, 49, 70, 71, 89, 90]\n",
      "8 bets, 0.18362775323697944 return. 0.02295346915462243 av return\n",
      "[12, 19, 31, 49, 70, 71, 89, 90]\n"
     ]
    }
   ],
   "source": [
    "svr_prob_preds = svr_train_pipeline.predict(X_test)\n",
    "get_weighted_return(svr_prob_preds, 0.021)\n",
    "get_unweighted_return(svr_prob_preds, 0.021)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = svr_train_pipeline.predict(X_test)\n",
    "a.sort()\n",
    "plt.plot(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = svc_train_pipeline.predict_proba(X_test)\n",
    "a.sort()\n",
    "plt.plot(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ya = y_train_continuous\n",
    "ya.sort()\n",
    "plt.plot(ya)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "% matplotlib inline\n",
    "pred = svc_train_pipeline.predict_proba(X_test)[:, 1]\n",
    "plt.hist(pred, bins=100, range=(0,1))\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.013314499907574611"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(y_test_continuous)/len(y_test_continuous)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are the returns given by this model over a range of confidence level cutoffs?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'svc_train_pipeline' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-102-2b5a237e0250>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mnum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mPlotter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msvc_train_pipeline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mnum\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mnum\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'svc_train_pipeline' is not defined"
     ]
    }
   ],
   "source": [
    "num = 10\n",
    "Plotter().plot_score(svc_train_pipeline, X_test[:num], y_test[:num])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import regularizers\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2730 samples, validate on 683 samples\n",
      "Epoch 1/2\n",
      "2730/2730 [==============================] - 2s 882us/step - loss: 0.3465 - acc: 0.5037 - val_loss: 0.6961 - val_acc: 0.4963\n",
      "Epoch 2/2\n",
      "2730/2730 [==============================] - 1s 269us/step - loss: 0.3474 - acc: 0.4872 - val_loss: 0.6967 - val_acc: 0.5066\n",
      "Train on 2730 samples, validate on 683 samples\n",
      "Epoch 1/20\n",
      "2730/2730 [==============================] - 1s 266us/step - loss: 0.3475 - acc: 0.4952 - val_loss: 0.6966 - val_acc: 0.5037\n",
      "Epoch 2/20\n",
      "2730/2730 [==============================] - 1s 267us/step - loss: 0.3427 - acc: 0.5264 - val_loss: 0.6965 - val_acc: 0.5066\n",
      "Epoch 3/20\n",
      "2730/2730 [==============================] - 1s 270us/step - loss: 0.3433 - acc: 0.5059 - val_loss: 0.6961 - val_acc: 0.5037\n",
      "Epoch 4/20\n",
      "2730/2730 [==============================] - 1s 269us/step - loss: 0.3429 - acc: 0.5223 - val_loss: 0.6961 - val_acc: 0.5051\n",
      "Epoch 5/20\n",
      "2730/2730 [==============================] - 1s 276us/step - loss: 0.3442 - acc: 0.5278 - val_loss: 0.6958 - val_acc: 0.5051\n",
      "Epoch 6/20\n",
      "2730/2730 [==============================] - 1s 268us/step - loss: 0.3420 - acc: 0.5304 - val_loss: 0.6957 - val_acc: 0.5095\n",
      "Epoch 7/20\n",
      "2730/2730 [==============================] - 1s 269us/step - loss: 0.3415 - acc: 0.5432 - val_loss: 0.6956 - val_acc: 0.5154\n",
      "Epoch 8/20\n",
      "2730/2730 [==============================] - 1s 269us/step - loss: 0.3391 - acc: 0.5388 - val_loss: 0.6956 - val_acc: 0.5139\n",
      "Epoch 9/20\n",
      "2730/2730 [==============================] - 1s 269us/step - loss: 0.3413 - acc: 0.5234 - val_loss: 0.6956 - val_acc: 0.5154\n",
      "Epoch 10/20\n",
      "2730/2730 [==============================] - 1s 271us/step - loss: 0.3385 - acc: 0.5487 - val_loss: 0.6955 - val_acc: 0.5198\n",
      "Epoch 11/20\n",
      "2730/2730 [==============================] - 1s 270us/step - loss: 0.3394 - acc: 0.5429 - val_loss: 0.6956 - val_acc: 0.5139\n",
      "Epoch 12/20\n",
      "2730/2730 [==============================] - 1s 271us/step - loss: 0.3395 - acc: 0.5304 - val_loss: 0.6956 - val_acc: 0.5095\n",
      "Epoch 13/20\n",
      "2730/2730 [==============================] - 1s 269us/step - loss: 0.3352 - acc: 0.5615 - val_loss: 0.6956 - val_acc: 0.5095\n",
      "Epoch 14/20\n",
      "2730/2730 [==============================] - 1s 269us/step - loss: 0.3373 - acc: 0.5385 - val_loss: 0.6955 - val_acc: 0.5095\n",
      "Epoch 15/20\n",
      "2730/2730 [==============================] - 1s 270us/step - loss: 0.3329 - acc: 0.5641 - val_loss: 0.6955 - val_acc: 0.5139\n",
      "Epoch 16/20\n",
      "2730/2730 [==============================] - 1s 268us/step - loss: 0.3360 - acc: 0.5531 - val_loss: 0.6955 - val_acc: 0.5139\n",
      "Epoch 17/20\n",
      "2730/2730 [==============================] - 1s 268us/step - loss: 0.3338 - acc: 0.5667 - val_loss: 0.6955 - val_acc: 0.5110\n",
      "Epoch 18/20\n",
      "2730/2730 [==============================] - 1s 270us/step - loss: 0.3296 - acc: 0.5835 - val_loss: 0.6955 - val_acc: 0.5124\n",
      "Epoch 19/20\n",
      "2730/2730 [==============================] - 1s 272us/step - loss: 0.3298 - acc: 0.5795 - val_loss: 0.6955 - val_acc: 0.5139\n",
      "Epoch 20/20\n",
      "2730/2730 [==============================] - 1s 272us/step - loss: 0.3302 - acc: 0.5714 - val_loss: 0.6955 - val_acc: 0.5139\n",
      "Train on 2730 samples, validate on 683 samples\n",
      "Epoch 1/2\n",
      "2730/2730 [==============================] - 1s 269us/step - loss: 0.3296 - acc: 0.5832 - val_loss: 0.6955 - val_acc: 0.5154\n",
      "Epoch 2/2\n",
      "2730/2730 [==============================] - 1s 271us/step - loss: 0.3299 - acc: 0.5835 - val_loss: 0.6955 - val_acc: 0.5198\n",
      "Train on 2730 samples, validate on 683 samples\n",
      "Epoch 1/2\n",
      "2730/2730 [==============================] - 1s 266us/step - loss: 0.3298 - acc: 0.5788 - val_loss: 0.6956 - val_acc: 0.5212\n",
      "Epoch 2/2\n",
      "2730/2730 [==============================] - 1s 274us/step - loss: 0.3261 - acc: 0.5916 - val_loss: 0.6956 - val_acc: 0.5168\n",
      "Train on 2730 samples, validate on 683 samples\n",
      "Epoch 1/1\n",
      "2730/2730 [==============================] - 1s 271us/step - loss: 0.3276 - acc: 0.5853 - val_loss: 0.6956 - val_acc: 0.5139\n",
      "Train on 2730 samples, validate on 683 samples\n",
      "Epoch 1/5\n",
      "2730/2730 [==============================] - 1s 269us/step - loss: 0.3244 - acc: 0.6084 - val_loss: 0.6957 - val_acc: 0.5095\n",
      "Epoch 2/5\n",
      "2730/2730 [==============================] - 1s 270us/step - loss: 0.3237 - acc: 0.6084 - val_loss: 0.6957 - val_acc: 0.5095\n",
      "Epoch 3/5\n",
      "2730/2730 [==============================] - 1s 271us/step - loss: 0.3248 - acc: 0.6000 - val_loss: 0.6958 - val_acc: 0.5081\n",
      "Epoch 4/5\n",
      "2730/2730 [==============================] - 1s 269us/step - loss: 0.3247 - acc: 0.6037 - val_loss: 0.6958 - val_acc: 0.5081\n",
      "Epoch 5/5\n",
      "2730/2730 [==============================] - 1s 268us/step - loss: 0.3221 - acc: 0.6048 - val_loss: 0.6959 - val_acc: 0.5051\n",
      "Train on 2730 samples, validate on 683 samples\n",
      "Epoch 1/20\n",
      "2730/2730 [==============================] - 1s 267us/step - loss: 0.3220 - acc: 0.6139 - val_loss: 0.6959 - val_acc: 0.5066\n",
      "Epoch 2/20\n",
      "2730/2730 [==============================] - 1s 272us/step - loss: 0.3209 - acc: 0.6117 - val_loss: 0.6960 - val_acc: 0.5051\n",
      "Epoch 3/20\n",
      "2730/2730 [==============================] - 1s 270us/step - loss: 0.3189 - acc: 0.6403 - val_loss: 0.6960 - val_acc: 0.5051\n",
      "Epoch 4/20\n",
      "2730/2730 [==============================] - 1s 270us/step - loss: 0.3190 - acc: 0.6308 - val_loss: 0.6960 - val_acc: 0.5022\n",
      "Epoch 5/20\n",
      "2730/2730 [==============================] - 1s 270us/step - loss: 0.3155 - acc: 0.6451 - val_loss: 0.6961 - val_acc: 0.5037\n",
      "Epoch 6/20\n",
      "2730/2730 [==============================] - 1s 270us/step - loss: 0.3192 - acc: 0.5989 - val_loss: 0.6962 - val_acc: 0.5007\n",
      "Epoch 7/20\n",
      "2730/2730 [==============================] - 1s 272us/step - loss: 0.3170 - acc: 0.6374 - val_loss: 0.6963 - val_acc: 0.5007\n",
      "Epoch 8/20\n",
      "2730/2730 [==============================] - 1s 273us/step - loss: 0.3148 - acc: 0.6359 - val_loss: 0.6964 - val_acc: 0.5037\n",
      "Epoch 9/20\n",
      "2730/2730 [==============================] - 1s 272us/step - loss: 0.3143 - acc: 0.6418 - val_loss: 0.6966 - val_acc: 0.5037\n",
      "Epoch 10/20\n",
      "2730/2730 [==============================] - 1s 272us/step - loss: 0.3109 - acc: 0.6538 - val_loss: 0.6967 - val_acc: 0.5022\n",
      "Epoch 11/20\n",
      "2730/2730 [==============================] - 1s 270us/step - loss: 0.3125 - acc: 0.6421 - val_loss: 0.6968 - val_acc: 0.5007\n",
      "Epoch 12/20\n",
      "2730/2730 [==============================] - 1s 270us/step - loss: 0.3107 - acc: 0.6487 - val_loss: 0.6968 - val_acc: 0.5007\n",
      "Epoch 13/20\n",
      "2730/2730 [==============================] - 1s 272us/step - loss: 0.3103 - acc: 0.6553 - val_loss: 0.6969 - val_acc: 0.5022\n",
      "Epoch 14/20\n",
      "2730/2730 [==============================] - 1s 271us/step - loss: 0.3078 - acc: 0.6725 - val_loss: 0.6970 - val_acc: 0.5007\n",
      "Epoch 15/20\n",
      "2730/2730 [==============================] - 1s 269us/step - loss: 0.3069 - acc: 0.6751 - val_loss: 0.6971 - val_acc: 0.5007\n",
      "Epoch 16/20\n",
      "2730/2730 [==============================] - 1s 270us/step - loss: 0.3062 - acc: 0.6766 - val_loss: 0.6972 - val_acc: 0.5022\n",
      "Epoch 17/20\n",
      "2730/2730 [==============================] - 1s 269us/step - loss: 0.3039 - acc: 0.6791 - val_loss: 0.6972 - val_acc: 0.5066\n",
      "Epoch 18/20\n",
      "2730/2730 [==============================] - 1s 268us/step - loss: 0.3051 - acc: 0.6784 - val_loss: 0.6973 - val_acc: 0.5066\n",
      "Epoch 19/20\n",
      "2730/2730 [==============================] - 1s 270us/step - loss: 0.3034 - acc: 0.6821 - val_loss: 0.6974 - val_acc: 0.5110\n",
      "Epoch 20/20\n",
      "2730/2730 [==============================] - 1s 269us/step - loss: 0.3016 - acc: 0.6806 - val_loss: 0.6975 - val_acc: 0.5081\n",
      "Train on 2730 samples, validate on 683 samples\n",
      "Epoch 1/40\n",
      "2730/2730 [==============================] - 1s 267us/step - loss: 0.3003 - acc: 0.6894 - val_loss: 0.6976 - val_acc: 0.5110\n",
      "Epoch 2/40\n",
      "2730/2730 [==============================] - 1s 270us/step - loss: 0.2997 - acc: 0.6901 - val_loss: 0.6977 - val_acc: 0.5124\n",
      "Epoch 3/40\n",
      "2730/2730 [==============================] - 1s 269us/step - loss: 0.2973 - acc: 0.6996 - val_loss: 0.6979 - val_acc: 0.5110\n",
      "Epoch 4/40\n",
      "2730/2730 [==============================] - 1s 268us/step - loss: 0.2953 - acc: 0.7011 - val_loss: 0.6981 - val_acc: 0.5110\n",
      "Epoch 5/40\n",
      "2730/2730 [==============================] - 1s 268us/step - loss: 0.2965 - acc: 0.6934 - val_loss: 0.6982 - val_acc: 0.5095\n",
      "Epoch 6/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2730/2730 [==============================] - 1s 270us/step - loss: 0.2965 - acc: 0.6945 - val_loss: 0.6984 - val_acc: 0.5110\n",
      "Epoch 7/40\n",
      "2730/2730 [==============================] - 1s 270us/step - loss: 0.2958 - acc: 0.7011 - val_loss: 0.6986 - val_acc: 0.5095\n",
      "Epoch 8/40\n",
      "2730/2730 [==============================] - 1s 271us/step - loss: 0.2916 - acc: 0.7154 - val_loss: 0.6988 - val_acc: 0.5124\n",
      "Epoch 9/40\n",
      "2730/2730 [==============================] - 1s 270us/step - loss: 0.2891 - acc: 0.7212 - val_loss: 0.6991 - val_acc: 0.5110\n",
      "Epoch 10/40\n",
      "2730/2730 [==============================] - 1s 271us/step - loss: 0.2891 - acc: 0.7216 - val_loss: 0.6993 - val_acc: 0.5110\n",
      "Epoch 11/40\n",
      "2730/2730 [==============================] - 1s 268us/step - loss: 0.2870 - acc: 0.7264 - val_loss: 0.6995 - val_acc: 0.5095\n",
      "Epoch 12/40\n",
      "2730/2730 [==============================] - 1s 272us/step - loss: 0.2842 - acc: 0.7410 - val_loss: 0.6998 - val_acc: 0.5081\n",
      "Epoch 13/40\n",
      "2730/2730 [==============================] - 1s 270us/step - loss: 0.2837 - acc: 0.7341 - val_loss: 0.7000 - val_acc: 0.5081\n",
      "Epoch 14/40\n",
      "2730/2730 [==============================] - 1s 272us/step - loss: 0.2817 - acc: 0.7469 - val_loss: 0.7002 - val_acc: 0.5037\n",
      "Epoch 15/40\n",
      "2730/2730 [==============================] - 1s 267us/step - loss: 0.2825 - acc: 0.7447 - val_loss: 0.7004 - val_acc: 0.5022\n",
      "Epoch 16/40\n",
      "2730/2730 [==============================] - 1s 273us/step - loss: 0.2794 - acc: 0.7473 - val_loss: 0.7006 - val_acc: 0.5037\n",
      "Epoch 17/40\n",
      "2730/2730 [==============================] - 1s 270us/step - loss: 0.2791 - acc: 0.7374 - val_loss: 0.7009 - val_acc: 0.5051\n",
      "Epoch 18/40\n",
      "2730/2730 [==============================] - 1s 269us/step - loss: 0.2771 - acc: 0.7535 - val_loss: 0.7012 - val_acc: 0.5022\n",
      "Epoch 19/40\n",
      "2730/2730 [==============================] - 1s 270us/step - loss: 0.2741 - acc: 0.7678 - val_loss: 0.7014 - val_acc: 0.5022\n",
      "Epoch 20/40\n",
      "2730/2730 [==============================] - 1s 271us/step - loss: 0.2751 - acc: 0.7626 - val_loss: 0.7016 - val_acc: 0.5022\n",
      "Epoch 21/40\n",
      "2730/2730 [==============================] - 1s 273us/step - loss: 0.2735 - acc: 0.7568 - val_loss: 0.7020 - val_acc: 0.5051\n",
      "Epoch 22/40\n",
      "2730/2730 [==============================] - 1s 275us/step - loss: 0.2710 - acc: 0.7645 - val_loss: 0.7023 - val_acc: 0.5066\n",
      "Epoch 23/40\n",
      "2730/2730 [==============================] - 1s 273us/step - loss: 0.2703 - acc: 0.7645 - val_loss: 0.7026 - val_acc: 0.5095\n",
      "Epoch 24/40\n",
      "2730/2730 [==============================] - 1s 273us/step - loss: 0.2696 - acc: 0.7714 - val_loss: 0.7029 - val_acc: 0.5139\n",
      "Epoch 25/40\n",
      "2730/2730 [==============================] - 1s 274us/step - loss: 0.2675 - acc: 0.7718 - val_loss: 0.7033 - val_acc: 0.5139\n",
      "Epoch 26/40\n",
      "2730/2730 [==============================] - 1s 272us/step - loss: 0.2660 - acc: 0.7784 - val_loss: 0.7036 - val_acc: 0.5110\n",
      "Epoch 27/40\n",
      "2730/2730 [==============================] - 1s 272us/step - loss: 0.2648 - acc: 0.7813 - val_loss: 0.7039 - val_acc: 0.5124\n",
      "Epoch 28/40\n",
      "2730/2730 [==============================] - 1s 271us/step - loss: 0.2627 - acc: 0.7853 - val_loss: 0.7044 - val_acc: 0.5139\n",
      "Epoch 29/40\n",
      "2730/2730 [==============================] - 1s 268us/step - loss: 0.2597 - acc: 0.7890 - val_loss: 0.7049 - val_acc: 0.5168\n",
      "Epoch 30/40\n",
      "2730/2730 [==============================] - 1s 270us/step - loss: 0.2599 - acc: 0.7835 - val_loss: 0.7053 - val_acc: 0.5168\n",
      "Epoch 31/40\n",
      "2730/2730 [==============================] - 1s 272us/step - loss: 0.2581 - acc: 0.7945 - val_loss: 0.7056 - val_acc: 0.5168\n",
      "Epoch 32/40\n",
      "2730/2730 [==============================] - 1s 271us/step - loss: 0.2587 - acc: 0.7923 - val_loss: 0.7060 - val_acc: 0.5168\n",
      "Epoch 33/40\n",
      "2730/2730 [==============================] - 1s 272us/step - loss: 0.2558 - acc: 0.8048 - val_loss: 0.7065 - val_acc: 0.5168\n",
      "Epoch 34/40\n",
      "2730/2730 [==============================] - 1s 277us/step - loss: 0.2515 - acc: 0.8026 - val_loss: 0.7069 - val_acc: 0.5139\n",
      "Epoch 35/40\n",
      "2730/2730 [==============================] - 1s 271us/step - loss: 0.2524 - acc: 0.8007 - val_loss: 0.7073 - val_acc: 0.5139\n",
      "Epoch 36/40\n",
      "2730/2730 [==============================] - 1s 270us/step - loss: 0.2531 - acc: 0.7993 - val_loss: 0.7076 - val_acc: 0.5154\n",
      "Epoch 37/40\n",
      "2730/2730 [==============================] - 1s 270us/step - loss: 0.2499 - acc: 0.8099 - val_loss: 0.7079 - val_acc: 0.5168\n",
      "Epoch 38/40\n",
      "2730/2730 [==============================] - 1s 268us/step - loss: 0.2484 - acc: 0.8092 - val_loss: 0.7082 - val_acc: 0.5168\n",
      "Epoch 39/40\n",
      "2730/2730 [==============================] - 1s 279us/step - loss: 0.2475 - acc: 0.8110 - val_loss: 0.7086 - val_acc: 0.5154\n",
      "Epoch 40/40\n",
      "2730/2730 [==============================] - 1s 274us/step - loss: 0.2459 - acc: 0.8205 - val_loss: 0.7091 - val_acc: 0.5168\n",
      "Train on 2730 samples, validate on 683 samples\n",
      "Epoch 1/80\n",
      "2730/2730 [==============================] - 1s 272us/step - loss: 0.2425 - acc: 0.8275 - val_loss: 0.7095 - val_acc: 0.5168\n",
      "Epoch 2/80\n",
      "2730/2730 [==============================] - 1s 270us/step - loss: 0.2425 - acc: 0.8311 - val_loss: 0.7099 - val_acc: 0.5168\n",
      "Epoch 3/80\n",
      "2730/2730 [==============================] - 1s 269us/step - loss: 0.2418 - acc: 0.8216 - val_loss: 0.7104 - val_acc: 0.5183\n",
      "Epoch 4/80\n",
      "2730/2730 [==============================] - 1s 271us/step - loss: 0.2391 - acc: 0.8201 - val_loss: 0.7108 - val_acc: 0.5183\n",
      "Epoch 5/80\n",
      "2730/2730 [==============================] - 1s 275us/step - loss: 0.2381 - acc: 0.8234 - val_loss: 0.7113 - val_acc: 0.5212\n",
      "Epoch 6/80\n",
      "2730/2730 [==============================] - 1s 269us/step - loss: 0.2358 - acc: 0.8227 - val_loss: 0.7118 - val_acc: 0.5227\n",
      "Epoch 7/80\n",
      "2730/2730 [==============================] - 1s 267us/step - loss: 0.2344 - acc: 0.8326 - val_loss: 0.7124 - val_acc: 0.5212\n",
      "Epoch 8/80\n",
      "2730/2730 [==============================] - 1s 272us/step - loss: 0.2330 - acc: 0.8293 - val_loss: 0.7129 - val_acc: 0.5212\n",
      "Epoch 9/80\n",
      "2730/2730 [==============================] - 1s 272us/step - loss: 0.2321 - acc: 0.8399 - val_loss: 0.7135 - val_acc: 0.5256\n",
      "Epoch 10/80\n",
      "2730/2730 [==============================] - 1s 270us/step - loss: 0.2324 - acc: 0.8341 - val_loss: 0.7140 - val_acc: 0.5256\n",
      "Epoch 11/80\n",
      "2730/2730 [==============================] - 1s 270us/step - loss: 0.2302 - acc: 0.8286 - val_loss: 0.7144 - val_acc: 0.5242\n",
      "Epoch 12/80\n",
      "2730/2730 [==============================] - 1s 270us/step - loss: 0.2267 - acc: 0.8355 - val_loss: 0.7149 - val_acc: 0.5212\n",
      "Epoch 13/80\n",
      "2730/2730 [==============================] - 1s 271us/step - loss: 0.2276 - acc: 0.8436 - val_loss: 0.7155 - val_acc: 0.5198\n",
      "Epoch 14/80\n",
      "2730/2730 [==============================] - 1s 272us/step - loss: 0.2249 - acc: 0.8403 - val_loss: 0.7161 - val_acc: 0.5183\n",
      "Epoch 15/80\n",
      "2730/2730 [==============================] - 1s 274us/step - loss: 0.2219 - acc: 0.8491 - val_loss: 0.7166 - val_acc: 0.5183\n",
      "Epoch 16/80\n",
      "2730/2730 [==============================] - 1s 273us/step - loss: 0.2213 - acc: 0.8520 - val_loss: 0.7173 - val_acc: 0.5183\n",
      "Epoch 17/80\n",
      "2730/2730 [==============================] - 1s 274us/step - loss: 0.2208 - acc: 0.8527 - val_loss: 0.7178 - val_acc: 0.5154\n",
      "Epoch 18/80\n",
      "2730/2730 [==============================] - 1s 271us/step - loss: 0.2192 - acc: 0.8480 - val_loss: 0.7184 - val_acc: 0.5168\n",
      "Epoch 19/80\n",
      "2730/2730 [==============================] - 1s 273us/step - loss: 0.2169 - acc: 0.8505 - val_loss: 0.7190 - val_acc: 0.5183\n",
      "Epoch 20/80\n",
      "2730/2730 [==============================] - 1s 276us/step - loss: 0.2161 - acc: 0.8546 - val_loss: 0.7195 - val_acc: 0.5183\n",
      "Epoch 21/80\n",
      "2730/2730 [==============================] - 1s 270us/step - loss: 0.2144 - acc: 0.8549 - val_loss: 0.7199 - val_acc: 0.5212\n",
      "Epoch 22/80\n",
      "2730/2730 [==============================] - 1s 275us/step - loss: 0.2142 - acc: 0.8513 - val_loss: 0.7204 - val_acc: 0.5198\n",
      "Epoch 23/80\n",
      "2730/2730 [==============================] - 1s 270us/step - loss: 0.2109 - acc: 0.8590 - val_loss: 0.7208 - val_acc: 0.5212\n",
      "Epoch 24/80\n",
      "2730/2730 [==============================] - 1s 278us/step - loss: 0.2113 - acc: 0.8538 - val_loss: 0.7213 - val_acc: 0.5212\n",
      "Epoch 25/80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2730/2730 [==============================] - 1s 268us/step - loss: 0.2107 - acc: 0.8535 - val_loss: 0.7219 - val_acc: 0.5242\n",
      "Epoch 26/80\n",
      "2730/2730 [==============================] - 1s 270us/step - loss: 0.2095 - acc: 0.8524 - val_loss: 0.7225 - val_acc: 0.5227\n",
      "Epoch 27/80\n",
      "2730/2730 [==============================] - 1s 271us/step - loss: 0.2064 - acc: 0.8659 - val_loss: 0.7230 - val_acc: 0.5198\n",
      "Epoch 28/80\n",
      "2730/2730 [==============================] - 1s 273us/step - loss: 0.2064 - acc: 0.8608 - val_loss: 0.7234 - val_acc: 0.5227\n",
      "Epoch 29/80\n",
      "2730/2730 [==============================] - 1s 268us/step - loss: 0.2036 - acc: 0.8626 - val_loss: 0.7238 - val_acc: 0.5227\n",
      "Epoch 30/80\n",
      "2730/2730 [==============================] - 1s 271us/step - loss: 0.2018 - acc: 0.8645 - val_loss: 0.7242 - val_acc: 0.5227\n",
      "Epoch 31/80\n",
      "2730/2730 [==============================] - 1s 271us/step - loss: 0.2023 - acc: 0.8707 - val_loss: 0.7247 - val_acc: 0.5198\n",
      "Epoch 32/80\n",
      "2730/2730 [==============================] - 1s 270us/step - loss: 0.1997 - acc: 0.8729 - val_loss: 0.7251 - val_acc: 0.5212\n",
      "Epoch 33/80\n",
      "2730/2730 [==============================] - 1s 269us/step - loss: 0.1984 - acc: 0.8733 - val_loss: 0.7256 - val_acc: 0.5227\n",
      "Epoch 34/80\n",
      "2730/2730 [==============================] - 1s 269us/step - loss: 0.1984 - acc: 0.8696 - val_loss: 0.7260 - val_acc: 0.5256\n",
      "Epoch 35/80\n",
      "2730/2730 [==============================] - 1s 271us/step - loss: 0.1967 - acc: 0.8747 - val_loss: 0.7265 - val_acc: 0.5256\n",
      "Epoch 36/80\n",
      "2730/2730 [==============================] - 1s 272us/step - loss: 0.1952 - acc: 0.8714 - val_loss: 0.7269 - val_acc: 0.5242\n",
      "Epoch 37/80\n",
      "2730/2730 [==============================] - 1s 269us/step - loss: 0.1936 - acc: 0.8780 - val_loss: 0.7274 - val_acc: 0.5271\n",
      "Epoch 38/80\n",
      "2730/2730 [==============================] - 1s 272us/step - loss: 0.1944 - acc: 0.8729 - val_loss: 0.7278 - val_acc: 0.5315\n",
      "Epoch 39/80\n",
      "2730/2730 [==============================] - 1s 269us/step - loss: 0.1913 - acc: 0.8751 - val_loss: 0.7282 - val_acc: 0.5315\n",
      "Epoch 40/80\n",
      "2730/2730 [==============================] - 1s 271us/step - loss: 0.1908 - acc: 0.8780 - val_loss: 0.7287 - val_acc: 0.5315\n",
      "Epoch 41/80\n",
      "2730/2730 [==============================] - 1s 271us/step - loss: 0.1884 - acc: 0.8788 - val_loss: 0.7293 - val_acc: 0.5329\n",
      "Epoch 42/80\n",
      "2730/2730 [==============================] - 1s 270us/step - loss: 0.1893 - acc: 0.8788 - val_loss: 0.7300 - val_acc: 0.5329\n",
      "Epoch 43/80\n",
      "2730/2730 [==============================] - 1s 269us/step - loss: 0.1855 - acc: 0.8802 - val_loss: 0.7307 - val_acc: 0.5344\n"
     ]
    }
   ],
   "source": [
    "class DeepEstimator():\n",
    "    def __init__(self):\n",
    "        return None\n",
    "    def fit(self, X, y, X_val, y_val):\n",
    "        class_weight = {0: 0.05,\n",
    "                1: 0.95}\n",
    "        self.model = self.__define_model(X)\n",
    "        sched = [[0.0001, 2], [0.001, 20], [0.01, 2], [0.1, 2], [0.5, 1], [0.1, 5], [0.01, 20], [0.001, 40], [0.0001, 80], [0.00005, 120]]\n",
    "        for lr, epochs in sched:\n",
    "            self.model.optimizer.lr = lr\n",
    "            self.model.fit(np.array(X), np.array(y), epochs=epochs, verbose=True, class_weight=class_weight, batch_size=2048, validation_data=(X_val, y_val))\n",
    "        return self\n",
    "    def predict(self, X):\n",
    "        return self.model.predict(X)\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        return self.model.predict(X)\n",
    "    \n",
    "    def __define_model(self, X):\n",
    "        shape = X.shape[1]\n",
    "        model = Sequential([\n",
    "            BatchNormalization(input_shape=(shape,)),\n",
    "            Dense(64, kernel_initializer='normal', activation='relu'),\n",
    "            Dropout(0.5),\n",
    "            BatchNormalization(),\n",
    "            Dense(32, kernel_initializer='normal', activation='relu'),\n",
    "            Dropout(0.5),\n",
    "            BatchNormalization(),\n",
    "            Dense(1, kernel_initializer='normal', activation='sigmoid')   \n",
    "        ])\n",
    "\n",
    "        model.compile(optimizer='adam',\n",
    "                      loss='binary_crossentropy',\n",
    "                      metrics=['accuracy'])\n",
    "        return model\n",
    "    \n",
    "#                 Dense(1024, activation='relu', kernel_regularizer=regularizers.l1(0.5)),\n",
    "#             Dropout(0.5),\n",
    "#             BatchNormalization(),\n",
    "#             Dense(64, activation='relu', kernel_regularizer=regularizers.l1(0.5)),\n",
    "#             Dropout(0.5),\n",
    "#             BatchNormalization(),\n",
    "    \n",
    "\n",
    "# select_fpr = SelectFpr(alpha=0.99)\n",
    "\n",
    "# X_train = select_fpr.fit_transform(X_train, y_train)\n",
    "# X_test = select_fpr.transform(X_test)\n",
    "\n",
    "# deep_estimator = DeepEstimator()\n",
    "# deep_estimator.fit(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = deep_estimator.predict(X_test)\n",
    "Plotter(preds).plot_score(np.array(y_test_continuous))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2730 samples, validate on 683 samples\n",
      "Epoch 1/2\n",
      "2730/2730 [==============================] - 2s 607us/step - loss: 35.7009 - acc: 0.4897 - val_loss: 36.0528 - val_acc: 0.4817\n",
      "Epoch 2/2\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 35.5556 - acc: 0.4930 - val_loss: 35.8884 - val_acc: 0.4817\n",
      "Train on 2730 samples, validate on 683 samples\n",
      "Epoch 1/20\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 35.3910 - acc: 0.5022 - val_loss: 35.7208 - val_acc: 0.4832\n",
      "Epoch 2/20\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 35.2213 - acc: 0.4985 - val_loss: 35.5481 - val_acc: 0.4832\n",
      "Epoch 3/20\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 35.0510 - acc: 0.4996 - val_loss: 35.3746 - val_acc: 0.4817\n",
      "Epoch 4/20\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 34.8768 - acc: 0.5099 - val_loss: 35.1991 - val_acc: 0.4817\n",
      "Epoch 5/20\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 34.7034 - acc: 0.5011 - val_loss: 35.0228 - val_acc: 0.4832\n",
      "Epoch 6/20\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 34.5301 - acc: 0.4945 - val_loss: 34.8479 - val_acc: 0.4846\n",
      "Epoch 7/20\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 34.3581 - acc: 0.4938 - val_loss: 34.6734 - val_acc: 0.4846\n",
      "Epoch 8/20\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 34.1842 - acc: 0.4916 - val_loss: 34.4987 - val_acc: 0.4846\n",
      "Epoch 9/20\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 34.0119 - acc: 0.5004 - val_loss: 34.3255 - val_acc: 0.4846\n",
      "Epoch 10/20\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 33.8377 - acc: 0.4978 - val_loss: 34.1521 - val_acc: 0.4846\n",
      "Epoch 11/20\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 33.6689 - acc: 0.4894 - val_loss: 33.9795 - val_acc: 0.4846\n",
      "Epoch 12/20\n",
      "2730/2730 [==============================] - 0s 15us/step - loss: 33.4953 - acc: 0.4832 - val_loss: 33.8071 - val_acc: 0.4832\n",
      "Epoch 13/20\n",
      "2730/2730 [==============================] - 0s 15us/step - loss: 33.3220 - acc: 0.4982 - val_loss: 33.6361 - val_acc: 0.4817\n",
      "Epoch 14/20\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 33.1538 - acc: 0.5059 - val_loss: 33.4651 - val_acc: 0.4817\n",
      "Epoch 15/20\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 32.9847 - acc: 0.4949 - val_loss: 33.2947 - val_acc: 0.4817\n",
      "Epoch 16/20\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 32.8151 - acc: 0.4857 - val_loss: 33.1249 - val_acc: 0.4788\n",
      "Epoch 17/20\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 32.6442 - acc: 0.5073 - val_loss: 32.9555 - val_acc: 0.4788\n",
      "Epoch 18/20\n",
      "2730/2730 [==============================] - 0s 15us/step - loss: 32.4773 - acc: 0.5051 - val_loss: 32.7872 - val_acc: 0.4788\n",
      "Epoch 19/20\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 32.3114 - acc: 0.4883 - val_loss: 32.6194 - val_acc: 0.4788\n",
      "Epoch 20/20\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 32.1439 - acc: 0.4912 - val_loss: 32.4524 - val_acc: 0.4802\n",
      "Train on 2730 samples, validate on 683 samples\n",
      "Epoch 1/2\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 31.9784 - acc: 0.4971 - val_loss: 32.2860 - val_acc: 0.4802\n",
      "Epoch 2/2\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 31.8130 - acc: 0.4872 - val_loss: 32.1202 - val_acc: 0.4817\n",
      "Train on 2730 samples, validate on 683 samples\n",
      "Epoch 1/2\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 31.6496 - acc: 0.4963 - val_loss: 31.9553 - val_acc: 0.4802\n",
      "Epoch 2/2\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 31.4822 - acc: 0.5095 - val_loss: 31.7906 - val_acc: 0.4817\n",
      "Train on 2730 samples, validate on 683 samples\n",
      "Epoch 1/1\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 31.3192 - acc: 0.5066 - val_loss: 31.6263 - val_acc: 0.4802\n",
      "Train on 2730 samples, validate on 683 samples\n",
      "Epoch 1/5\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 31.1573 - acc: 0.4971 - val_loss: 31.4629 - val_acc: 0.4802\n",
      "Epoch 2/5\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 30.9953 - acc: 0.4850 - val_loss: 31.2999 - val_acc: 0.4832\n",
      "Epoch 3/5\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 30.8317 - acc: 0.5007 - val_loss: 31.1377 - val_acc: 0.4817\n",
      "Epoch 4/5\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 30.6717 - acc: 0.4967 - val_loss: 30.9762 - val_acc: 0.4817\n",
      "Epoch 5/5\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 30.5098 - acc: 0.5099 - val_loss: 30.8152 - val_acc: 0.4802\n",
      "Train on 2730 samples, validate on 683 samples\n",
      "Epoch 1/20\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 30.3520 - acc: 0.4908 - val_loss: 30.6548 - val_acc: 0.4832\n",
      "Epoch 2/20\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 30.1911 - acc: 0.4960 - val_loss: 30.4949 - val_acc: 0.4832\n",
      "Epoch 3/20\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 30.0318 - acc: 0.4952 - val_loss: 30.3358 - val_acc: 0.4817\n",
      "Epoch 4/20\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 29.8715 - acc: 0.5139 - val_loss: 30.1774 - val_acc: 0.4802\n",
      "Epoch 5/20\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 29.7167 - acc: 0.5015 - val_loss: 30.0194 - val_acc: 0.4802\n",
      "Epoch 6/20\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 29.5574 - acc: 0.5077 - val_loss: 29.8620 - val_acc: 0.4817\n",
      "Epoch 7/20\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 29.4012 - acc: 0.5136 - val_loss: 29.7053 - val_acc: 0.4817\n",
      "Epoch 8/20\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 29.2465 - acc: 0.4949 - val_loss: 29.5493 - val_acc: 0.4817\n",
      "Epoch 9/20\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 29.0919 - acc: 0.4916 - val_loss: 29.3939 - val_acc: 0.4817\n",
      "Epoch 10/20\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 28.9381 - acc: 0.4908 - val_loss: 29.2390 - val_acc: 0.4802\n",
      "Epoch 11/20\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 28.7818 - acc: 0.5048 - val_loss: 29.0846 - val_acc: 0.4802\n",
      "Epoch 12/20\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 28.6277 - acc: 0.4974 - val_loss: 28.9308 - val_acc: 0.4802\n",
      "Epoch 13/20\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 28.4754 - acc: 0.5070 - val_loss: 28.7775 - val_acc: 0.4788\n",
      "Epoch 14/20\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 28.3219 - acc: 0.5084 - val_loss: 28.6250 - val_acc: 0.4788\n",
      "Epoch 15/20\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 28.1718 - acc: 0.4952 - val_loss: 28.4730 - val_acc: 0.4817\n",
      "Epoch 16/20\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 28.0186 - acc: 0.5077 - val_loss: 28.3216 - val_acc: 0.4846\n",
      "Epoch 17/20\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 27.8693 - acc: 0.4974 - val_loss: 28.1706 - val_acc: 0.4832\n",
      "Epoch 18/20\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 27.7195 - acc: 0.4967 - val_loss: 28.0204 - val_acc: 0.4832\n",
      "Epoch 19/20\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 27.5693 - acc: 0.5051 - val_loss: 27.8707 - val_acc: 0.4832\n",
      "Epoch 20/20\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 27.4215 - acc: 0.4835 - val_loss: 27.7217 - val_acc: 0.4846\n",
      "Train on 2730 samples, validate on 683 samples\n",
      "Epoch 1/40\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 27.2721 - acc: 0.5073 - val_loss: 27.5734 - val_acc: 0.4846\n",
      "Epoch 2/40\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 27.1247 - acc: 0.5136 - val_loss: 27.4256 - val_acc: 0.4846\n",
      "Epoch 3/40\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 26.9775 - acc: 0.4941 - val_loss: 27.2781 - val_acc: 0.4861\n",
      "Epoch 4/40\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 26.8332 - acc: 0.4879 - val_loss: 27.1313 - val_acc: 0.4861\n",
      "Epoch 5/40\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 26.6857 - acc: 0.4956 - val_loss: 26.9851 - val_acc: 0.4846\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/40\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 26.5360 - acc: 0.5044 - val_loss: 26.8395 - val_acc: 0.4846\n",
      "Epoch 7/40\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 26.3954 - acc: 0.4901 - val_loss: 26.6945 - val_acc: 0.4832\n",
      "Epoch 8/40\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 26.2495 - acc: 0.4982 - val_loss: 26.5500 - val_acc: 0.4817\n",
      "Epoch 9/40\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 26.1060 - acc: 0.4857 - val_loss: 26.4062 - val_acc: 0.4832\n",
      "Epoch 10/40\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 25.9635 - acc: 0.4901 - val_loss: 26.2628 - val_acc: 0.4802\n",
      "Epoch 11/40\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 25.8185 - acc: 0.4978 - val_loss: 26.1202 - val_acc: 0.4802\n",
      "Epoch 12/40\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 25.6773 - acc: 0.4982 - val_loss: 25.9781 - val_acc: 0.4788\n",
      "Epoch 13/40\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 25.5350 - acc: 0.4956 - val_loss: 25.8367 - val_acc: 0.4788\n",
      "Epoch 14/40\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 25.3957 - acc: 0.5051 - val_loss: 25.6957 - val_acc: 0.4788\n",
      "Epoch 15/40\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 25.2526 - acc: 0.5048 - val_loss: 25.5553 - val_acc: 0.4788\n",
      "Epoch 16/40\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 25.1153 - acc: 0.4919 - val_loss: 25.4153 - val_acc: 0.4817\n",
      "Epoch 17/40\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 24.9752 - acc: 0.4985 - val_loss: 25.2759 - val_acc: 0.4802\n",
      "Epoch 18/40\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 24.8358 - acc: 0.4949 - val_loss: 25.1372 - val_acc: 0.4802\n",
      "Epoch 19/40\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 24.6962 - acc: 0.5059 - val_loss: 24.9991 - val_acc: 0.4788\n",
      "Epoch 20/40\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 24.5585 - acc: 0.5048 - val_loss: 24.8615 - val_acc: 0.4758\n",
      "Epoch 21/40\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 24.4223 - acc: 0.4963 - val_loss: 24.7244 - val_acc: 0.4758\n",
      "Epoch 22/40\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 24.2865 - acc: 0.5044 - val_loss: 24.5879 - val_acc: 0.4744\n",
      "Epoch 23/40\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 24.1504 - acc: 0.4912 - val_loss: 24.4519 - val_acc: 0.4714\n",
      "Epoch 24/40\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 24.0151 - acc: 0.4952 - val_loss: 24.3164 - val_acc: 0.4714\n",
      "Epoch 25/40\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 23.8805 - acc: 0.4875 - val_loss: 24.1814 - val_acc: 0.4744\n",
      "Epoch 26/40\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 23.7464 - acc: 0.4890 - val_loss: 24.0468 - val_acc: 0.4758\n",
      "Epoch 27/40\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 23.6101 - acc: 0.5070 - val_loss: 23.9127 - val_acc: 0.4758\n",
      "Epoch 28/40\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 23.4770 - acc: 0.4996 - val_loss: 23.7790 - val_acc: 0.4773\n",
      "Epoch 29/40\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 23.3438 - acc: 0.5062 - val_loss: 23.6459 - val_acc: 0.4758\n",
      "Epoch 30/40\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 23.2115 - acc: 0.4996 - val_loss: 23.5135 - val_acc: 0.4758\n",
      "Epoch 31/40\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 23.0788 - acc: 0.4967 - val_loss: 23.3814 - val_acc: 0.4773\n",
      "Epoch 32/40\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 22.9470 - acc: 0.5066 - val_loss: 23.2501 - val_acc: 0.4773\n",
      "Epoch 33/40\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 22.8176 - acc: 0.4806 - val_loss: 23.1193 - val_acc: 0.4802\n",
      "Epoch 34/40\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 22.6858 - acc: 0.4996 - val_loss: 22.9890 - val_acc: 0.4788\n",
      "Epoch 35/40\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 22.5569 - acc: 0.5059 - val_loss: 22.8591 - val_acc: 0.4788\n",
      "Epoch 36/40\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 22.4298 - acc: 0.4729 - val_loss: 22.7298 - val_acc: 0.4802\n",
      "Epoch 37/40\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 22.2978 - acc: 0.4949 - val_loss: 22.6009 - val_acc: 0.4817\n",
      "Epoch 38/40\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 22.1709 - acc: 0.4886 - val_loss: 22.4726 - val_acc: 0.4817\n",
      "Epoch 39/40\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 22.0431 - acc: 0.4897 - val_loss: 22.3449 - val_acc: 0.4802\n",
      "Epoch 40/40\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 21.9149 - acc: 0.4971 - val_loss: 22.2177 - val_acc: 0.4802\n",
      "Train on 2730 samples, validate on 683 samples\n",
      "Epoch 1/80\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 21.7885 - acc: 0.4967 - val_loss: 22.0909 - val_acc: 0.4773\n",
      "Epoch 2/80\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 21.6641 - acc: 0.4875 - val_loss: 21.9647 - val_acc: 0.4758\n",
      "Epoch 3/80\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 21.5381 - acc: 0.4842 - val_loss: 21.8392 - val_acc: 0.4773\n",
      "Epoch 4/80\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 21.4092 - acc: 0.5139 - val_loss: 21.7143 - val_acc: 0.4773\n",
      "Epoch 5/80\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 21.2876 - acc: 0.5048 - val_loss: 21.5898 - val_acc: 0.4788\n",
      "Epoch 6/80\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 21.1642 - acc: 0.4916 - val_loss: 21.4657 - val_acc: 0.4817\n",
      "Epoch 7/80\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 21.0390 - acc: 0.4985 - val_loss: 21.3422 - val_acc: 0.4817\n",
      "Epoch 8/80\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 20.9151 - acc: 0.5095 - val_loss: 21.2190 - val_acc: 0.4817\n",
      "Epoch 9/80\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 20.7934 - acc: 0.5033 - val_loss: 21.0964 - val_acc: 0.4817\n",
      "Epoch 10/80\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 20.6713 - acc: 0.4985 - val_loss: 20.9744 - val_acc: 0.4817\n",
      "Epoch 11/80\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 20.5480 - acc: 0.5136 - val_loss: 20.8530 - val_acc: 0.4832\n",
      "Epoch 12/80\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 20.4274 - acc: 0.5216 - val_loss: 20.7321 - val_acc: 0.4832\n",
      "Epoch 13/80\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 20.3078 - acc: 0.5084 - val_loss: 20.6119 - val_acc: 0.4832\n",
      "Epoch 14/80\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 20.1880 - acc: 0.4949 - val_loss: 20.4921 - val_acc: 0.4832\n",
      "Epoch 15/80\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 20.0694 - acc: 0.4971 - val_loss: 20.3730 - val_acc: 0.4832\n",
      "Epoch 16/80\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 19.9490 - acc: 0.5026 - val_loss: 20.2544 - val_acc: 0.4832\n",
      "Epoch 17/80\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 19.8308 - acc: 0.4956 - val_loss: 20.1363 - val_acc: 0.4817\n",
      "Epoch 18/80\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 19.7133 - acc: 0.4941 - val_loss: 20.0188 - val_acc: 0.4817\n",
      "Epoch 19/80\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 19.5950 - acc: 0.5081 - val_loss: 19.9016 - val_acc: 0.4832\n",
      "Epoch 20/80\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 19.4793 - acc: 0.5026 - val_loss: 19.7850 - val_acc: 0.4832\n",
      "Epoch 21/80\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 19.3621 - acc: 0.4996 - val_loss: 19.6688 - val_acc: 0.4817\n",
      "Epoch 22/80\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 19.2463 - acc: 0.4945 - val_loss: 19.5532 - val_acc: 0.4817\n",
      "Epoch 23/80\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 19.1314 - acc: 0.4897 - val_loss: 19.4381 - val_acc: 0.4832\n",
      "Epoch 24/80\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 19.0169 - acc: 0.4996 - val_loss: 19.3236 - val_acc: 0.4846\n",
      "Epoch 25/80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2730/2730 [==============================] - 0s 14us/step - loss: 18.9005 - acc: 0.5125 - val_loss: 19.2094 - val_acc: 0.4861\n",
      "Epoch 26/80\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 18.7885 - acc: 0.5007 - val_loss: 19.0959 - val_acc: 0.4861\n",
      "Epoch 27/80\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 18.6747 - acc: 0.4963 - val_loss: 18.9828 - val_acc: 0.4832\n",
      "Epoch 28/80\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 18.5608 - acc: 0.5073 - val_loss: 18.8703 - val_acc: 0.4817\n",
      "Epoch 29/80\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 18.4489 - acc: 0.5033 - val_loss: 18.7583 - val_acc: 0.4817\n",
      "Epoch 30/80\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 18.3366 - acc: 0.5132 - val_loss: 18.6469 - val_acc: 0.4817\n",
      "Epoch 31/80\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 18.2264 - acc: 0.4864 - val_loss: 18.5360 - val_acc: 0.4817\n",
      "Epoch 32/80\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 18.1142 - acc: 0.5048 - val_loss: 18.4255 - val_acc: 0.4817\n",
      "Epoch 33/80\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 18.0040 - acc: 0.5073 - val_loss: 18.3156 - val_acc: 0.4802\n",
      "Epoch 34/80\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 17.8962 - acc: 0.4967 - val_loss: 18.2062 - val_acc: 0.4817\n",
      "Epoch 35/80\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 17.7849 - acc: 0.5066 - val_loss: 18.0973 - val_acc: 0.4802\n",
      "Epoch 36/80\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 17.6773 - acc: 0.4978 - val_loss: 17.9888 - val_acc: 0.4802\n",
      "Epoch 37/80\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 17.5687 - acc: 0.5066 - val_loss: 17.8808 - val_acc: 0.4817\n",
      "Epoch 38/80\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 17.4622 - acc: 0.4875 - val_loss: 17.7732 - val_acc: 0.4802\n",
      "Epoch 39/80\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 17.3544 - acc: 0.4927 - val_loss: 17.6661 - val_acc: 0.4832\n",
      "Epoch 40/80\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 17.2458 - acc: 0.5066 - val_loss: 17.5595 - val_acc: 0.4861\n",
      "Epoch 41/80\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 17.1391 - acc: 0.4993 - val_loss: 17.4534 - val_acc: 0.4876\n",
      "Epoch 42/80\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 17.0342 - acc: 0.5015 - val_loss: 17.3478 - val_acc: 0.4861\n",
      "Epoch 43/80\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 16.9279 - acc: 0.5154 - val_loss: 17.2426 - val_acc: 0.4876\n",
      "Epoch 44/80\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 16.8249 - acc: 0.4886 - val_loss: 17.1380 - val_acc: 0.4890\n",
      "Epoch 45/80\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 16.7196 - acc: 0.4963 - val_loss: 17.0339 - val_acc: 0.4876\n",
      "Epoch 46/80\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 16.6146 - acc: 0.5073 - val_loss: 16.9302 - val_acc: 0.4890\n",
      "Epoch 47/80\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 16.5112 - acc: 0.5092 - val_loss: 16.8271 - val_acc: 0.4905\n",
      "Epoch 48/80\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 16.4093 - acc: 0.5051 - val_loss: 16.7246 - val_acc: 0.4905\n",
      "Epoch 49/80\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 16.3064 - acc: 0.4985 - val_loss: 16.6225 - val_acc: 0.4905\n",
      "Epoch 50/80\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 16.2050 - acc: 0.5004 - val_loss: 16.5209 - val_acc: 0.4919\n",
      "Epoch 51/80\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 16.1027 - acc: 0.4989 - val_loss: 16.4197 - val_acc: 0.4919\n",
      "Epoch 52/80\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 16.0023 - acc: 0.4941 - val_loss: 16.3190 - val_acc: 0.4919\n",
      "Epoch 53/80\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 15.9015 - acc: 0.5037 - val_loss: 16.2187 - val_acc: 0.4934\n",
      "Epoch 54/80\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 15.8019 - acc: 0.4963 - val_loss: 16.1189 - val_acc: 0.4963\n",
      "Epoch 55/80\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 15.7022 - acc: 0.4930 - val_loss: 16.0197 - val_acc: 0.4963\n",
      "Epoch 56/80\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 15.6030 - acc: 0.5018 - val_loss: 15.9209 - val_acc: 0.4993\n",
      "Epoch 57/80\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 15.5048 - acc: 0.4985 - val_loss: 15.8227 - val_acc: 0.4949\n",
      "Epoch 58/80\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 15.4066 - acc: 0.5022 - val_loss: 15.7248 - val_acc: 0.4919\n",
      "Epoch 59/80\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 15.3089 - acc: 0.4930 - val_loss: 15.6276 - val_acc: 0.4919\n",
      "Epoch 60/80\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 15.2119 - acc: 0.5081 - val_loss: 15.5308 - val_acc: 0.4934\n",
      "Epoch 61/80\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 15.1148 - acc: 0.5033 - val_loss: 15.4344 - val_acc: 0.4934\n",
      "Epoch 62/80\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 15.0179 - acc: 0.5037 - val_loss: 15.3386 - val_acc: 0.4890\n",
      "Epoch 63/80\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 14.9223 - acc: 0.5121 - val_loss: 15.2433 - val_acc: 0.4890\n",
      "Epoch 64/80\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 14.8266 - acc: 0.5073 - val_loss: 15.1485 - val_acc: 0.4905\n",
      "Epoch 65/80\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 14.7326 - acc: 0.4927 - val_loss: 15.0541 - val_acc: 0.4919\n",
      "Epoch 66/80\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 14.6370 - acc: 0.5128 - val_loss: 14.9603 - val_acc: 0.4890\n",
      "Epoch 67/80\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 14.5439 - acc: 0.5147 - val_loss: 14.8668 - val_acc: 0.4905\n",
      "Epoch 68/80\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 14.4496 - acc: 0.5227 - val_loss: 14.7738 - val_acc: 0.4890\n",
      "Epoch 69/80\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 14.3577 - acc: 0.5081 - val_loss: 14.6813 - val_acc: 0.4890\n",
      "Epoch 70/80\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 14.2659 - acc: 0.4956 - val_loss: 14.5892 - val_acc: 0.4890\n",
      "Epoch 71/80\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 14.1741 - acc: 0.5033 - val_loss: 14.4976 - val_acc: 0.4890\n",
      "Epoch 72/80\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 14.0824 - acc: 0.4883 - val_loss: 14.4064 - val_acc: 0.4861\n",
      "Epoch 73/80\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 13.9897 - acc: 0.5084 - val_loss: 14.3157 - val_acc: 0.4876\n",
      "Epoch 74/80\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 13.9007 - acc: 0.4956 - val_loss: 14.2254 - val_acc: 0.4876\n",
      "Epoch 75/80\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 13.8096 - acc: 0.5004 - val_loss: 14.1356 - val_acc: 0.4890\n",
      "Epoch 76/80\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 13.7195 - acc: 0.5044 - val_loss: 14.0462 - val_acc: 0.4905\n",
      "Epoch 77/80\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 13.6307 - acc: 0.4963 - val_loss: 13.9574 - val_acc: 0.4905\n",
      "Epoch 78/80\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 13.5436 - acc: 0.4886 - val_loss: 13.8690 - val_acc: 0.4934\n",
      "Epoch 79/80\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 13.4531 - acc: 0.5077 - val_loss: 13.7811 - val_acc: 0.4949\n",
      "Epoch 80/80\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 13.3666 - acc: 0.4941 - val_loss: 13.6936 - val_acc: 0.4949\n",
      "Train on 2730 samples, validate on 683 samples\n",
      "Epoch 1/120\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 13.2782 - acc: 0.4963 - val_loss: 13.6065 - val_acc: 0.4934\n",
      "Epoch 2/120\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 13.1918 - acc: 0.4993 - val_loss: 13.5199 - val_acc: 0.4934\n",
      "Epoch 3/120\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 13.1046 - acc: 0.5000 - val_loss: 13.4336 - val_acc: 0.4949\n",
      "Epoch 4/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2730/2730 [==============================] - 0s 14us/step - loss: 13.0182 - acc: 0.5048 - val_loss: 13.3477 - val_acc: 0.4919\n",
      "Epoch 5/120\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 12.9326 - acc: 0.5037 - val_loss: 13.2623 - val_acc: 0.4949\n",
      "Epoch 6/120\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 12.8473 - acc: 0.4956 - val_loss: 13.1773 - val_acc: 0.4963\n",
      "Epoch 7/120\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 12.7609 - acc: 0.5011 - val_loss: 13.0928 - val_acc: 0.4949\n",
      "Epoch 8/120\n",
      "2730/2730 [==============================] - 0s 15us/step - loss: 12.6761 - acc: 0.5092 - val_loss: 13.0088 - val_acc: 0.4949\n",
      "Epoch 9/120\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 12.5937 - acc: 0.5004 - val_loss: 12.9252 - val_acc: 0.4934\n",
      "Epoch 10/120\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 12.5087 - acc: 0.5059 - val_loss: 12.8421 - val_acc: 0.4919\n",
      "Epoch 11/120\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 12.4265 - acc: 0.5117 - val_loss: 12.7593 - val_acc: 0.4919\n",
      "Epoch 12/120\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 12.3435 - acc: 0.5095 - val_loss: 12.6771 - val_acc: 0.4890\n",
      "Epoch 13/120\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 12.2611 - acc: 0.5092 - val_loss: 12.5953 - val_acc: 0.4905\n",
      "Epoch 14/120\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 12.1800 - acc: 0.5000 - val_loss: 12.5139 - val_acc: 0.4905\n",
      "Epoch 15/120\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 12.0989 - acc: 0.4982 - val_loss: 12.4330 - val_acc: 0.4919\n",
      "Epoch 16/120\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 12.0172 - acc: 0.5015 - val_loss: 12.3523 - val_acc: 0.4919\n",
      "Epoch 17/120\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 11.9373 - acc: 0.5044 - val_loss: 12.2723 - val_acc: 0.4919\n",
      "Epoch 18/120\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 11.8571 - acc: 0.5051 - val_loss: 12.1927 - val_acc: 0.4934\n",
      "Epoch 19/120\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 11.7760 - acc: 0.5062 - val_loss: 12.1134 - val_acc: 0.4949\n",
      "Epoch 20/120\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 11.6981 - acc: 0.4978 - val_loss: 12.0345 - val_acc: 0.4949\n",
      "Epoch 21/120\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 11.6173 - acc: 0.5114 - val_loss: 11.9561 - val_acc: 0.4949\n",
      "Epoch 22/120\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 11.5411 - acc: 0.4993 - val_loss: 11.8781 - val_acc: 0.4949\n",
      "Epoch 23/120\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 11.4628 - acc: 0.5066 - val_loss: 11.8005 - val_acc: 0.4949\n",
      "Epoch 24/120\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 11.3852 - acc: 0.4963 - val_loss: 11.7234 - val_acc: 0.4949\n",
      "Epoch 25/120\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 11.3080 - acc: 0.5026 - val_loss: 11.6466 - val_acc: 0.4949\n",
      "Epoch 26/120\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 11.2317 - acc: 0.4978 - val_loss: 11.5704 - val_acc: 0.4949\n",
      "Epoch 27/120\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 11.1553 - acc: 0.4952 - val_loss: 11.4945 - val_acc: 0.4949\n",
      "Epoch 28/120\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 11.0786 - acc: 0.5040 - val_loss: 11.4189 - val_acc: 0.4963\n",
      "Epoch 29/120\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 11.0031 - acc: 0.5018 - val_loss: 11.3438 - val_acc: 0.4963\n",
      "Epoch 30/120\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 10.9271 - acc: 0.5143 - val_loss: 11.2691 - val_acc: 0.4949\n",
      "Epoch 31/120\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 10.8510 - acc: 0.5223 - val_loss: 11.1948 - val_acc: 0.4934\n",
      "Epoch 32/120\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 10.7772 - acc: 0.5128 - val_loss: 11.1209 - val_acc: 0.4934\n",
      "Epoch 33/120\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 10.7037 - acc: 0.5081 - val_loss: 11.0475 - val_acc: 0.4949\n",
      "Epoch 34/120\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 10.6301 - acc: 0.5165 - val_loss: 10.9745 - val_acc: 0.4978\n",
      "Epoch 35/120\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 10.5567 - acc: 0.5084 - val_loss: 10.9019 - val_acc: 0.4978\n",
      "Epoch 36/120\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 10.4847 - acc: 0.5037 - val_loss: 10.8297 - val_acc: 0.4963\n",
      "Epoch 37/120\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 10.4111 - acc: 0.5154 - val_loss: 10.7579 - val_acc: 0.4963\n",
      "Epoch 38/120\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 10.3407 - acc: 0.5059 - val_loss: 10.6867 - val_acc: 0.4993\n",
      "Epoch 39/120\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 10.2695 - acc: 0.5158 - val_loss: 10.6158 - val_acc: 0.4993\n",
      "Epoch 40/120\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 10.1971 - acc: 0.5088 - val_loss: 10.5453 - val_acc: 0.5007\n",
      "Epoch 41/120\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 10.1255 - acc: 0.5117 - val_loss: 10.4752 - val_acc: 0.5007\n",
      "Epoch 42/120\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 10.0572 - acc: 0.5066 - val_loss: 10.4055 - val_acc: 0.4963\n",
      "Epoch 43/120\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 9.9882 - acc: 0.5051 - val_loss: 10.3361 - val_acc: 0.4963\n",
      "Epoch 44/120\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 9.9161 - acc: 0.5161 - val_loss: 10.2672 - val_acc: 0.4963\n",
      "Epoch 45/120\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 9.8498 - acc: 0.4919 - val_loss: 10.1987 - val_acc: 0.4949\n",
      "Epoch 46/120\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 9.7793 - acc: 0.5044 - val_loss: 10.1306 - val_acc: 0.4963\n",
      "Epoch 47/120\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 9.7121 - acc: 0.4989 - val_loss: 10.0630 - val_acc: 0.4934\n",
      "Epoch 48/120\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 9.6439 - acc: 0.5040 - val_loss: 9.9957 - val_acc: 0.4905\n",
      "Epoch 49/120\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 9.5761 - acc: 0.5059 - val_loss: 9.9288 - val_acc: 0.4919\n",
      "Epoch 50/120\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 9.5113 - acc: 0.4941 - val_loss: 9.8622 - val_acc: 0.4949\n",
      "Epoch 51/120\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 9.4422 - acc: 0.4974 - val_loss: 9.7959 - val_acc: 0.4905\n",
      "Epoch 52/120\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 9.3751 - acc: 0.5139 - val_loss: 9.7301 - val_acc: 0.4934\n",
      "Epoch 53/120\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 9.3108 - acc: 0.5051 - val_loss: 9.6645 - val_acc: 0.4963\n",
      "Epoch 54/120\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 9.2454 - acc: 0.4963 - val_loss: 9.5994 - val_acc: 0.4963\n",
      "Epoch 55/120\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 9.1784 - acc: 0.4982 - val_loss: 9.5346 - val_acc: 0.4934\n",
      "Epoch 56/120\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 9.1138 - acc: 0.5136 - val_loss: 9.4703 - val_acc: 0.4934\n",
      "Epoch 57/120\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 9.0492 - acc: 0.4996 - val_loss: 9.4063 - val_acc: 0.4905\n",
      "Epoch 58/120\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 8.9852 - acc: 0.5037 - val_loss: 9.3426 - val_acc: 0.4890\n",
      "Epoch 59/120\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 8.9215 - acc: 0.5073 - val_loss: 9.2793 - val_acc: 0.4890\n",
      "Epoch 60/120\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 8.8578 - acc: 0.5059 - val_loss: 9.2164 - val_acc: 0.4890\n",
      "Epoch 61/120\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 8.7940 - acc: 0.5168 - val_loss: 9.1540 - val_acc: 0.4905\n",
      "Epoch 62/120\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 8.7346 - acc: 0.5040 - val_loss: 9.0917 - val_acc: 0.4905\n",
      "Epoch 63/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2730/2730 [==============================] - 0s 14us/step - loss: 8.6686 - acc: 0.5165 - val_loss: 9.0299 - val_acc: 0.4919\n",
      "Epoch 64/120\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 8.6091 - acc: 0.4938 - val_loss: 8.9685 - val_acc: 0.4919\n",
      "Epoch 65/120\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 8.5465 - acc: 0.5073 - val_loss: 8.9075 - val_acc: 0.4934\n",
      "Epoch 66/120\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 8.4850 - acc: 0.5037 - val_loss: 8.8468 - val_acc: 0.4963\n",
      "Epoch 67/120\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 8.4250 - acc: 0.5103 - val_loss: 8.7865 - val_acc: 0.4978\n",
      "Epoch 68/120\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 8.3642 - acc: 0.4971 - val_loss: 8.7265 - val_acc: 0.5022\n",
      "Epoch 69/120\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 8.3052 - acc: 0.4963 - val_loss: 8.6668 - val_acc: 0.5022\n",
      "Epoch 70/120\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 8.2465 - acc: 0.4952 - val_loss: 8.6076 - val_acc: 0.5007\n",
      "Epoch 71/120\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 8.1864 - acc: 0.4982 - val_loss: 8.5486 - val_acc: 0.4993\n",
      "Epoch 72/120\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 8.1253 - acc: 0.5011 - val_loss: 8.4902 - val_acc: 0.5007\n",
      "Epoch 73/120\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 8.0675 - acc: 0.5026 - val_loss: 8.4320 - val_acc: 0.4993\n",
      "Epoch 74/120\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 8.0088 - acc: 0.5092 - val_loss: 8.3743 - val_acc: 0.4993\n",
      "Epoch 75/120\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 7.9514 - acc: 0.5062 - val_loss: 8.3169 - val_acc: 0.4978\n",
      "Epoch 76/120\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 7.8934 - acc: 0.5117 - val_loss: 8.2598 - val_acc: 0.4978\n",
      "Epoch 77/120\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 7.8369 - acc: 0.4985 - val_loss: 8.2030 - val_acc: 0.4949\n",
      "Epoch 78/120\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 7.7791 - acc: 0.5037 - val_loss: 8.1466 - val_acc: 0.4919\n",
      "Epoch 79/120\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 7.7223 - acc: 0.5044 - val_loss: 8.0905 - val_acc: 0.4934\n",
      "Epoch 80/120\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 7.6680 - acc: 0.5015 - val_loss: 8.0347 - val_acc: 0.4919\n",
      "Epoch 81/120\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 7.6087 - acc: 0.5183 - val_loss: 7.9794 - val_acc: 0.4934\n",
      "Epoch 82/120\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 7.5575 - acc: 0.4875 - val_loss: 7.9243 - val_acc: 0.4949\n",
      "Epoch 83/120\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 7.5020 - acc: 0.4916 - val_loss: 7.8695 - val_acc: 0.4949\n",
      "Epoch 84/120\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 7.4464 - acc: 0.4923 - val_loss: 7.8150 - val_acc: 0.4949\n",
      "Epoch 85/120\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 7.3915 - acc: 0.5106 - val_loss: 7.7609 - val_acc: 0.4949\n",
      "Epoch 86/120\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 7.3366 - acc: 0.5106 - val_loss: 7.7071 - val_acc: 0.4963\n",
      "Epoch 87/120\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 7.2824 - acc: 0.5073 - val_loss: 7.6536 - val_acc: 0.4949\n",
      "Epoch 88/120\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 7.2295 - acc: 0.5099 - val_loss: 7.6005 - val_acc: 0.4949\n",
      "Epoch 89/120\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 7.1757 - acc: 0.5073 - val_loss: 7.5478 - val_acc: 0.4890\n",
      "Epoch 90/120\n",
      "2730/2730 [==============================] - 0s 14us/step - loss: 7.1234 - acc: 0.5125 - val_loss: 7.4953 - val_acc: 0.4876\n",
      "Epoch 91/120\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-203-70aaf7726aa4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdeep_estimator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDeepEstimator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdeep_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_pca\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test_pca\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-202-490ce3fb63b5>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, X_val, y_val)\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msched\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2048\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/sls_sec/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1000\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1001\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1002\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1003\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1004\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda2/envs/sls_sec/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1703\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1704\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1705\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1707\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda2/envs/sls_sec/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1220\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1221\u001b[0m                             \u001b[0;31m# Do not slice the training phase flag.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1222\u001b[0;31m                             \u001b[0mins_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_slice_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1223\u001b[0m                         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1224\u001b[0m                             \u001b[0mins_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_slice_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/sls_sec/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_slice_arrays\u001b[0;34m(arrays, start, stop)\u001b[0m\n\u001b[1;32m    383\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'shape'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m                 \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 385\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    386\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/sls_sec/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    383\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'shape'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m                 \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 385\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    386\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "deep_estimator = DeepEstimator()\n",
    "deep_estimator.fit(X_train_pca, y_train, X_test_pca, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pca():\n",
    "    X_train, X_test, y_train_continuous, y_test_continuous = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    y_train = bool_arr(y_train_continuous, 0.0)\n",
    "    y_test = bool_arr(y_test_continuous, 0.0)\n",
    "\n",
    "    from sklearn.decomposition import PCA\n",
    "    pca = PCA()\n",
    "    pca.fit(X_train)\n",
    "    X_train_pca = pca.transform(X_train)\n",
    "    X_test_pca = pca.transform(X_test)\n",
    "    return [X_train_pca, X_test_pca]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 1172.51it/s]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.00016559160722919868"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.average(y_test_continuous)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = deep_estimator.predict(X_train_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.18163335],\n",
       "       [0.18946415],\n",
       "       [0.18070437],\n",
       "       ...,\n",
       "       [0.19051173],\n",
       "       [0.18986735],\n",
       "       [0.18386003]], dtype=float32)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking : expected batch_normalization_64_input to have shape (2730,) but got array with shape (50429,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-195-c7c55b532fa2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mxs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinspace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdeep_estimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test_continuous\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0mys_attained\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mys_potential\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-195-c7c55b532fa2>\u001b[0m in \u001b[0;36mscore\u001b[0;34m(est, X, y, cutoff)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcutoff\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0my_pred_proba\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m# Invest if over cutoff% confident\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0my_pred_bool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbool_arr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcutoff\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-192-2e72d4bf757b>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/sls_sec/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1062\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1063\u001b[0m         return self.model.predict(x, batch_size=batch_size, verbose=verbose,\n\u001b[0;32m-> 1064\u001b[0;31m                                   steps=steps)\n\u001b[0m\u001b[1;32m   1065\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1066\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/sls_sec/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1815\u001b[0m         x = _standardize_input_data(x, self._feed_input_names,\n\u001b[1;32m   1816\u001b[0m                                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_feed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1817\u001b[0;31m                                     check_batch_axis=False)\n\u001b[0m\u001b[1;32m   1818\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstateful\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1819\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/sls_sec/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    121\u001b[0m                             \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m                             str(data_shape))\n\u001b[0m\u001b[1;32m    124\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking : expected batch_normalization_64_input to have shape (2730,) but got array with shape (50429,)"
     ]
    }
   ],
   "source": [
    "def score(est, X, y, cutoff):\n",
    "    y_pred = est.predict(X)\n",
    "    y_pred_proba = est.model.predict_proba(X)\n",
    "    # Invest if over cutoff% confident \n",
    "    y_pred_bool = bool_arr(y_pred, cutoff)\n",
    "    # Score correct if you make postive returns\n",
    "    y_true_bool = bool_arr(y, 0)\n",
    "    \n",
    "    total = np.dot(y_pred_bool.reshape(X.shape[0],), y)\n",
    "    confidence_weighted = np.dot(y_pred_proba.reshape(X.shape[0],), y)\n",
    "    buy_all = np.dot(np.ones(X.shape[0]), y)\n",
    "    \n",
    "    av = total/len(X)\n",
    "    buy_all_av = buy_all/len(X)\n",
    "    confidence_weighted_av = confidence_weighted/len(X)\n",
    "    return [av, buy_all_av, confidence_weighted_av]\n",
    "\n",
    "\n",
    "ys_attained = []\n",
    "ys_potential = []\n",
    "ys_confidence_weighted = []\n",
    "xs = []\n",
    "for i in np.linspace(0.1, 1, num=10):\n",
    "    sc = score(deep_estimator, X_test, y_test_continuous,i)\n",
    "    ys_attained.append(sc[0])\n",
    "    ys_potential.append(sc[1])\n",
    "    ys_confidence_weighted.append(sc[2])\n",
    "    xs.append(i)\n",
    "    \n",
    "buy_rec = plt.plot(xs, ys_attained, label='Buy Rec')\n",
    "buy_all = plt.plot(xs, ys_potential, label='Buy all')\n",
    "buy_wc = plt.plot(xs, ys_confidence_weighted, label='Bets Weighted Confidence')\n",
    "\n",
    "ax = plt.gca()\n",
    "\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "ax.legend()\n",
    "\n",
    "plt.ylim(-0.01, 0.01)\n",
    "\n",
    "# set the ylim to ymin, ymax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 395 samples, validate on 99 samples\n",
      "Epoch 1/10\n",
      "395/395 [==============================] - 1s 3ms/step - loss: 782.9136 - acc: 0.4886 - val_loss: 385.5391 - val_acc: 0.5859\n",
      "Epoch 2/10\n",
      "395/395 [==============================] - 1s 2ms/step - loss: 184.9604 - acc: 0.4835 - val_loss: 74.3654 - val_acc: 0.5455\n",
      "Epoch 3/10\n",
      "395/395 [==============================] - 1s 2ms/step - loss: 62.9197 - acc: 0.5215 - val_loss: 55.8841 - val_acc: 0.4444\n",
      "Epoch 4/10\n",
      "395/395 [==============================] - 1s 2ms/step - loss: 51.8920 - acc: 0.5544 - val_loss: 48.8269 - val_acc: 0.5051\n",
      "Epoch 5/10\n",
      "395/395 [==============================] - 1s 2ms/step - loss: 43.0366 - acc: 0.5747 - val_loss: 39.6641 - val_acc: 0.5758\n",
      "Epoch 6/10\n",
      "395/395 [==============================] - 1s 2ms/step - loss: 36.6025 - acc: 0.5089 - val_loss: 32.4293 - val_acc: 0.4444\n",
      "Epoch 7/10\n",
      "395/395 [==============================] - 1s 2ms/step - loss: 29.2403 - acc: 0.5620 - val_loss: 28.7175 - val_acc: 0.4949\n",
      "Epoch 8/10\n",
      "395/395 [==============================] - 1s 2ms/step - loss: 23.5813 - acc: 0.5772 - val_loss: 21.0366 - val_acc: 0.5455\n",
      "Epoch 9/10\n",
      "395/395 [==============================] - 1s 2ms/step - loss: 18.0876 - acc: 0.5747 - val_loss: 16.6903 - val_acc: 0.4444\n",
      "Epoch 10/10\n",
      "395/395 [==============================] - 1s 2ms/step - loss: 13.6045 - acc: 0.5772 - val_loss: 11.8911 - val_acc: 0.5960\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 20%|██        | 1/5 [00:07<00:31,  7.80s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 395 samples, validate on 99 samples\n",
      "Epoch 1/10\n",
      "395/395 [==============================] - 1s 3ms/step - loss: 838.6319 - acc: 0.5013 - val_loss: 319.5284 - val_acc: 0.4848\n",
      "Epoch 2/10\n",
      "395/395 [==============================] - 1s 2ms/step - loss: 136.0465 - acc: 0.6076 - val_loss: 71.4278 - val_acc: 0.8485\n",
      "Epoch 3/10\n",
      "395/395 [==============================] - 1s 2ms/step - loss: 61.6412 - acc: 0.6304 - val_loss: 53.3218 - val_acc: 0.4040\n",
      "Epoch 4/10\n",
      "395/395 [==============================] - 1s 2ms/step - loss: 50.5065 - acc: 0.6557 - val_loss: 44.9282 - val_acc: 0.4444\n",
      "Epoch 5/10\n",
      "395/395 [==============================] - 1s 2ms/step - loss: 42.3939 - acc: 0.6785 - val_loss: 38.5857 - val_acc: 0.7778\n",
      "Epoch 6/10\n",
      "395/395 [==============================] - 1s 2ms/step - loss: 35.3930 - acc: 0.7342 - val_loss: 30.5746 - val_acc: 0.7778\n",
      "Epoch 7/10\n",
      "395/395 [==============================] - 1s 2ms/step - loss: 28.9677 - acc: 0.7696 - val_loss: 25.3562 - val_acc: 0.8081\n",
      "Epoch 8/10\n",
      "395/395 [==============================] - 1s 2ms/step - loss: 23.2101 - acc: 0.7924 - val_loss: 20.9244 - val_acc: 0.8081\n",
      "Epoch 9/10\n",
      "395/395 [==============================] - 1s 2ms/step - loss: 17.8379 - acc: 0.8304 - val_loss: 14.8495 - val_acc: 0.8889\n",
      "Epoch 10/10\n",
      "395/395 [==============================] - 1s 2ms/step - loss: 13.4120 - acc: 0.8582 - val_loss: 10.7872 - val_acc: 0.8788\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 40%|████      | 2/5 [00:15<00:23,  7.99s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 395 samples, validate on 99 samples\n",
      "Epoch 1/10\n",
      "395/395 [==============================] - 1s 3ms/step - loss: 814.4856 - acc: 0.5342 - val_loss: 336.3111 - val_acc: 0.8788\n",
      "Epoch 2/10\n",
      "395/395 [==============================] - 1s 2ms/step - loss: 141.8063 - acc: 0.5949 - val_loss: 66.6091 - val_acc: 0.4141\n",
      "Epoch 3/10\n",
      "395/395 [==============================] - 1s 2ms/step - loss: 59.3873 - acc: 0.6582 - val_loss: 54.7897 - val_acc: 0.6465\n",
      "Epoch 4/10\n",
      "395/395 [==============================] - 1s 2ms/step - loss: 49.9166 - acc: 0.7089 - val_loss: 45.4332 - val_acc: 0.6364\n",
      "Epoch 5/10\n",
      "395/395 [==============================] - 1s 2ms/step - loss: 41.3727 - acc: 0.7823 - val_loss: 37.9590 - val_acc: 0.9495\n",
      "Epoch 6/10\n",
      "395/395 [==============================] - 1s 2ms/step - loss: 35.3992 - acc: 0.8506 - val_loss: 34.5799 - val_acc: 0.4343\n",
      "Epoch 7/10\n",
      "395/395 [==============================] - 1s 2ms/step - loss: 28.8786 - acc: 0.8734 - val_loss: 25.7028 - val_acc: 0.9495\n",
      "Epoch 8/10\n",
      "395/395 [==============================] - 1s 2ms/step - loss: 23.8005 - acc: 0.8987 - val_loss: 20.4520 - val_acc: 0.9697\n",
      "Epoch 9/10\n",
      "395/395 [==============================] - 1s 2ms/step - loss: 17.7657 - acc: 0.9291 - val_loss: 15.9452 - val_acc: 0.5556\n",
      "Epoch 10/10\n",
      "395/395 [==============================] - 1s 2ms/step - loss: 13.1694 - acc: 0.9468 - val_loss: 10.8998 - val_acc: 0.9697\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 60%|██████    | 3/5 [00:24<00:16,  8.04s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 395 samples, validate on 99 samples\n",
      "Epoch 1/10\n",
      "395/395 [==============================] - 1s 3ms/step - loss: 728.5609 - acc: 0.5316 - val_loss: 407.4310 - val_acc: 0.5657\n",
      "Epoch 2/10\n",
      "395/395 [==============================] - 1s 2ms/step - loss: 215.1073 - acc: 0.6152 - val_loss: 85.3864 - val_acc: 0.7879\n",
      "Epoch 3/10\n",
      "395/395 [==============================] - 1s 2ms/step - loss: 61.8023 - acc: 0.6759 - val_loss: 56.3565 - val_acc: 0.2525\n",
      "Epoch 4/10\n",
      "395/395 [==============================] - 1s 2ms/step - loss: 49.3674 - acc: 0.7443 - val_loss: 44.8255 - val_acc: 0.9697\n",
      "Epoch 5/10\n",
      "395/395 [==============================] - 1s 2ms/step - loss: 41.6270 - acc: 0.8304 - val_loss: 38.4864 - val_acc: 0.8384\n",
      "Epoch 6/10\n",
      "395/395 [==============================] - 1s 2ms/step - loss: 34.4543 - acc: 0.8734 - val_loss: 32.3487 - val_acc: 0.6768\n",
      "Epoch 7/10\n",
      "395/395 [==============================] - 1s 2ms/step - loss: 28.3719 - acc: 0.8937 - val_loss: 25.3998 - val_acc: 0.9596\n",
      "Epoch 8/10\n",
      "395/395 [==============================] - 1s 2ms/step - loss: 22.3892 - acc: 0.9392 - val_loss: 21.9376 - val_acc: 0.6465\n",
      "Epoch 9/10\n",
      "395/395 [==============================] - 1s 2ms/step - loss: 17.5234 - acc: 0.9620 - val_loss: 14.9847 - val_acc: 0.9697\n",
      "Epoch 10/10\n",
      "395/395 [==============================] - 1s 2ms/step - loss: 12.9667 - acc: 0.9671 - val_loss: 11.0789 - val_acc: 0.9697\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 80%|████████  | 4/5 [00:32<00:08,  8.03s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 395 samples, validate on 99 samples\n",
      "Epoch 1/10\n",
      "395/395 [==============================] - 1s 3ms/step - loss: 696.7543 - acc: 0.5342 - val_loss: 411.0017 - val_acc: 0.4949\n",
      "Epoch 2/10\n",
      "395/395 [==============================] - 1s 2ms/step - loss: 230.0261 - acc: 0.5975 - val_loss: 98.9191 - val_acc: 0.8283\n",
      "Epoch 3/10\n",
      "395/395 [==============================] - 1s 2ms/step - loss: 61.5024 - acc: 0.6962 - val_loss: 50.3639 - val_acc: 0.8182\n",
      "Epoch 4/10\n",
      "395/395 [==============================] - 1s 2ms/step - loss: 45.5703 - acc: 0.8051 - val_loss: 41.4631 - val_acc: 0.9495\n",
      "Epoch 5/10\n",
      "395/395 [==============================] - 1s 2ms/step - loss: 37.9643 - acc: 0.8962 - val_loss: 37.8119 - val_acc: 0.6162\n",
      "Epoch 6/10\n",
      "395/395 [==============================] - 1s 2ms/step - loss: 31.1674 - acc: 0.9595 - val_loss: 28.1202 - val_acc: 0.9798\n",
      "Epoch 7/10\n",
      "395/395 [==============================] - 1s 2ms/step - loss: 25.2545 - acc: 0.9722 - val_loss: 22.6497 - val_acc: 0.9798\n",
      "Epoch 8/10\n",
      "395/395 [==============================] - 1s 2ms/step - loss: 19.9744 - acc: 0.9797 - val_loss: 17.8094 - val_acc: 0.9798\n",
      "Epoch 9/10\n",
      "395/395 [==============================] - 1s 2ms/step - loss: 15.4994 - acc: 0.9797 - val_loss: 13.6302 - val_acc: 0.9798\n",
      "Epoch 10/10\n",
      "395/395 [==============================] - 1s 2ms/step - loss: 11.6096 - acc: 0.9797 - val_loss: 9.9832 - val_acc: 0.9798\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 5/5 [00:40<00:00,  8.01s/it]\u001b[A\n",
      "\u001b[A"
     ]
    }
   ],
   "source": [
    "ys_confidence_weighted = []\n",
    "ys_potential = []\n",
    "xs = []\n",
    "\n",
    "for i in tqdm(np.linspace(0,0.2, num=5)):\n",
    "    X_train, X_test, y_train_continuous, y_test_continuous = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    y_train = bool_arr(y_train_continuous, i)\n",
    "    y_test = bool_arr(y_test_continuous, i)\n",
    "\n",
    "    select_fpr = SelectFpr(alpha=0.2)\n",
    "\n",
    "    X_train = select_fpr.fit_transform(X_train, y_train)\n",
    "    X_test = select_fpr.transform(X_test)\n",
    "\n",
    "    deep_estimator = DeepEstimator()\n",
    "    deep_estimator.fit(X_train, y_train, X_test, y_test)\n",
    "    \n",
    "    round_score = score(deep_estimator, X_test, y_test_continuous,0)\n",
    "    \n",
    "    buy_all_score = round_score[1]\n",
    "    bwc_score = round_score[2]\n",
    "    \n",
    "    xs.append(i)\n",
    "    ys_potential.append(buy_all_score)\n",
    "    ys_confidence_weighted.append(bwc_score)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot earnings on buying weighted by confidence vs buying all stocks as the prediction threshold increases in training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f5c2de2c7b8>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(xs, ys_confidence_weighted, label='Bets Weighted Confidence')\n",
    "buy_all = plt.plot(xs, ys_potential, label='Buy all')\n",
    "\n",
    "ax = plt.gca()\n",
    "\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5.06329114e-02, -8.33333333e-02,  2.77752315e-02, -5.07614213e-03,\n",
       "        3.63813020e-02, -5.13392857e-02,  4.96592404e-02,  3.64958496e-02,\n",
       "        6.07476636e-02,  2.98762271e-03, -3.37804878e-02, -8.06538337e-03,\n",
       "       -3.45423143e-04,  5.00000000e-02,  1.20481928e-02,  9.83606557e-03,\n",
       "        1.39475909e-02,  3.27552987e-02,  1.27659574e-02,  1.65539753e-02,\n",
       "       -4.98960499e-02,  3.86431945e-03,  3.63108206e-03,  1.57728707e-02,\n",
       "        8.87110802e-03,  6.61150152e-03,  2.71158587e-02, -4.44444444e-02,\n",
       "       -1.57728707e-02, -3.55439965e-02, -2.62928620e-02,  4.93421053e-03,\n",
       "        2.36951983e-01,  1.26874279e-02,  2.94627383e-02, -8.69565217e-03,\n",
       "        9.99826620e-03,  2.28859238e-02,  3.09697933e-02, -1.86831594e-02,\n",
       "        1.75438596e-02, -3.11662671e-02, -2.27968697e-02,  1.78117048e-02,\n",
       "       -5.29801325e-02,  9.94087454e-03, -2.50000000e-02, -1.44000000e-01,\n",
       "        6.03681580e-02,  3.90015601e-02,  2.59342214e-02,  4.15754923e-02,\n",
       "       -6.39417240e-03,  4.31479223e-03,  1.02406554e-02, -5.50711335e-03,\n",
       "        2.42662353e-03, -3.09520612e-02, -5.31408485e-02, -2.58175559e-02,\n",
       "        1.93925419e-16,  5.39798719e-02,  1.09812879e-02,  5.94584223e-02,\n",
       "        9.35828877e-03,  1.27143132e-02,  1.44305772e-02,  1.43790850e-02,\n",
       "        3.31682102e-02, -5.10745186e-02,  4.47110142e-02,  3.94335902e-02,\n",
       "        9.50323974e-02,  4.87580497e-02,  7.58676352e-03,  1.53366584e-01,\n",
       "       -1.13360324e-02, -3.55878143e-02, -6.10932476e-02,  7.55939525e-02,\n",
       "        3.61230529e-02, -5.00454959e-02, -1.76056338e-02,  3.40479193e-02,\n",
       "       -3.30312774e-02, -5.08474576e-03, -6.77290837e-02, -1.66340509e-01,\n",
       "       -5.72088674e-03, -5.69411765e-02, -1.12745772e-02, -4.72914875e-02,\n",
       "       -2.12985228e-03,  3.06990881e-01,  4.90083593e-02, -2.33181453e-02,\n",
       "       -1.78080886e-16, -3.04182510e-02,  2.32981434e-03])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_continuous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class_weights = []\n",
    "# for i in np.linspace(0.01,0.1, num=10):\n",
    "#     class_weights.append({0: i, 1: 1-i})\n",
    "\n",
    "# param_grid = dict(\n",
    "#     class_weight=class_weights\n",
    "# )\n",
    "\n",
    "# svc_train_pipeline = Pipeline([\n",
    "#     ('reduce_false_pos', SelectFpr(alpha=0.9)),\n",
    "#     ('svc', RandomizedSearchCV(estimator=SVC(\n",
    "#         gamma='auto', \n",
    "#         kernel='sigmoid',\n",
    "#         C=0.5623,\n",
    "#         probability=True,\n",
    "#         class_weight='balanced'), \n",
    "#         scoring='average_precision',\n",
    "#         param_distributions=param_grid))\n",
    "#      ])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:sls_sec]",
   "language": "python",
   "name": "conda-env-sls_sec-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
